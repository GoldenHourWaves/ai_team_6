"""
Task 2: ë‚ ì§œ í˜•ì‹ í†µì¼ ë° ë°ì´í„° ì •ì œ
ëª¨ë“  ë°ì´í„°ì˜ ë‚ ì§œ í˜•ì‹ì„ datetimeìœ¼ë¡œ ë³€í™˜í•˜ê³  ê²°ì¸¡ì¹˜ ì²˜ë¦¬
"""

import pandas as pd
import numpy as np
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# ë°ì´í„° ê²½ë¡œ ì„¤ì •
DATA_DIR = Path("data/processed")

def convert_date_to_datetime(date_value):
    """
    ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ì„ datetimeìœ¼ë¡œ ë³€í™˜
    YYYYMMDD (int/str) -> datetime
    YYYY-MM-DD (str) -> datetime
    """
    if pd.isna(date_value):
        return pd.NaT
    
    # int64 ë˜ëŠ” ìˆ«ì í˜•íƒœì˜ YYYYMMDD
    if isinstance(date_value, (int, np.integer)):
        return pd.to_datetime(str(date_value), format='%Y%m%d')
    
    # ë¬¸ìì—´ í˜•íƒœ
    date_str = str(date_value)
    
    # YYYYMMDD í˜•ì‹ (8ìë¦¬)
    if len(date_str) == 8 and date_str.isdigit():
        return pd.to_datetime(date_str, format='%Y%m%d')
    
    # ê¸°íƒ€ í˜•ì‹ì€ pandasê°€ ìë™ íŒŒì‹±
    try:
        return pd.to_datetime(date_str)
    except:
        return pd.NaT

def clean_and_standardize_data():
    """ëª¨ë“  CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë‚ ì§œ í˜•ì‹ í†µì¼ ë° ê²°ì¸¡ì¹˜ ì²˜ë¦¬"""
    
    print("=" * 80)
    print("Task 2: ë‚ ì§œ í˜•ì‹ í†µì¼ ë° ë°ì´í„° ì •ì œ ì‹œì‘")
    print("=" * 80)
    
    # ===== 1. Bitcoin News Data =====
    print("\n[1/6] Bitcoin News ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
    df_news = pd.read_csv(DATA_DIR / "bitcoin_news_merged_0.csv")
    print(f"  ì›ë³¸ shape: {df_news.shape}")
    
    # ë‚ ì§œ ë³€í™˜
    df_news['date'] = df_news['date'].apply(convert_date_to_datetime)
    print(f"  âœ… date ì»¬ëŸ¼ datetime ë³€í™˜ ì™„ë£Œ")
    print(f"  ê²°ì¸¡ì¹˜: date={df_news['date'].isna().sum()}, v2_themes={df_news['v2_themes'].isna().sum()}")
    
    # ===== 2. Features Daily Data =====
    print("\n[2/6] Features Daily ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
    df_features = pd.read_csv(DATA_DIR / "features_daily.csv")
    print(f"  ì›ë³¸ shape: {df_features.shape}")
    
    df_features['date'] = df_features['date'].apply(convert_date_to_datetime)
    print(f"  âœ… date ì»¬ëŸ¼ datetime ë³€í™˜ ì™„ë£Œ")
    print(f"  ê²°ì¸¡ì¹˜: {df_features.isna().sum().sum()}ê°œ (ëª¨ë“  ì»¬ëŸ¼)")
    
    # ===== 3. GDELT Articles Data =====
    print("\n[3/6] GDELT Articles ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
    df_gdelt = pd.read_csv(DATA_DIR / "gdelt_articles_modified_0.csv")
    print(f"  ì›ë³¸ shape: {df_gdelt.shape}")
    
    df_gdelt['date'] = df_gdelt['date'].apply(convert_date_to_datetime)
    df_gdelt['published_at_utc_dt'] = pd.to_datetime(df_gdelt['published_at_utc_dt'])
    print(f"  âœ… ë‚ ì§œ ì»¬ëŸ¼ datetime ë³€í™˜ ì™„ë£Œ")
    print(f"  ê²°ì¸¡ì¹˜: date={df_gdelt['date'].isna().sum()}, title={df_gdelt['title'].isna().sum()}")
    
    # ===== 4. Daily Data (ê±°ì‹œê²½ì œ + ê°€ê²© ë°ì´í„°) =====
    print("\n[4/6] Daily Data ì²˜ë¦¬ ì¤‘...")
    df_daily = pd.read_csv(DATA_DIR / "merged_ì •í˜•ë°ì´í„°" / "daily_data_merged.csv")
    print(f"  ì›ë³¸ shape: {df_daily.shape}")
    
    # ë‚ ì§œ ë³€í™˜
    df_daily['Date'] = df_daily['Date'].apply(convert_date_to_datetime)
    df_daily = df_daily.rename(columns={'Date': 'date'})  # ì»¬ëŸ¼ëª… í†µì¼
    
    print(f"\n  [ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì „]")
    for col in df_daily.columns:
        if df_daily[col].isna().sum() > 0:
            print(f"    {col}: {df_daily[col].isna().sum()} ({df_daily[col].isna().sum()/len(df_daily)*100:.2f}%)")
    
    # ë‚ ì§œ ìˆœ ì •ë ¬
    df_daily = df_daily.sort_values('date').reset_index(drop=True)
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì „ëµ
    # 1. ì„ í˜• ë³´ê°„ (ì—°ì†ì ì¸ ìˆ˜ì¹˜ ë°ì´í„°)
    numeric_cols = ['Yield_10Y', 'Gold_Price_YF', 'Gold_Price_Investing', 'USD_Index']
    for col in numeric_cols:
        if col in df_daily.columns:
            df_daily[col] = df_daily[col].interpolate(method='linear', limit_direction='both')
    
    # 2. Forward fill (ì†ë„ ê´€ë ¨ ë°ì´í„° - ì´ì „ ê°’ ìœ ì§€)
    speed_cols = ['BTC_Price_Speed', 'M2_Expansion_Speed']
    for col in speed_cols:
        if col in df_daily.columns:
            df_daily[col] = df_daily[col].fillna(0)  # ì†ë„ëŠ” 0ìœ¼ë¡œ ì´ˆê¸°í™”
    
    print(f"\n  [ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í›„]")
    for col in df_daily.columns:
        if df_daily[col].isna().sum() > 0:
            print(f"    {col}: {df_daily[col].isna().sum()} ({df_daily[col].isna().sum()/len(df_daily)*100:.2f}%)")
    
    if df_daily.isna().sum().sum() == 0:
        print(f"  âœ… ëª¨ë“  ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
    
    # ===== 5. M2 & Inflation Data =====
    print("\n[5/6] M2 & Inflation ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
    df_m2 = pd.read_csv(DATA_DIR / "merged_ì •í˜•ë°ì´í„°" / "merged_m2_inflation.csv")
    print(f"  ì›ë³¸ shape: {df_m2.shape}")
    
    df_m2['Date'] = df_m2['Date'].apply(convert_date_to_datetime)
    df_m2 = df_m2.rename(columns={'Date': 'date'})
    
    print(f"  ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì „: M2SL={df_m2['M2SL'].isna().sum()}")
    
    # M2ëŠ” ì›”ë³„ ë°ì´í„°ì´ë¯€ë¡œ ì¼ë³„ë¡œ í™•ì¥ (Forward Fill)
    # ë‚ ì§œ ë²”ìœ„ ìƒì„± (9ì›” 1ì¼ ~ 11ì›” 1ì¼)
    date_range = pd.date_range(start='2025-09-01', end='2025-11-01', freq='D')
    df_m2_expanded = pd.DataFrame({'date': date_range})
    
    # ê¸°ì¡´ M2 ë°ì´í„°ì™€ ë³‘í•© (ì›” ì²«ë‚  ë°ì´í„°ë§Œ ìˆìŒ)
    df_m2_expanded = df_m2_expanded.merge(df_m2, on='date', how='left')
    
    # Forward fillë¡œ ì¼ë³„ í™•ì¥
    df_m2_expanded['M2SL'] = df_m2_expanded['M2SL'].ffill()
    df_m2_expanded['CPI_YoY_Inflation_Rate'] = df_m2_expanded['CPI_YoY_Inflation_Rate'].ffill()
    
    print(f"  âœ… M2 ë°ì´í„°ë¥¼ ì¼ë³„ë¡œ í™•ì¥ (Forward Fill)")
    print(f"  í™•ì¥ í›„ shape: {df_m2_expanded.shape}")
    print(f"  ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í›„: M2SL={df_m2_expanded['M2SL'].isna().sum()}, CPI={df_m2_expanded['CPI_YoY_Inflation_Rate'].isna().sum()}")
    
    df_m2 = df_m2_expanded
    
    # ===== 6. SNS/YouTube Data =====
    print("\n[6/6] SNS/YouTube ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
    df_sns = pd.read_csv(DATA_DIR / "SNS_Youtube_data" / "FINAL_SNS_YOUTUBE.csv")
    print(f"  ì›ë³¸ shape: {df_sns.shape}")
    
    df_sns['STD_DATE'] = df_sns['STD_DATE'].apply(convert_date_to_datetime)
    df_sns = df_sns.rename(columns={'STD_DATE': 'date'})
    df_sns['original_date'] = pd.to_datetime(df_sns['original_date'], utc=True, errors='coerce')
    
    print(f"  âœ… ë‚ ì§œ ì»¬ëŸ¼ datetime ë³€í™˜ ì™„ë£Œ")
    print(f"  ê²°ì¸¡ì¹˜: url={df_sns['url'].isna().sum()}")
    
    # ===== ì •ì œëœ ë°ì´í„° ì €ì¥ =====
    print("\n" + "=" * 80)
    print("ì •ì œëœ ë°ì´í„° ì €ì¥ ì¤‘...")
    print("=" * 80)
    
    output_dir = Path("data/processed/cleaned")
    output_dir.mkdir(exist_ok=True)
    
    df_news.to_csv(output_dir / "bitcoin_news_cleaned.csv", index=False)
    print(f"  âœ… bitcoin_news_cleaned.csv ì €ì¥")
    
    df_features.to_csv(output_dir / "features_daily_cleaned.csv", index=False)
    print(f"  âœ… features_daily_cleaned.csv ì €ì¥")
    
    df_gdelt.to_csv(output_dir / "gdelt_articles_cleaned.csv", index=False)
    print(f"  âœ… gdelt_articles_cleaned.csv ì €ì¥")
    
    df_daily.to_csv(output_dir / "daily_data_cleaned.csv", index=False)
    print(f"  âœ… daily_data_cleaned.csv ì €ì¥")
    
    df_m2.to_csv(output_dir / "m2_inflation_daily_expanded.csv", index=False)
    print(f"  âœ… m2_inflation_daily_expanded.csv ì €ì¥")
    
    df_sns.to_csv(output_dir / "sns_youtube_cleaned.csv", index=False)
    print(f"  âœ… sns_youtube_cleaned.csv ì €ì¥")
    
    # ===== ìµœì¢… ê²€ì¦ =====
    print("\n" + "=" * 80)
    print("ğŸ“Š ìµœì¢… ê²€ì¦ ê²°ê³¼")
    print("=" * 80)
    
    datasets = {
        'bitcoin_news': df_news,
        'features_daily': df_features,
        'gdelt_articles': df_gdelt,
        'daily_data': df_daily,
        'm2_inflation': df_m2,
        'sns_youtube': df_sns
    }
    
    for name, df in datasets.items():
        date_col = 'date'
        if date_col in df.columns:
            print(f"\nâœ… {name}")
            print(f"   ë‚ ì§œ íƒ€ì…: {df[date_col].dtype}")
            print(f"   ë‚ ì§œ ë²”ìœ„: {df[date_col].min()} ~ {df[date_col].max()}")
            print(f"   ì´ ê²°ì¸¡ì¹˜: {df.isna().sum().sum()}ê°œ")
            print(f"   Shape: {df.shape}")
    
    print("\n" + "=" * 80)
    print("Task 2 ì™„ë£Œ! âœ…")
    print("=" * 80)
    
    return datasets

if __name__ == "__main__":
    cleaned_data = clean_and_standardize_data()
    print(f"\nâœ… ì •ì œëœ ë°ì´í„°ì…‹ì´ data/processed/cleaned/ ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
