{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392a8c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "YouTube ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ê¸°\n",
      "====================================================================================================\n",
      "\n",
      "âœ… YouTube API ì—°ê²° ì„±ê³µ!\n",
      "\n",
      "ìˆ˜ì§‘ ì‹œì‘...\n",
      "  ê¸°ê°„: 2025-09-01 ~ 2025-10-31\n",
      "  ëª©í‘œ: ì˜ìƒ 1000ê°œ, ëŒ“ê¸€ 15000ê°œ\n",
      "\n",
      "[1/4] ì˜ìƒ ê²€ìƒ‰ ì¤‘...\n",
      "  [1/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'October 2025 crypto crash' â†’ 50ê°œ ì˜ìƒ\n",
      "  [2/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'crypto crash October 2025' â†’ 50ê°œ ì˜ìƒ\n",
      "  [3/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ '$19 billion liquidation' â†’ 25ê°œ ì˜ìƒ\n",
      "  [4/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'October 10 2025 crypto' â†’ 50ê°œ ì˜ìƒ\n",
      "  [5/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'October 11 2025 bitcoin' â†’ 50ê°œ ì˜ìƒ\n",
      "  [6/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'Trump tariff crypto crash' â†’ 50ê°œ ì˜ìƒ\n",
      "  [7/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'Binance crash October 2025' â†’ 50ê°œ ì˜ìƒ\n",
      "  [8/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'Hyperliquid liquidation' â†’ 50ê°œ ì˜ìƒ\n",
      "  [9/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'crypto flash crash 2025' â†’ 50ê°œ ì˜ìƒ\n",
      "  [10/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'October 2025 crypto analysis' â†’ 25ê°œ ì˜ìƒ\n",
      "  [11/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'crypto market crash 2025' â†’ 25ê°œ ì˜ìƒ\n",
      "  [12/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'bitcoin crash October' â†’ 50ê°œ ì˜ìƒ\n",
      "  [13/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'ethereum crash October 2025' â†’ 50ê°œ ì˜ìƒ\n",
      "  [14/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'crypto liquidation 2025' â†’ 50ê°œ ì˜ìƒ\n",
      "  [15/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'whale liquidation October' â†’ 50ê°œ ì˜ìƒ\n",
      "  [16/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'crypto bloodbath 2025' â†’ 50ê°œ ì˜ìƒ\n",
      "  [17/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'October crypto dump' â†’ 50ê°œ ì˜ìƒ\n",
      "  [18/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'crypto crash reaction October' â†’ 50ê°œ ì˜ìƒ\n",
      "  [19/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'October 2025 trading disaster' â†’ 50ê°œ ì˜ìƒ\n",
      "  [20/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'crypto portfolio destroyed' â†’ 50ê°œ ì˜ìƒ\n",
      "  [21/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'bitcoin September 2025' â†’ 50ê°œ ì˜ìƒ\n",
      "  [22/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'ethereum October 2025' â†’ 50ê°œ ì˜ìƒ\n",
      "  ğŸ¯ ëª©í‘œ ë‹¬ì„±! (1000ê°œ)\n",
      "\n",
      "âœ… ì´ 719ê°œ ì˜ìƒ ë°œê²¬ (ì¤‘ë³µ ì œê±° í›„)\n",
      "ğŸ’¾ ì €ì¥: ./youtube_data_collection/videos_metadata.csv\n",
      "\n",
      "[2/4] ì˜ìƒ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...\n",
      "âœ… 719ê°œ ì˜ìƒ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘\n",
      "\n",
      "[3/4] ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘... (ì˜ìƒ 719ê°œ)\n",
      "  ì§„í–‰: 50/719 (7.0%) | ëŒ“ê¸€: 2554ê°œ | ETA: 18.2ë¶„\n",
      "  ì§„í–‰: 100/719 (13.9%) | ëŒ“ê¸€: 3219ê°œ | ETA: 12.0ë¶„\n",
      "  ì§„í–‰: 150/719 (20.9%) | ëŒ“ê¸€: 3453ê°œ | ETA: 9.5ë¶„\n",
      "  ì§„í–‰: 200/719 (27.8%) | ëŒ“ê¸€: 4606ê°œ | ETA: 8.1ë¶„\n",
      "  ì§„í–‰: 250/719 (34.8%) | ëŒ“ê¸€: 5503ê°œ | ETA: 6.9ë¶„\n",
      "  ì§„í–‰: 300/719 (41.7%) | ëŒ“ê¸€: 6344ê°œ | ETA: 6.0ë¶„\n",
      "  ì§„í–‰: 350/719 (48.7%) | ëŒ“ê¸€: 8234ê°œ | ETA: 5.2ë¶„\n",
      "  ì§„í–‰: 400/719 (55.6%) | ëŒ“ê¸€: 10155ê°œ | ETA: 4.4ë¶„\n",
      "  ì§„í–‰: 450/719 (62.6%) | ëŒ“ê¸€: 10643ê°œ | ETA: 3.6ë¶„\n",
      "  ì§„í–‰: 500/719 (69.5%) | ëŒ“ê¸€: 11248ê°œ | ETA: 2.9ë¶„\n",
      "  ì§„í–‰: 550/719 (76.5%) | ëŒ“ê¸€: 13987ê°œ | ETA: 2.2ë¶„\n",
      "  ğŸ¯ ëŒ“ê¸€ ëª©í‘œ ë‹¬ì„±! (15000ê°œ)\n",
      "\n",
      "âœ… ì´ 15060ê°œ ëŒ“ê¸€ ìˆ˜ì§‘\n",
      "ğŸ’¾ ì €ì¥: ./youtube_data_collection/comments_temp.csv\n",
      "\n",
      "[4/4] ìë§‰ ìˆ˜ì§‘ ì¤‘... (ì˜ìƒ 719ê°œ)\n",
      "  ì§„í–‰: 50/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 100/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 150/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 200/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 250/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 300/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 350/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 400/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 450/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 500/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 550/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 600/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 650/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 700/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "\n",
      "âœ… ì´ 0ê°œ ìë§‰ ë¬¸ì¥ ìˆ˜ì§‘ (0ê°œ ì˜ìƒ)\n",
      "\n",
      "====================================================================================================\n",
      "ìˆ˜ì§‘ ì™„ë£Œ!\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“Š ìˆ˜ì§‘ í†µê³„:\n",
      "  ì˜ìƒ: 719ê°œ\n",
      "    - ì œëª©: 719ê°œ\n",
      "    - ì„¤ëª…: 596ê°œ\n",
      "  ëŒ“ê¸€: 15060ê°œ\n",
      "  ìë§‰: 0ê°œ ë¬¸ì¥\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  ì´ í…ìŠ¤íŠ¸ í•­ëª©: 15,779ê°œ\n",
      "  ì†Œìš” ì‹œê°„: 11.2ë¶„\n",
      "  ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: 1,292,575 ê¸€ì\n",
      "\n",
      "ğŸ’¾ ì˜ìƒ ì €ì¥: ./youtube_data_collection/youtube_videos_20260202_174228.csv\n",
      "   í¬ê¸°: 230.2 KB\n",
      "ğŸ’¾ ëŒ“ê¸€ ì €ì¥: ./youtube_data_collection/youtube_comments_20260202_174228.csv\n",
      "   í¬ê¸°: 2390.5 KB\n",
      "ğŸ’¾ JSON ì €ì¥: ./youtube_data_collection/youtube_collection_20260202_174228.json\n",
      "\n",
      "====================================================================================================\n",
      "ë°ì´í„° ë¶„ì„ ë¦¬í¬íŠ¸\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“¹ ì˜ìƒ ë¶„ì„:\n",
      "  í‰ê·  ì¡°íšŒìˆ˜: 248,808\n",
      "  í‰ê·  ì¢‹ì•„ìš”: 6,399\n",
      "  í‰ê·  ëŒ“ê¸€ìˆ˜: 169\n",
      "\n",
      "  ìƒìœ„ ì±„ë„:\n",
      "    - Crypto Nutshell: 13ê°œ\n",
      "    - Altcoin Daily: 11ê°œ\n",
      "    - Coin Bureau: 6ê°œ\n",
      "    - Gerhard - Bitcoin Strategy: 6ê°œ\n",
      "    - Crypto Prices: 6ê°œ\n",
      "\n",
      "ğŸ’¬ ëŒ“ê¸€ ë¶„ì„:\n",
      "  í‰ê·  ê¸¸ì´: 78 ê¸€ì\n",
      "  í‰ê·  ì¢‹ì•„ìš”: 56.2\n",
      "  ë‹µê¸€ ìˆëŠ” ëŒ“ê¸€: 3619ê°œ\n",
      "\n",
      "====================================================================================================\n",
      "ëª©í‘œ ë‹¬ì„± í˜„í™©\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ¯ ëª©í‘œ: 10,000 - 20,000ê°œ í…ìŠ¤íŠ¸ ë°ì´í„°\n",
      "âœ… ë‹¬ì„±: 15,779ê°œ\n",
      "ğŸ‰ ëª©í‘œ ë‹¬ì„±! (105.2%)\n",
      "\n",
      "ğŸ“‚ ì €ì¥ ìœ„ì¹˜: c:\\junwoo\\AI_Project_01_Team6\\data\\Youtube_data\\youtube_data_collection\n",
      "\n",
      "âœ… ìˆ˜ì§‘ ì™„ë£Œ! íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "YouTube ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ê¸°\n",
    "October 2025 Crypto Crash ì „ìš©\n",
    "\n",
    "ëª©í‘œ: 10,000 - 20,000ê°œ í…ìŠ¤íŠ¸ ë°ì´í„°\n",
    "ê¸°ê°„: 2025-09-01 ~ 2025-10-31\n",
    "ìˆ˜ì§‘ ëŒ€ìƒ: ì˜ìƒ ë©”íƒ€ë°ì´í„° + ëŒ“ê¸€ + ìë§‰(ìº¡ì…˜)\n",
    "\n",
    "ì˜ˆìƒ ìˆ˜ì§‘ëŸ‰:\n",
    "- ì˜ìƒ: 500-1,000ê°œ\n",
    "- ëŒ“ê¸€: 10,000-15,000ê°œ\n",
    "- ìë§‰: 5,000-10,000ê°œ ë¬¸ì¥\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ì´: 15,000-26,000ê°œ í…ìŠ¤íŠ¸\n",
    "\n",
    "ì†Œìš” ì‹œê°„: 2-4ì‹œê°„\n",
    "ë¹„ìš©: ë¬´ë£Œ (í• ë‹¹ëŸ‰ 10,000 units/ì¼)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "\n",
    "# ============================================================================\n",
    "# ì„¤ì • (ì—¬ê¸°ë§Œ ìˆ˜ì •í•˜ì„¸ìš”!)\n",
    "# ============================================================================\n",
    "\n",
    "# YouTube API í‚¤\n",
    "# ë°œê¸‰ ë°©ë²•: https://console.cloud.google.com/apis/credentials\n",
    "API_KEY = 'AIzaSyDj4y7cahjxnvq3ffSQHelAGjEWhea65JE'  # â† ì—¬ê¸°ì— ë¶™ì—¬ë„£ê¸°!\n",
    "\n",
    "# ìˆ˜ì§‘ ì„¤ì •\n",
    "COLLECTION_CONFIG = {\n",
    "    'date_range': {\n",
    "        'start': '2025-09-01T00:00:00Z',\n",
    "        'end': '2025-10-31T23:59:59Z'\n",
    "    },\n",
    "    'target_videos': 1000,          # ëª©í‘œ ì˜ìƒ ìˆ˜\n",
    "    'target_comments': 15000,       # ëª©í‘œ ëŒ“ê¸€ ìˆ˜\n",
    "    'max_comments_per_video': 100,  # ì˜ìƒë‹¹ ìµœëŒ€ ëŒ“ê¸€\n",
    "    'target_captions': 10000,       # ëª©í‘œ ìë§‰ ë¬¸ì¥ ìˆ˜\n",
    "}\n",
    "\n",
    "# ê²€ìƒ‰ í‚¤ì›Œë“œ (í¬ë¦½í†  í¬ë˜ì‹œ ê´€ë ¨)\n",
    "SEARCH_KEYWORDS = [\n",
    "    # í•µì‹¬ í‚¤ì›Œë“œ\n",
    "    'October 2025 crypto crash',\n",
    "    'crypto crash October 2025',\n",
    "    '$19 billion liquidation',\n",
    "    'October 10 2025 crypto',\n",
    "    'October 11 2025 bitcoin',\n",
    "    \n",
    "    # ì´ë²¤íŠ¸\n",
    "    'Trump tariff crypto crash',\n",
    "    'Binance crash October 2025',\n",
    "    'Hyperliquid liquidation',\n",
    "    'crypto flash crash 2025',\n",
    "    \n",
    "    # ë¶„ì„\n",
    "    'October 2025 crypto analysis',\n",
    "    'crypto market crash 2025',\n",
    "    'bitcoin crash October',\n",
    "    'ethereum crash October 2025',\n",
    "    \n",
    "    # ì˜í–¥\n",
    "    'crypto liquidation 2025',\n",
    "    'whale liquidation October',\n",
    "    'crypto bloodbath 2025',\n",
    "    'October crypto dump',\n",
    "    \n",
    "    # ë°˜ì‘\n",
    "    'crypto crash reaction October',\n",
    "    'October 2025 trading disaster',\n",
    "    'crypto portfolio destroyed',\n",
    "    \n",
    "    # ì¼ë°˜ í¬ë¦½í†  (ê¸°ê°„ ë‚´)\n",
    "    'bitcoin September 2025',\n",
    "    'ethereum October 2025',\n",
    "    'crypto news October 2025',\n",
    "    'altcoin crash 2025',\n",
    "]\n",
    "\n",
    "# ì €ì¥ ê²½ë¡œ\n",
    "OUTPUT_DIR = './youtube_data_collection'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# YouTube API ì´ˆê¸°í™”\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"YouTube ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ê¸°\")\n",
    "print(\"=\" * 100)\n",
    "print()\n",
    "\n",
    "if API_KEY == 'AIzaSyCKDA_WCioPkAG29BoUOtw7T6KmUH1QQjw':\n",
    "    print(\"âŒ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!\")\n",
    "    print()\n",
    "    print(\"ë°œê¸‰ ë°©ë²•:\")\n",
    "    print(\"1. https://console.cloud.google.com/ ì ‘ì†\")\n",
    "    print(\"2. í”„ë¡œì íŠ¸ ìƒì„±\")\n",
    "    print(\"3. YouTube Data API v3 í™œì„±í™”\")\n",
    "    print(\"4. API í‚¤ ìƒì„±\")\n",
    "    print(\"5. ìœ„ API_KEY ë³€ìˆ˜ì— ë¶™ì—¬ë„£ê¸°\")\n",
    "    print()\n",
    "    exit(1)\n",
    "\n",
    "try:\n",
    "    youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "    print(\"âœ… YouTube API ì—°ê²° ì„±ê³µ!\")\n",
    "    print()\n",
    "except Exception as e:\n",
    "    print(f\"âŒ YouTube API ì—°ê²° ì‹¤íŒ¨: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# ìˆ˜ì§‘ í•¨ìˆ˜ë“¤\n",
    "# ============================================================================\n",
    "\n",
    "def search_videos(keyword, max_results=50):\n",
    "    \"\"\"í‚¤ì›Œë“œë¡œ ì˜ìƒ ê²€ìƒ‰\"\"\"\n",
    "    videos = []\n",
    "    \n",
    "    try:\n",
    "        request = youtube.search().list(\n",
    "            part='snippet',\n",
    "            q=keyword,\n",
    "            type='video',\n",
    "            maxResults=max_results,\n",
    "            order='relevance',\n",
    "            publishedAfter=COLLECTION_CONFIG['date_range']['start'],\n",
    "            publishedBefore=COLLECTION_CONFIG['date_range']['end'],\n",
    "            relevanceLanguage='en',  # ì˜ì–´ ìš°ì„ \n",
    "        )\n",
    "        \n",
    "        response = request.execute()\n",
    "        \n",
    "        for item in response.get('items', []):\n",
    "            video_id = item['id']['videoId']\n",
    "            snippet = item['snippet']\n",
    "            \n",
    "            video_data = {\n",
    "                'video_id': video_id,\n",
    "                'title': snippet.get('title', ''),\n",
    "                'description': snippet.get('description', ''),\n",
    "                'channel_title': snippet.get('channelTitle', ''),\n",
    "                'channel_id': snippet.get('channelId', ''),\n",
    "                'published_at': snippet.get('publishedAt', ''),\n",
    "                'search_keyword': keyword,\n",
    "                'url': f'https://www.youtube.com/watch?v={video_id}',\n",
    "            }\n",
    "            \n",
    "            videos.append(video_data)\n",
    "        \n",
    "        print(f\"    âœ“ '{keyword}' â†’ {len(videos)}ê°œ ì˜ìƒ\")\n",
    "        \n",
    "    except HttpError as e:\n",
    "        print(f\"    âœ— ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    return videos\n",
    "\n",
    "def get_video_details(video_ids):\n",
    "    \"\"\"ì˜ìƒ ìƒì„¸ ì •ë³´ (ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”, ëŒ“ê¸€ ìˆ˜ ë“±)\"\"\"\n",
    "    details = {}\n",
    "    \n",
    "    try:\n",
    "        # 50ê°œì”© ë°°ì¹˜ ì²˜ë¦¬\n",
    "        for i in range(0, len(video_ids), 50):\n",
    "            batch = video_ids[i:i+50]\n",
    "            \n",
    "            request = youtube.videos().list(\n",
    "                part='statistics,contentDetails',\n",
    "                id=','.join(batch)\n",
    "            )\n",
    "            \n",
    "            response = request.execute()\n",
    "            \n",
    "            for item in response.get('items', []):\n",
    "                video_id = item['id']\n",
    "                stats = item.get('statistics', {})\n",
    "                content = item.get('contentDetails', {})\n",
    "                \n",
    "                details[video_id] = {\n",
    "                    'view_count': int(stats.get('viewCount', 0)),\n",
    "                    'like_count': int(stats.get('likeCount', 0)),\n",
    "                    'comment_count': int(stats.get('commentCount', 0)),\n",
    "                    'duration': content.get('duration', ''),\n",
    "                }\n",
    "            \n",
    "            time.sleep(0.5)  # Rate limit\n",
    "        \n",
    "    except HttpError as e:\n",
    "        print(f\"    âœ— ìƒì„¸ ì •ë³´ ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    return details\n",
    "\n",
    "def get_video_comments(video_id, max_comments=100):\n",
    "    \"\"\"ì˜ìƒ ëŒ“ê¸€ ìˆ˜ì§‘\"\"\"\n",
    "    comments = []\n",
    "    \n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part='snippet',\n",
    "            videoId=video_id,\n",
    "            maxResults=min(max_comments, 100),\n",
    "            order='relevance',\n",
    "            textFormat='plainText'\n",
    "        )\n",
    "        \n",
    "        while request and len(comments) < max_comments:\n",
    "            response = request.execute()\n",
    "            \n",
    "            for item in response.get('items', []):\n",
    "                snippet = item['snippet']['topLevelComment']['snippet']\n",
    "                \n",
    "                comment_data = {\n",
    "                    'video_id': video_id,\n",
    "                    'comment_id': item['snippet']['topLevelComment']['id'],\n",
    "                    'text': snippet.get('textDisplay', ''),\n",
    "                    'author': snippet.get('authorDisplayName', ''),\n",
    "                    'like_count': snippet.get('likeCount', 0),\n",
    "                    'published_at': snippet.get('publishedAt', ''),\n",
    "                    'reply_count': item['snippet'].get('totalReplyCount', 0),\n",
    "                }\n",
    "                \n",
    "                comments.append(comment_data)\n",
    "            \n",
    "            # ë‹¤ìŒ í˜ì´ì§€\n",
    "            if 'nextPageToken' in response and len(comments) < max_comments:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part='snippet',\n",
    "                    videoId=video_id,\n",
    "                    pageToken=response['nextPageToken'],\n",
    "                    maxResults=min(max_comments - len(comments), 100),\n",
    "                    order='relevance',\n",
    "                    textFormat='plainText'\n",
    "                )\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "    except HttpError as e:\n",
    "        # ëŒ“ê¸€ ë¹„í™œì„±í™”ëœ ê²½ìš° ë“±\n",
    "        pass\n",
    "    \n",
    "    return comments\n",
    "\n",
    "def get_video_captions(video_id):\n",
    "    \"\"\"ì˜ìƒ ìë§‰(ìº¡ì…˜) ìˆ˜ì§‘\"\"\"\n",
    "    captions = []\n",
    "    \n",
    "    try:\n",
    "        # í•œêµ­ì–´ ë˜ëŠ” ì˜ì–´ ìë§‰ ì‹œë„\n",
    "        for lang in ['ko', 'en']:\n",
    "            try:\n",
    "                transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=[lang])\n",
    "                \n",
    "                for entry in transcript:\n",
    "                    caption_data = {\n",
    "                        'video_id': video_id,\n",
    "                        'text': entry['text'],\n",
    "                        'start': entry['start'],\n",
    "                        'duration': entry['duration'],\n",
    "                        'language': lang,\n",
    "                    }\n",
    "                    captions.append(caption_data)\n",
    "                \n",
    "                # ì„±ê³µí•˜ë©´ ë£¨í”„ íƒˆì¶œ\n",
    "                break\n",
    "                \n",
    "            except (TranscriptsDisabled, NoTranscriptFound):\n",
    "                continue\n",
    "        \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return captions\n",
    "\n",
    "# ============================================================================\n",
    "# ë©”ì¸ ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ìˆ˜ì§‘ ì‹œì‘...\")\n",
    "print(f\"  ê¸°ê°„: {COLLECTION_CONFIG['date_range']['start'][:10]} ~ {COLLECTION_CONFIG['date_range']['end'][:10]}\")\n",
    "print(f\"  ëª©í‘œ: ì˜ìƒ {COLLECTION_CONFIG['target_videos']}ê°œ, ëŒ“ê¸€ {COLLECTION_CONFIG['target_comments']}ê°œ\")\n",
    "print()\n",
    "\n",
    "all_videos = []\n",
    "all_comments = []\n",
    "all_captions = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: ì˜ìƒ ê²€ìƒ‰\n",
    "# ============================================================================\n",
    "print(\"[1/4] ì˜ìƒ ê²€ìƒ‰ ì¤‘...\")\n",
    "\n",
    "for i, keyword in enumerate(SEARCH_KEYWORDS, 1):\n",
    "    print(f\"  [{i}/{len(SEARCH_KEYWORDS)}] ê²€ìƒ‰ ì¤‘...\")\n",
    "    \n",
    "    videos = search_videos(keyword, max_results=50)\n",
    "    all_videos.extend(videos)\n",
    "    \n",
    "    time.sleep(1)  # Rate limit\n",
    "    \n",
    "    # ëª©í‘œ ë‹¬ì„± ì²´í¬\n",
    "    if len(all_videos) >= COLLECTION_CONFIG['target_videos']:\n",
    "        print(f\"  ğŸ¯ ëª©í‘œ ë‹¬ì„±! ({COLLECTION_CONFIG['target_videos']}ê°œ)\")\n",
    "        break\n",
    "\n",
    "# ì¤‘ë³µ ì œê±°\n",
    "videos_df = pd.DataFrame(all_videos)\n",
    "if len(videos_df) > 0:\n",
    "    videos_df = videos_df.drop_duplicates(subset=['video_id'])\n",
    "    all_videos = videos_df.to_dict('records')\n",
    "\n",
    "print(f\"\\nâœ… ì´ {len(all_videos)}ê°œ ì˜ìƒ ë°œê²¬ (ì¤‘ë³µ ì œê±° í›„)\")\n",
    "\n",
    "# ì¤‘ê°„ ì €ì¥\n",
    "videos_file = f\"{OUTPUT_DIR}/videos_metadata.csv\"\n",
    "videos_df.to_csv(videos_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"ğŸ’¾ ì €ì¥: {videos_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: ì˜ìƒ ìƒì„¸ ì •ë³´\n",
    "# ============================================================================\n",
    "print(f\"\\n[2/4] ì˜ìƒ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...\")\n",
    "\n",
    "video_ids = [v['video_id'] for v in all_videos]\n",
    "details = get_video_details(video_ids)\n",
    "\n",
    "# ìƒì„¸ ì •ë³´ ë³‘í•©\n",
    "for video in all_videos:\n",
    "    video_id = video['video_id']\n",
    "    if video_id in details:\n",
    "        video.update(details[video_id])\n",
    "\n",
    "print(f\"âœ… {len(details)}ê°œ ì˜ìƒ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: ëŒ“ê¸€ ìˆ˜ì§‘\n",
    "# ============================================================================\n",
    "print(f\"\\n[3/4] ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘... (ì˜ìƒ {len(all_videos)}ê°œ)\")\n",
    "\n",
    "for i, video in enumerate(all_videos, 1):\n",
    "    video_id = video['video_id']\n",
    "    \n",
    "    # ì§„í–‰ ìƒí™©\n",
    "    if i % 50 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = i / elapsed * 60  # ì˜ìƒ/ë¶„\n",
    "        eta = (len(all_videos) - i) / rate if rate > 0 else 0\n",
    "        print(f\"  ì§„í–‰: {i}/{len(all_videos)} ({i/len(all_videos)*100:.1f}%) | \"\n",
    "              f\"ëŒ“ê¸€: {len(all_comments)}ê°œ | ETA: {eta:.1f}ë¶„\")\n",
    "    \n",
    "    # ëŒ“ê¸€ ìˆ˜ì§‘\n",
    "    comments = get_video_comments(video_id, \n",
    "                                  max_comments=COLLECTION_CONFIG['max_comments_per_video'])\n",
    "    all_comments.extend(comments)\n",
    "    \n",
    "    time.sleep(0.5)  # Rate limit\n",
    "    \n",
    "    # ëª©í‘œ ë‹¬ì„± ì²´í¬\n",
    "    if len(all_comments) >= COLLECTION_CONFIG['target_comments']:\n",
    "        print(f\"  ğŸ¯ ëŒ“ê¸€ ëª©í‘œ ë‹¬ì„±! ({COLLECTION_CONFIG['target_comments']}ê°œ)\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nâœ… ì´ {len(all_comments)}ê°œ ëŒ“ê¸€ ìˆ˜ì§‘\")\n",
    "\n",
    "# ì¤‘ê°„ ì €ì¥\n",
    "if len(all_comments) > 0:\n",
    "    comments_df = pd.DataFrame(all_comments)\n",
    "    comments_file = f\"{OUTPUT_DIR}/comments_temp.csv\"\n",
    "    comments_df.to_csv(comments_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"ğŸ’¾ ì €ì¥: {comments_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: ìë§‰(ìº¡ì…˜) ìˆ˜ì§‘\n",
    "# ============================================================================\n",
    "print(f\"\\n[4/4] ìë§‰ ìˆ˜ì§‘ ì¤‘... (ì˜ìƒ {len(all_videos)}ê°œ)\")\n",
    "\n",
    "caption_videos = 0\n",
    "for i, video in enumerate(all_videos, 1):\n",
    "    video_id = video['video_id']\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(f\"  ì§„í–‰: {i}/{len(all_videos)} | ìë§‰: {len(all_captions)}ê°œ ë¬¸ì¥ | \"\n",
    "              f\"ì„±ê³µ: {caption_videos}ê°œ ì˜ìƒ\")\n",
    "    \n",
    "    # ìë§‰ ìˆ˜ì§‘\n",
    "    captions = get_video_captions(video_id)\n",
    "    \n",
    "    if len(captions) > 0:\n",
    "        all_captions.extend(captions)\n",
    "        caption_videos += 1\n",
    "    \n",
    "    time.sleep(0.3)  # Rate limit\n",
    "    \n",
    "    # ëª©í‘œ ë‹¬ì„± ì²´í¬\n",
    "    if len(all_captions) >= COLLECTION_CONFIG['target_captions']:\n",
    "        print(f\"  ğŸ¯ ìë§‰ ëª©í‘œ ë‹¬ì„±! ({COLLECTION_CONFIG['target_captions']}ê°œ)\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nâœ… ì´ {len(all_captions)}ê°œ ìë§‰ ë¬¸ì¥ ìˆ˜ì§‘ ({caption_videos}ê°œ ì˜ìƒ)\")\n",
    "\n",
    "# ============================================================================\n",
    "# ìµœì¢… ì €ì¥\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ìˆ˜ì§‘ ì™„ë£Œ!\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_text_items = len(all_videos) + len(all_comments) + len(all_captions)\n",
    "\n",
    "print(f\"\\nğŸ“Š ìˆ˜ì§‘ í†µê³„:\")\n",
    "print(f\"  ì˜ìƒ: {len(all_videos)}ê°œ\")\n",
    "print(f\"    - ì œëª©: {len(all_videos)}ê°œ\")\n",
    "print(f\"    - ì„¤ëª…: {len([v for v in all_videos if v.get('description')])}ê°œ\")\n",
    "print(f\"  ëŒ“ê¸€: {len(all_comments)}ê°œ\")\n",
    "print(f\"  ìë§‰: {len(all_captions)}ê°œ ë¬¸ì¥\")\n",
    "print(f\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(f\"  ì´ í…ìŠ¤íŠ¸ í•­ëª©: {total_text_items:,}ê°œ\")\n",
    "print(f\"  ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„\")\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ê¸¸ì´ ê³„ì‚°\n",
    "total_chars = 0\n",
    "if len(all_videos) > 0:\n",
    "    total_chars += sum(len(str(v.get('title', ''))) + len(str(v.get('description', ''))) for v in all_videos)\n",
    "if len(all_comments) > 0:\n",
    "    total_chars += sum(len(str(c.get('text', ''))) for c in all_comments)\n",
    "if len(all_captions) > 0:\n",
    "    total_chars += sum(len(str(c.get('text', ''))) for c in all_captions)\n",
    "\n",
    "print(f\"  ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: {total_chars:,} ê¸€ì\")\n",
    "\n",
    "# ì €ì¥\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# 1. ì˜ìƒ ë©”íƒ€ë°ì´í„°\n",
    "if len(all_videos) > 0:\n",
    "    videos_df = pd.DataFrame(all_videos)\n",
    "    videos_final = f\"{OUTPUT_DIR}/youtube_videos_{timestamp}.csv\"\n",
    "    videos_df.to_csv(videos_final, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nğŸ’¾ ì˜ìƒ ì €ì¥: {videos_final}\")\n",
    "    print(f\"   í¬ê¸°: {os.path.getsize(videos_final) / 1024:.1f} KB\")\n",
    "\n",
    "# 2. ëŒ“ê¸€\n",
    "if len(all_comments) > 0:\n",
    "    comments_df = pd.DataFrame(all_comments)\n",
    "    comments_final = f\"{OUTPUT_DIR}/youtube_comments_{timestamp}.csv\"\n",
    "    comments_df.to_csv(comments_final, index=False, encoding='utf-8-sig')\n",
    "    print(f\"ğŸ’¾ ëŒ“ê¸€ ì €ì¥: {comments_final}\")\n",
    "    print(f\"   í¬ê¸°: {os.path.getsize(comments_final) / 1024:.1f} KB\")\n",
    "\n",
    "# 3. ìë§‰\n",
    "if len(all_captions) > 0:\n",
    "    captions_df = pd.DataFrame(all_captions)\n",
    "    captions_final = f\"{OUTPUT_DIR}/youtube_captions_{timestamp}.csv\"\n",
    "    captions_df.to_csv(captions_final, index=False, encoding='utf-8-sig')\n",
    "    print(f\"ğŸ’¾ ìë§‰ ì €ì¥: {captions_final}\")\n",
    "    print(f\"   í¬ê¸°: {os.path.getsize(captions_final) / 1024:.1f} KB\")\n",
    "\n",
    "# 4. í†µí•© JSON\n",
    "combined_data = {\n",
    "    'metadata': {\n",
    "        'collection_date': timestamp,\n",
    "        'total_videos': len(all_videos),\n",
    "        'total_comments': len(all_comments),\n",
    "        'total_captions': len(all_captions),\n",
    "        'total_text_items': total_text_items,\n",
    "        'duration_minutes': total_time / 60,\n",
    "        'date_range': COLLECTION_CONFIG['date_range'],\n",
    "    },\n",
    "    'videos': all_videos,\n",
    "    'comments': all_comments,\n",
    "    'captions': all_captions,\n",
    "}\n",
    "\n",
    "json_file = f\"{OUTPUT_DIR}/youtube_collection_{timestamp}.json\"\n",
    "with open(json_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(combined_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"ğŸ’¾ JSON ì €ì¥: {json_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ë¶„ì„ ë¦¬í¬íŠ¸\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ë°ì´í„° ë¶„ì„ ë¦¬í¬íŠ¸\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "if len(all_videos) > 0:\n",
    "    print(\"\\nğŸ“¹ ì˜ìƒ ë¶„ì„:\")\n",
    "    print(f\"  í‰ê·  ì¡°íšŒìˆ˜: {videos_df['view_count'].mean():,.0f}\")\n",
    "    print(f\"  í‰ê·  ì¢‹ì•„ìš”: {videos_df['like_count'].mean():,.0f}\")\n",
    "    print(f\"  í‰ê·  ëŒ“ê¸€ìˆ˜: {videos_df['comment_count'].mean():,.0f}\")\n",
    "    print(f\"\\n  ìƒìœ„ ì±„ë„:\")\n",
    "    for channel, count in videos_df['channel_title'].value_counts().head(5).items():\n",
    "        print(f\"    - {channel}: {count}ê°œ\")\n",
    "\n",
    "if len(all_comments) > 0:\n",
    "    print(f\"\\nğŸ’¬ ëŒ“ê¸€ ë¶„ì„:\")\n",
    "    print(f\"  í‰ê·  ê¸¸ì´: {comments_df['text'].str.len().mean():.0f} ê¸€ì\")\n",
    "    print(f\"  í‰ê·  ì¢‹ì•„ìš”: {comments_df['like_count'].mean():.1f}\")\n",
    "    print(f\"  ë‹µê¸€ ìˆëŠ” ëŒ“ê¸€: {len(comments_df[comments_df['reply_count'] > 0])}ê°œ\")\n",
    "\n",
    "if len(all_captions) > 0:\n",
    "    print(f\"\\nğŸ“ ìë§‰ ë¶„ì„:\")\n",
    "    print(f\"  ìë§‰ ìˆëŠ” ì˜ìƒ: {caption_videos}ê°œ ({caption_videos/len(all_videos)*100:.1f}%)\")\n",
    "    print(f\"  í‰ê·  ë¬¸ì¥ ê¸¸ì´: {captions_df['text'].str.len().mean():.0f} ê¸€ì\")\n",
    "    print(f\"  ì–¸ì–´ ë¶„í¬:\")\n",
    "    for lang, count in captions_df['language'].value_counts().items():\n",
    "        lang_name = 'Korean' if lang == 'ko' else 'English'\n",
    "        print(f\"    - {lang_name}: {count}ê°œ\")\n",
    "\n",
    "# ëª©í‘œ ë‹¬ì„± í™•ì¸\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ëª©í‘œ ë‹¬ì„± í˜„í™©\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "target_total = 15000  # ëª©í‘œ í…ìŠ¤íŠ¸ ìˆ˜\n",
    "achieved = total_text_items\n",
    "\n",
    "print(f\"\\nğŸ¯ ëª©í‘œ: 10,000 - 20,000ê°œ í…ìŠ¤íŠ¸ ë°ì´í„°\")\n",
    "print(f\"âœ… ë‹¬ì„±: {achieved:,}ê°œ\")\n",
    "\n",
    "if achieved >= 10000:\n",
    "    print(f\"ğŸ‰ ëª©í‘œ ë‹¬ì„±! ({achieved/target_total*100:.1f}%)\")\n",
    "else:\n",
    "    remaining = 10000 - achieved\n",
    "    print(f\"âš ï¸  {remaining:,}ê°œ ë” í•„ìš”\")\n",
    "\n",
    "print(f\"\\nğŸ“‚ ì €ì¥ ìœ„ì¹˜: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "print(\"\\nâœ… ìˆ˜ì§‘ ì™„ë£Œ! íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59839b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "YouTube ë°ì´í„° ë¶„ì„ ì‹œì‘\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“‚ ì˜ìƒ ë°ì´í„°: youtube_videos_20260202_174228.csv\n",
      "ğŸ“‚ ëŒ“ê¸€ ë°ì´í„°: youtube_comments_20260202_174228.csv\n",
      "\n",
      "âœ… ì˜ìƒ: 719ê°œ\n",
      "âœ… ëŒ“ê¸€: 15060ê°œ\n",
      "\n",
      "[1/8] ë°ì´í„° ì „ì²˜ë¦¬...\n",
      "âœ… ì „ì²˜ë¦¬ ë° ë³‘í•© ì™„ë£Œ\n",
      "   - ë³‘í•©ëœ ë°ì´í„° í¬ê¸°: (719, 17)\n",
      "   - comment_count ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€: True\n",
      "\n",
      "[2/8] ì±„ë„ ë¶„ì„...\n",
      "âœ… ì €ì¥: 01_channel_analysis.png\n",
      "\n",
      "[3/8] ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„...\n",
      "âœ… ì €ì¥: 02_time_trends.png\n",
      "\n",
      "[4/8] ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±...\n",
      "âœ… ì €ì¥: 03_wordclouds.png\n",
      "\n",
      "[5/8] ì°¸ì—¬ë„ ë¶„ì„...\n",
      "âœ… ì €ì¥: 04_engagement_analysis.png\n",
      "\n",
      "[6/8] í‚¤ì›Œë“œ ë¶„ì„...\n",
      "âœ… ì €ì¥: 05_keyword_analysis.png\n",
      "\n",
      "[7/8] ê²€ìƒ‰ í‚¤ì›Œë“œ íš¨ê³¼ ë¶„ì„...\n",
      "âœ… ì €ì¥: 06_search_keyword_performance.png\n",
      "\n",
      "[8/8] ì¢…í•© ëŒ€ì‹œë³´ë“œ ìƒì„±...\n",
      "âœ… ì €ì¥: 07_comprehensive_dashboard.png\n",
      "\n",
      "====================================================================================================\n",
      "ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ì™„ë£Œ!\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "YOUTUBE DATA INSIGHTS REPORT\n",
      "October 2025 Crypto Crash Analysis\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“Š ë°ì´í„° ê°œìš”\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  â€¢ ì´ ì˜ìƒ: 719ê°œ\n",
      "  â€¢ ì´ ëŒ“ê¸€: 15,060ê°œ\n",
      "  â€¢ ë¶„ì„ ê¸°ê°„: 2025-09-01 ~ 2025-10-31\n",
      "  â€¢ ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: 1,292,575.0 ê¸€ì\n",
      "\n",
      "ğŸ† ì£¼ìš” ë°œê²¬ì‚¬í•­\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "1. ê°€ì¥ í™œë°œí•œ ì±„ë„\n",
      "   â†’ Crypto Nutshell (13ê°œ ì˜ìƒ)\n",
      "   â†’ í‰ê·  ì¡°íšŒìˆ˜: 31,988\n",
      "\n",
      "2. ê°€ì¥ ì¸ê¸°ìˆëŠ” ê²€ìƒ‰ í‚¤ì›Œë“œ\n",
      "   â†’ October 2025 crypto crash\n",
      "   â†’ ì˜ìƒ ìˆ˜: 50ê°œ\n",
      "\n",
      "3. í‰ê·  ì°¸ì—¬ë„\n",
      "   â†’ ì¡°íšŒìˆ˜: 248,808 (ì¤‘ì•™ê°’: 1,140)\n",
      "   â†’ ì¢‹ì•„ìš”: 6,399 (ì¤‘ì•™ê°’: 16)\n",
      "   â†’ ëŒ“ê¸€: 20.9 (ì¤‘ì•™ê°’: 0.0)\n",
      "\n",
      "4. ìµœê³  ì¡°íšŒìˆ˜ ì˜ìƒ\n",
      "   â†’ How Chicago Raised Its Buildings ğŸ˜®\n",
      "   â†’ ì¡°íšŒìˆ˜: 43,927,000\n",
      "   â†’ ì±„ë„: Zack D. Films\n",
      "\n",
      "5. ê°€ì¥ ë§ì€ ëŒ“ê¸€ì„ ë°›ì€ ì˜ìƒ\n",
      "   â†’ ğŸš¨OCT 10TH 2025 CRASH The Last WarningğŸš¨\n",
      "   â†’ ëŒ“ê¸€: 100ê°œ\n",
      "\n",
      "ğŸ“ˆ ì‹œê°„ëŒ€ë³„ íŠ¸ë Œë“œ\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  â€¢ ê°€ì¥ í™œë°œí•œ ë‚ : 2025-10-11 (118ê°œ ì˜ìƒ)\n",
      "  â€¢ ì¼í‰ê·  ì—…ë¡œë“œ: 11.8ê°œ\n",
      "\n",
      "ğŸ’¬ ëŒ“ê¸€ ë¶„ì„\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  â€¢ í‰ê·  ëŒ“ê¸€ ê¸¸ì´: 78ì\n",
      "  â€¢ ê°€ì¥ ì¢‹ì•„ìš” ë§ì€ ëŒ“ê¸€: 99371ê°œ\n",
      "  â€¢ ë‹µê¸€ ìˆëŠ” ëŒ“ê¸€: 3619ê°œ (24.0%)\n",
      "\n",
      "ğŸ”‘ í•µì‹¬ í‚¤ì›Œë“œ TOP 5\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  1. Crash: 218ê°œ (30.3%)\n",
      "  2. October: 165ê°œ (22.9%)\n",
      "  3. Liquidation: 99ê°œ (13.8%)\n",
      "  4. Trump: 66ê°œ (9.2%)\n",
      "  5. $19B: 49ê°œ (6.8%)\n",
      "\n",
      "ğŸ’¡ ì¸ì‚¬ì´íŠ¸ & ê²°ë¡ \n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "1. ì½˜í…ì¸  ì§‘ì¤‘ë„\n",
      "   â€¢ October 2025 í¬ë˜ì‹œëŠ” 719ê°œì˜ ì˜ìƒì„ ìƒì„±í–ˆìœ¼ë©°,\n",
      "     ì´ëŠ” crypto ì»¤ë®¤ë‹ˆí‹°ì˜ ë†’ì€ ê´€ì‹¬ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
      "\n",
      "2. ì±„ë„ ë‹¤ì–‘ì„±\n",
      "   â€¢ 545ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ì±„ë„ì´ ì°¸ì—¬\n",
      "   â€¢ ìƒìœ„ 5ê°œ ì±„ë„ì´ 42ê°œ (5.8%)ì˜ ì½˜í…ì¸  ìƒì‚°\n",
      "\n",
      "3. ì°¸ì—¬ë„ íŒ¨í„´\n",
      "   â€¢ ì¢‹ì•„ìš”ìœ¨(Like Ratio) ì¤‘ì•™ê°’: 2.16%\n",
      "   â€¢ ë†’ì€ ì°¸ì—¬ë„ëŠ” ì»¤ë®¤ë‹ˆí‹°ì˜ ê°•í•œ ë°˜ì‘ì„ ì‹œì‚¬\n",
      "\n",
      "4. í‚¤ì›Œë“œ ì¸ì‚¬ì´íŠ¸\n",
      "   â€¢ 'Crash'ê°€ ê°€ì¥ ìì£¼ ì–¸ê¸‰ë¨ (218íšŒ)\n",
      "   â€¢ 'Crash', 'Liquidation', 'October' ë“±ì˜ í‚¤ì›Œë“œê°€ ì§€ë°°ì \n",
      "\n",
      "5. ì‹œê°„ì  ë¶„í¬\n",
      "   â€¢ íŠ¹ì • ë‚ ì§œ(2025-10-11)ì— \n",
      "     ì½˜í…ì¸  ìƒì‚°ì´ ì§‘ì¤‘ë¨ â†’ ì´ë²¤íŠ¸ì˜ ì¦‰ê°ì  ì˜í–¥\n",
      "\n",
      "====================================================================================================\n",
      "ë¦¬í¬íŠ¸ ìƒì„± ì¼ì‹œ: 2026-02-02 17:56:26\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "ğŸ’¾ ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸: c:\\junwoo\\AI_Project_01_Team6\\data\\Youtube_data\\youtube_data_collection\\visualizations/INSIGHTS_REPORT.txt\n",
      "\n",
      "====================================================================================================\n",
      "ìƒì„±ëœ íŒŒì¼\n",
      "====================================================================================================\n",
      "  1. 01_channel_analysis.png\n",
      "  2. 02_time_trends.png\n",
      "  3. 03_wordclouds.png\n",
      "  4. 04_engagement_analysis.png\n",
      "  5. 05_keyword_analysis.png\n",
      "  6. 06_search_keyword_performance.png\n",
      "  7. 07_comprehensive_dashboard.png\n",
      "  8. INSIGHTS_REPORT.txt\n",
      "\n",
      "ğŸ“‚ ì €ì¥ ìœ„ì¹˜: c:\\junwoo\\AI_Project_01_Team6\\data\\Youtube_data\\youtube_data_collection\\visualizations\n",
      "\n",
      "âœ… ëª¨ë“  ë¶„ì„ ì™„ë£Œ! íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "YouTube ë°ì´í„° ì‹œê°í™” & ì¸ì‚¬ì´íŠ¸ ë¶„ì„\n",
    "15,779ê°œ í…ìŠ¤íŠ¸ ë°ì´í„° ê¸°ë°˜\n",
    "\n",
    "ìƒì„± ì°¨íŠ¸:\n",
    "1. ì˜ìƒ ë¶„í¬ (ì±„ë„ë³„, ì‹œê°„ë³„)\n",
    "2. ëŒ“ê¸€ ê°ì • ë¶„ì„\n",
    "3. ì›Œë“œí´ë¼ìš°ë“œ (ì œëª©, ëŒ“ê¸€)\n",
    "4. ì°¸ì—¬ë„ ë¶„ì„ (ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”)\n",
    "5. ì‹œê°„ëŒ€ë³„ íŠ¸ë Œë“œ\n",
    "6. í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬\n",
    "7. ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "try:\n",
    "    import koreanize_matplotlib\n",
    "    koreanize_matplotlib.matplotlib_settings()\n",
    "except:\n",
    "    plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ============================================================================\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"YouTube ë°ì´í„° ë¶„ì„ ì‹œì‘\")\n",
    "print(\"=\" * 100)\n",
    "print()\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "DATA_DIR = r'c:\\junwoo\\AI_Project_01_Team6\\data\\Youtube_data\\youtube_data_collection'\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, 'visualizations')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ìµœì‹  íŒŒì¼ ì°¾ê¸°\n",
    "video_files = [f for f in os.listdir(DATA_DIR) if f.startswith('youtube_videos_') and f.endswith('.csv')]\n",
    "comment_files = [f for f in os.listdir(DATA_DIR) if f.startswith('youtube_comments_') and f.endswith('.csv')]\n",
    "\n",
    "if not video_files or not comment_files:\n",
    "    print(\"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "    print(f\"   ê²½ë¡œ: {DATA_DIR}\")\n",
    "    exit(1)\n",
    "\n",
    "video_file = os.path.join(DATA_DIR, sorted(video_files)[-1])\n",
    "comment_file = os.path.join(DATA_DIR, sorted(comment_files)[-1])\n",
    "\n",
    "print(f\"ğŸ“‚ ì˜ìƒ ë°ì´í„°: {os.path.basename(video_file)}\")\n",
    "print(f\"ğŸ“‚ ëŒ“ê¸€ ë°ì´í„°: {os.path.basename(comment_file)}\")\n",
    "print()\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "df_videos = pd.read_csv(video_file)\n",
    "df_comments = pd.read_csv(comment_file)\n",
    "\n",
    "print(f\"âœ… ì˜ìƒ: {len(df_videos)}ê°œ\")\n",
    "print(f\"âœ… ëŒ“ê¸€: {len(df_comments)}ê°œ\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ (ìˆ˜ì •ë¨)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"[1/8] ë°ì´í„° ì „ì²˜ë¦¬...\")\n",
    "\n",
    "# 1. Timezone ê²½ê³  í•´ê²°: ë‚ ì§œë¥¼ timezone-naiveí•˜ê²Œ ë³€í™˜ í›„ ì²˜ë¦¬\n",
    "# published_atì„ datetimeìœ¼ë¡œ ë³€í™˜ (utc=Trueë¡œ í–ˆë‹¤ê°€ timezone ì œê±°)\n",
    "df_videos['published_date'] = pd.to_datetime(df_videos['published_at'], errors='coerce', utc=True)\n",
    "df_videos['published_date'] = df_videos['published_date'].dt.tz_localize(None) # íƒ€ì„ì¡´ ì •ë³´ ì œê±°\n",
    "\n",
    "df_videos['published_day'] = df_videos['published_date'].dt.date\n",
    "df_videos['published_month'] = df_videos['published_date'].dt.to_period('M')\n",
    "\n",
    "# ëŒ“ê¸€ ë‚ ì§œë„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬\n",
    "df_comments['published_date'] = pd.to_datetime(df_comments['published_at'], errors='coerce', utc=True)\n",
    "df_comments['published_date'] = df_comments['published_date'].dt.tz_localize(None)\n",
    "df_comments['published_day'] = df_comments['published_date'].dt.date\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ê¸¸ì´\n",
    "df_videos['title_length'] = df_videos['title'].str.len()\n",
    "df_videos['desc_length'] = df_videos['description'].fillna('').str.len()\n",
    "df_comments['text_length'] = df_comments['text'].str.len()\n",
    "\n",
    "# 2. KeyError í•´ê²°: ì•ˆì „í•œ ë³‘í•© ë¡œì§\n",
    "# ë§Œì•½ df_videosì— ì´ë¯¸ 'comment_count' ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ì‚­ì œ (ì¶©ëŒ ë°©ì§€)\n",
    "if 'comment_count' in df_videos.columns:\n",
    "    df_videos = df_videos.drop(columns=['comment_count'])\n",
    "\n",
    "# video_id íƒ€ì… í†µì¼ (ë§¤ì¹­ ì‹¤íŒ¨ ë°©ì§€)\n",
    "df_videos['video_id'] = df_videos['video_id'].astype(str).str.strip()\n",
    "df_comments['video_id'] = df_comments['video_id'].astype(str).str.strip()\n",
    "\n",
    "# ëŒ“ê¸€ ìˆ˜ ì§‘ê³„\n",
    "video_comments = df_comments.groupby('video_id').size().reset_index(name='comment_count')\n",
    "\n",
    "# ë³‘í•© ìˆ˜í–‰\n",
    "df_videos_merged = df_videos.merge(video_comments, on='video_id', how='left')\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ ì±„ìš°ê¸° (ì´ì œ ì•ˆì „í•¨)\n",
    "df_videos_merged['comment_count'] = df_videos_merged['comment_count'].fillna(0)\n",
    "\n",
    "print(\"âœ… ì „ì²˜ë¦¬ ë° ë³‘í•© ì™„ë£Œ\")\n",
    "print(f\"   - ë³‘í•©ëœ ë°ì´í„° í¬ê¸°: {df_videos_merged.shape}\")\n",
    "print(f\"   - comment_count ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€: {'comment_count' in df_videos_merged.columns}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. ì±„ë„ ë¶„ì„\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[2/8] ì±„ë„ ë¶„ì„...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# ìƒìœ„ ì±„ë„ (ì˜ìƒ ìˆ˜)\n",
    "top_channels = df_videos['channel_title'].value_counts().head(15)\n",
    "axes[0, 0].barh(range(len(top_channels)), top_channels.values,\n",
    "                color=plt.cm.viridis(np.linspace(0, 1, len(top_channels))),\n",
    "                edgecolor='black', linewidth=0.8)\n",
    "axes[0, 0].set_yticks(range(len(top_channels)))\n",
    "axes[0, 0].set_yticklabels(top_channels.index, fontsize=9)\n",
    "axes[0, 0].set_xlabel('Video Count', fontsize=11)\n",
    "axes[0, 0].set_title('Top 15 Channels by Video Count', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# ì±„ë„ë³„ í‰ê·  ì¡°íšŒìˆ˜\n",
    "channel_views = df_videos.groupby('channel_title')['view_count'].mean().sort_values(ascending=False).head(10)\n",
    "axes[0, 1].barh(range(len(channel_views)), channel_views.values,\n",
    "                color=plt.cm.plasma(np.linspace(0, 1, len(channel_views))),\n",
    "                edgecolor='black', linewidth=0.8)\n",
    "axes[0, 1].set_yticks(range(len(channel_views)))\n",
    "axes[0, 1].set_yticklabels(channel_views.index, fontsize=9)\n",
    "axes[0, 1].set_xlabel('Average Views', fontsize=11)\n",
    "axes[0, 1].set_title('Top 10 Channels by Avg Views', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# ì¡°íšŒìˆ˜ vs ì¢‹ì•„ìš” (ìƒìœ„ ì±„ë„)\n",
    "top_10_channels = df_videos['channel_title'].value_counts().head(10).index\n",
    "df_top = df_videos[df_videos['channel_title'].isin(top_10_channels)]\n",
    "\n",
    "for channel in top_10_channels:\n",
    "    channel_data = df_top[df_top['channel_title'] == channel]\n",
    "    axes[1, 0].scatter(channel_data['view_count'], channel_data['like_count'],\n",
    "                      alpha=0.6, s=50, label=channel)\n",
    "\n",
    "axes[1, 0].set_xlabel('View Count', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Like Count', fontsize=11)\n",
    "axes[1, 0].set_title('Views vs Likes (Top 10 Channels)', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=7, loc='upper left')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# ì±„ë„ë³„ ëŒ“ê¸€ ìˆ˜\n",
    "video_comments = df_comments.groupby('video_id').size().reset_index(name='comment_count')\n",
    "df_videos_merged = df_videos.merge(video_comments, on='video_id', how='left')\n",
    "df_videos_merged['comment_count'] = df_videos_merged['comment_count'].fillna(0)\n",
    "\n",
    "channel_comments = df_videos_merged.groupby('channel_title')['comment_count'].sum().sort_values(ascending=False).head(10)\n",
    "axes[1, 1].barh(range(len(channel_comments)), channel_comments.values,\n",
    "                color=plt.cm.coolwarm(np.linspace(0, 1, len(channel_comments))),\n",
    "                edgecolor='black', linewidth=0.8)\n",
    "axes[1, 1].set_yticks(range(len(channel_comments)))\n",
    "axes[1, 1].set_yticklabels(channel_comments.index, fontsize=9)\n",
    "axes[1, 1].set_xlabel('Total Comments', fontsize=11)\n",
    "axes[1, 1].set_title('Top 10 Channels by Total Comments', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/01_channel_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… ì €ì¥: 01_channel_analysis.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. ì‹œê°„ íŠ¸ë Œë“œ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[3/8] ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# ì¼ë³„ ì˜ìƒ ì—…ë¡œë“œ ìˆ˜\n",
    "daily_videos = df_videos.groupby('published_day').size().reset_index(name='count')\n",
    "daily_videos = daily_videos.sort_values('published_day')\n",
    "\n",
    "axes[0, 0].plot(daily_videos['published_day'], daily_videos['count'], \n",
    "               marker='o', linewidth=2, markersize=4, color='#1f77b4')\n",
    "axes[0, 0].fill_between(daily_videos['published_day'], daily_videos['count'], alpha=0.3)\n",
    "axes[0, 0].set_xlabel('Date', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Video Count', fontsize=11)\n",
    "axes[0, 0].set_title('Daily Video Upload Trend (Sep-Oct 2025)', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# ëˆ„ì  ì˜ìƒ ìˆ˜\n",
    "daily_videos['cumulative'] = daily_videos['count'].cumsum()\n",
    "axes[0, 1].plot(daily_videos['published_day'], daily_videos['cumulative'],\n",
    "               linewidth=3, color='#ff7f0e')\n",
    "axes[0, 1].set_xlabel('Date', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Cumulative Videos', fontsize=11)\n",
    "axes[0, 1].set_title('Cumulative Video Count', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# ì›”ë³„ í‰ê·  ì¡°íšŒìˆ˜\n",
    "monthly_views = df_videos.groupby('published_month')['view_count'].mean()\n",
    "axes[1, 0].bar(range(len(monthly_views)), monthly_views.values,\n",
    "              color=plt.cm.Spectral(np.linspace(0, 1, len(monthly_views))),\n",
    "              edgecolor='black', linewidth=1)\n",
    "axes[1, 0].set_xticks(range(len(monthly_views)))\n",
    "axes[1, 0].set_xticklabels([str(m) for m in monthly_views.index], rotation=0)\n",
    "axes[1, 0].set_ylabel('Average Views', fontsize=11)\n",
    "axes[1, 0].set_title('Average Views by Month', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# ì¼ë³„ ëŒ“ê¸€ ì¶”ì„¸\n",
    "daily_comments = df_comments.groupby('published_day').size().reset_index(name='count')\n",
    "daily_comments = daily_comments.sort_values('published_day')\n",
    "\n",
    "axes[1, 1].plot(daily_comments['published_day'], daily_comments['count'],\n",
    "               marker='s', linewidth=2, markersize=4, color='#2ca02c')\n",
    "axes[1, 1].fill_between(daily_comments['published_day'], daily_comments['count'], alpha=0.3, color='#2ca02c')\n",
    "axes[1, 1].set_xlabel('Date', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Comment Count', fontsize=11)\n",
    "axes[1, 1].set_title('Daily Comment Activity', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/02_time_trends.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… ì €ì¥: 02_time_trends.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. ì›Œë“œí´ë¼ìš°ë“œ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[4/8] ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±...\")\n",
    "\n",
    "stopwords = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
    "                'of', 'with', 'by', 'from', 'is', 'was', 'are', 'were', 'been', 'be',\n",
    "                'crypto', 'bitcoin', 'btc', 'ethereum', 'eth'])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "# ì˜ìƒ ì œëª©\n",
    "titles_text = ' '.join(df_videos['title'].dropna().astype(str))\n",
    "wc_titles = WordCloud(width=800, height=400, background_color='white',\n",
    "                     stopwords=stopwords, colormap='viridis',\n",
    "                     max_words=100, relative_scaling=0.5).generate(titles_text)\n",
    "axes[0, 0].imshow(wc_titles, interpolation='bilinear')\n",
    "axes[0, 0].axis('off')\n",
    "axes[0, 0].set_title('Video Titles Word Cloud', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# ì˜ìƒ ì„¤ëª…\n",
    "desc_text = ' '.join(df_videos['description'].dropna().astype(str))\n",
    "wc_desc = WordCloud(width=800, height=400, background_color='white',\n",
    "                   stopwords=stopwords, colormap='plasma',\n",
    "                   max_words=80, relative_scaling=0.5).generate(desc_text)\n",
    "axes[0, 1].imshow(wc_desc, interpolation='bilinear')\n",
    "axes[0, 1].axis('off')\n",
    "axes[0, 1].set_title('Video Descriptions Word Cloud', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# ëŒ“ê¸€ (ì „ì²´)\n",
    "comments_text = ' '.join(df_comments['text'].dropna().astype(str))\n",
    "wc_comments = WordCloud(width=800, height=400, background_color='white',\n",
    "                       stopwords=stopwords, colormap='coolwarm',\n",
    "                       max_words=100, relative_scaling=0.5).generate(comments_text)\n",
    "axes[1, 0].imshow(wc_comments, interpolation='bilinear')\n",
    "axes[1, 0].axis('off')\n",
    "axes[1, 0].set_title('All Comments Word Cloud', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# ì¸ê¸° ëŒ“ê¸€ (ì¢‹ì•„ìš” 100+)\n",
    "popular_comments = df_comments[df_comments['like_count'] >= 100]\n",
    "if len(popular_comments) > 0:\n",
    "    popular_text = ' '.join(popular_comments['text'].dropna().astype(str))\n",
    "    wc_popular = WordCloud(width=800, height=400, background_color='white',\n",
    "                          stopwords=stopwords, colormap='inferno',\n",
    "                          max_words=80, relative_scaling=0.5).generate(popular_text)\n",
    "    axes[1, 1].imshow(wc_popular, interpolation='bilinear')\n",
    "    axes[1, 1].axis('off')\n",
    "    axes[1, 1].set_title('Popular Comments (100+ Likes)', fontsize=16, fontweight='bold', pad=20)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'No comments with 100+ likes', \n",
    "                   ha='center', va='center', fontsize=14)\n",
    "    axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/03_wordclouds.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… ì €ì¥: 03_wordclouds.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 4. ì°¸ì—¬ë„ ë¶„ì„\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[5/8] ì°¸ì—¬ë„ ë¶„ì„...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# ì¡°íšŒìˆ˜ ë¶„í¬\n",
    "axes[0, 0].hist(df_videos['view_count'], bins=50, color='skyblue', \n",
    "               edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(df_videos['view_count'].median(), color='red', \n",
    "                  linestyle='--', linewidth=2, label=f'Median: {df_videos[\"view_count\"].median():,.0f}')\n",
    "axes[0, 0].set_xlabel('View Count', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0, 0].set_title('View Count Distribution', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# ì¢‹ì•„ìš”ìœ¨ (Like Ratio)\n",
    "df_videos['like_ratio'] = (df_videos['like_count'] / df_videos['view_count'] * 100).replace([np.inf, -np.inf], np.nan)\n",
    "axes[0, 1].hist(df_videos['like_ratio'].dropna(), bins=50, color='lightcoral',\n",
    "               edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].axvline(df_videos['like_ratio'].median(), color='darkred',\n",
    "                  linestyle='--', linewidth=2, label=f'Median: {df_videos[\"like_ratio\"].median():.2f}%')\n",
    "axes[0, 1].set_xlabel('Like Ratio (%)', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0, 1].set_title('Like Ratio Distribution', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# ëŒ“ê¸€ ê¸¸ì´ ë¶„í¬\n",
    "axes[1, 0].hist(df_comments['text_length'], bins=50, color='lightgreen',\n",
    "               edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].axvline(df_comments['text_length'].median(), color='darkgreen',\n",
    "                  linestyle='--', linewidth=2, label=f'Median: {df_comments[\"text_length\"].median():.0f}')\n",
    "axes[1, 0].set_xlabel('Comment Length (characters)', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1, 0].set_title('Comment Length Distribution', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# ëŒ“ê¸€ ì¢‹ì•„ìš” ë¶„í¬\n",
    "comment_likes_filtered = df_comments[df_comments['like_count'] <= df_comments['like_count'].quantile(0.95)]\n",
    "axes[1, 1].hist(comment_likes_filtered['like_count'], bins=50, color='gold',\n",
    "               edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].axvline(df_comments['like_count'].median(), color='orange',\n",
    "                  linestyle='--', linewidth=2, label=f'Median: {df_comments[\"like_count\"].median():.0f}')\n",
    "axes[1, 1].set_xlabel('Comment Likes', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1, 1].set_title('Comment Likes Distribution (95th percentile)', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/04_engagement_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… ì €ì¥: 04_engagement_analysis.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 5. í‚¤ì›Œë“œ ë¶„ì„\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[6/8] í‚¤ì›Œë“œ ë¶„ì„...\")\n",
    "\n",
    "# ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "def extract_keywords(text):\n",
    "    keywords = []\n",
    "    patterns = {\n",
    "        'October': r'\\boctober\\b',\n",
    "        'Crash': r'\\bcrash\\b',\n",
    "        'Liquidation': r'\\bliquidat',\n",
    "        '$19B': r'\\$19\\s?b',\n",
    "        'Trump': r'\\btrump\\b',\n",
    "        'Binance': r'\\bbinance\\b',\n",
    "        'Hyperliquid': r'\\bhyperliquid\\b',\n",
    "        'Flash Crash': r'\\bflash\\s?crash\\b',\n",
    "        'Whale': r'\\bwhale\\b',\n",
    "        'Bloodbath': r'\\bbloodbath\\b',\n",
    "    }\n",
    "    \n",
    "    text_lower = str(text).lower()\n",
    "    for keyword, pattern in patterns.items():\n",
    "        if re.search(pattern, text_lower):\n",
    "            keywords.append(keyword)\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "df_videos['keywords'] = df_videos['title'].apply(extract_keywords)\n",
    "\n",
    "# í‚¤ì›Œë“œ ë¹ˆë„\n",
    "all_keywords = []\n",
    "for kws in df_videos['keywords']:\n",
    "    all_keywords.extend(kws)\n",
    "\n",
    "keyword_counts = Counter(all_keywords)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "if len(keyword_counts) > 0:\n",
    "    keywords, counts = zip(*keyword_counts.most_common(15))\n",
    "    \n",
    "    ax.barh(range(len(keywords)), counts,\n",
    "           color=plt.cm.rainbow(np.linspace(0, 1, len(keywords))),\n",
    "           edgecolor='black', linewidth=1)\n",
    "    ax.set_yticks(range(len(keywords)))\n",
    "    ax.set_yticklabels(keywords, fontsize=11)\n",
    "    ax.set_xlabel('Frequency', fontsize=12)\n",
    "    ax.set_title('Top 15 Keywords in Video Titles', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    for i, count in enumerate(counts):\n",
    "        ax.text(count + 1, i, str(count), va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/05_keyword_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… ì €ì¥: 05_keyword_analysis.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 6. ê²€ìƒ‰ í‚¤ì›Œë“œ íš¨ê³¼\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[7/8] ê²€ìƒ‰ í‚¤ì›Œë“œ íš¨ê³¼ ë¶„ì„...\")\n",
    "\n",
    "# ìˆ˜ì •: df_videos -> df_videos_merged ë¡œ ë³€ê²½\n",
    "# (df_videos_mergedì—ë§Œ 'comment_count' ì»¬ëŸ¼ì´ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤)\n",
    "if 'df_videos_merged' not in locals():\n",
    "    # í˜¹ì‹œ ì• ë‹¨ê³„ì—ì„œ merged ë³€ìˆ˜ê°€ ì œëŒ€ë¡œ ìƒì„± ì•ˆ ëì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì•ˆì „ì¥ì¹˜\n",
    "    video_comments_count = df_comments.groupby('video_id').size().reset_index(name='comment_count')\n",
    "    df_videos_merged = df_videos.merge(video_comments_count, on='video_id', how='left')\n",
    "    df_videos_merged['comment_count'] = df_videos_merged['comment_count'].fillna(0)\n",
    "\n",
    "# ì´ì œ df_videos_mergedë¥¼ ì‚¬ìš©í•˜ì—¬ ì§‘ê³„í•©ë‹ˆë‹¤\n",
    "search_keyword_stats = df_videos_merged.groupby('search_keyword').agg({\n",
    "    'video_id': 'count',\n",
    "    'view_count': 'mean',\n",
    "    'like_count': 'mean',\n",
    "    'comment_count': 'mean' \n",
    "}).rename(columns={'video_id': 'video_count'})\n",
    "\n",
    "search_keyword_stats = search_keyword_stats.sort_values('video_count', ascending=False).head(15)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# í‚¤ì›Œë“œë³„ ì˜ìƒ ìˆ˜\n",
    "axes[0, 0].barh(range(len(search_keyword_stats)), search_keyword_stats['video_count'],\n",
    "                color=plt.cm.tab20(np.linspace(0, 1, len(search_keyword_stats))),\n",
    "                edgecolor='black', linewidth=0.8)\n",
    "axes[0, 0].set_yticks(range(len(search_keyword_stats)))\n",
    "axes[0, 0].set_yticklabels(search_keyword_stats.index, fontsize=9)\n",
    "axes[0, 0].set_xlabel('Video Count', fontsize=11)\n",
    "axes[0, 0].set_title('Videos by Search Keyword (Top 15)', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# í‚¤ì›Œë“œë³„ í‰ê·  ì¡°íšŒìˆ˜\n",
    "search_views = search_keyword_stats.sort_values('view_count', ascending=False)\n",
    "axes[0, 1].barh(range(len(search_views)), search_views['view_count'],\n",
    "                color=plt.cm.viridis(np.linspace(0, 1, len(search_views))),\n",
    "                edgecolor='black', linewidth=0.8)\n",
    "axes[0, 1].set_yticks(range(len(search_views)))\n",
    "axes[0, 1].set_yticklabels(search_views.index, fontsize=9)\n",
    "axes[0, 1].set_xlabel('Avg Views', fontsize=11)\n",
    "axes[0, 1].set_title('Average Views by Search Keyword', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# í‚¤ì›Œë“œë³„ í‰ê·  ì¢‹ì•„ìš”\n",
    "search_likes = search_keyword_stats.sort_values('like_count', ascending=False)\n",
    "axes[1, 0].barh(range(len(search_likes)), search_likes['like_count'],\n",
    "                color=plt.cm.plasma(np.linspace(0, 1, len(search_likes))),\n",
    "                edgecolor='black', linewidth=0.8)\n",
    "axes[1, 0].set_yticks(range(len(search_likes)))\n",
    "axes[1, 0].set_yticklabels(search_likes.index, fontsize=9)\n",
    "axes[1, 0].set_xlabel('Avg Likes', fontsize=11)\n",
    "axes[1, 0].set_title('Average Likes by Search Keyword', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# í‚¤ì›Œë“œë³„ í‰ê·  ëŒ“ê¸€\n",
    "search_comments = search_keyword_stats.sort_values('comment_count', ascending=False)\n",
    "axes[1, 1].barh(range(len(search_comments)), search_comments['comment_count'],\n",
    "                color=plt.cm.coolwarm(np.linspace(0, 1, len(search_comments))),\n",
    "                edgecolor='black', linewidth=0.8)\n",
    "axes[1, 1].set_yticks(range(len(search_comments)))\n",
    "axes[1, 1].set_yticklabels(search_comments.index, fontsize=9)\n",
    "axes[1, 1].set_xlabel('Avg Comments', fontsize=11)\n",
    "axes[1, 1].set_title('Average Comments by Search Keyword', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/06_search_keyword_performance.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… ì €ì¥: 06_search_keyword_performance.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 7. ì¢…í•© ëŒ€ì‹œë³´ë“œ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[8/8] ì¢…í•© ëŒ€ì‹œë³´ë“œ ìƒì„±...\")\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. ê¸°ë³¸ í†µê³„\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "stats_text = f\"\"\"\n",
    "YOUTUBE DATA SUMMARY\n",
    "{'='*40}\n",
    "\n",
    "Videos: {len(df_videos):,}\n",
    "Comments: {len(df_comments):,}\n",
    "Total Text Items: {len(df_videos) + len(df_comments):,}\n",
    "\n",
    "Video Stats:\n",
    "  Avg Views: {df_videos['view_count'].mean():,.0f}\n",
    "  Avg Likes: {df_videos['like_count'].mean():,.0f}\n",
    "  Avg Comments: {df_videos_merged['comment_count'].mean():.1f}\n",
    "\n",
    "Comment Stats:\n",
    "  Avg Length: {df_comments['text_length'].mean():.0f} chars\n",
    "  Avg Likes: {df_comments['like_count'].mean():.1f}\n",
    "\n",
    "Date Range:\n",
    "  {df_videos['published_date'].min().strftime('%Y-%m-%d')} \n",
    "  to \n",
    "  {df_videos['published_date'].max().strftime('%Y-%m-%d')}\n",
    "\n",
    "Top Channel:\n",
    "  {df_videos['channel_title'].value_counts().index[0]}\n",
    "  ({df_videos['channel_title'].value_counts().values[0]} videos)\n",
    "\"\"\"\n",
    "ax1.text(0.05, 0.95, stats_text, transform=ax1.transAxes,\n",
    "        fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "ax1.axis('off')\n",
    "\n",
    "# 2. ìƒìœ„ ì±„ë„\n",
    "ax2 = fig.add_subplot(gs[0, 1:])\n",
    "top5_channels = df_videos['channel_title'].value_counts().head(5)\n",
    "ax2.bar(range(len(top5_channels)), top5_channels.values,\n",
    "       color=plt.cm.Set3(np.linspace(0, 1, len(top5_channels))),\n",
    "       edgecolor='black', linewidth=1)\n",
    "ax2.set_xticks(range(len(top5_channels)))\n",
    "ax2.set_xticklabels(top5_channels.index, rotation=20, ha='right', fontsize=9)\n",
    "ax2.set_ylabel('Videos', fontsize=10)\n",
    "ax2.set_title('Top 5 Channels', fontsize=12, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. ì‹œê°„ íŠ¸ë Œë“œ\n",
    "ax3 = fig.add_subplot(gs[1, :])\n",
    "daily_agg = df_videos.groupby('published_day').agg({\n",
    "    'video_id': 'count',\n",
    "    'view_count': 'sum'\n",
    "}).reset_index()\n",
    "daily_agg = daily_agg.sort_values('published_day')\n",
    "\n",
    "ax3_twin = ax3.twinx()\n",
    "ax3.plot(daily_agg['published_day'], daily_agg['video_id'],\n",
    "        marker='o', linewidth=2, color='#1f77b4', label='Video Count')\n",
    "ax3_twin.plot(daily_agg['published_day'], daily_agg['view_count'],\n",
    "             marker='s', linewidth=2, color='#ff7f0e', label='Total Views')\n",
    "\n",
    "ax3.set_xlabel('Date', fontsize=10)\n",
    "ax3.set_ylabel('Video Count', fontsize=10, color='#1f77b4')\n",
    "ax3_twin.set_ylabel('Total Views', fontsize=10, color='#ff7f0e')\n",
    "ax3.set_title('Daily Upload & View Trend', fontsize=12, fontweight='bold')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.legend(loc='upper left')\n",
    "ax3_twin.legend(loc='upper right')\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# 4. í‚¤ì›Œë“œ ë¹ˆë„\n",
    "ax4 = fig.add_subplot(gs[2, :2])\n",
    "top10_keywords = dict(keyword_counts.most_common(10))\n",
    "ax4.barh(range(len(top10_keywords)), list(top10_keywords.values()),\n",
    "        color=plt.cm.Paired(np.linspace(0, 1, len(top10_keywords))),\n",
    "        edgecolor='black', linewidth=0.8)\n",
    "ax4.set_yticks(range(len(top10_keywords)))\n",
    "ax4.set_yticklabels(list(top10_keywords.keys()), fontsize=9)\n",
    "ax4.set_xlabel('Frequency', fontsize=10)\n",
    "ax4.set_title('Top 10 Keywords', fontsize=12, fontweight='bold')\n",
    "ax4.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 5. ì°¸ì—¬ë„\n",
    "ax5 = fig.add_subplot(gs[2, 2])\n",
    "engagement_data = {\n",
    "    'High Views\\n(>500K)': len(df_videos[df_videos['view_count'] > 500000]),\n",
    "    'Medium Views\\n(100K-500K)': len(df_videos[(df_videos['view_count'] >= 100000) & \n",
    "                                                 (df_videos['view_count'] <= 500000)]),\n",
    "    'Low Views\\n(<100K)': len(df_videos[df_videos['view_count'] < 100000]),\n",
    "}\n",
    "colors_engagement = ['#2ecc71', '#f39c12', '#e74c3c']\n",
    "ax5.pie(engagement_data.values(), labels=engagement_data.keys(), autopct='%1.1f%%',\n",
    "       colors=colors_engagement, startangle=90, textprops={'fontsize': 9})\n",
    "ax5.set_title('View Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "fig.suptitle('YouTube Data Analysis Dashboard - October 2025 Crypto Crash',\n",
    "            fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.savefig(f'{OUTPUT_DIR}/07_comprehensive_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… ì €ì¥: 07_comprehensive_dashboard.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 8. ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ì™„ë£Œ!\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "insights_report = f\"\"\"\n",
    "{'='*100}\n",
    "YOUTUBE DATA INSIGHTS REPORT\n",
    "October 2025 Crypto Crash Analysis\n",
    "{'='*100}\n",
    "\n",
    "ğŸ“Š ë°ì´í„° ê°œìš”\n",
    "{'â”€'*100}\n",
    "  â€¢ ì´ ì˜ìƒ: {len(df_videos):,}ê°œ\n",
    "  â€¢ ì´ ëŒ“ê¸€: {len(df_comments):,}ê°œ\n",
    "  â€¢ ë¶„ì„ ê¸°ê°„: {df_videos['published_date'].min().strftime('%Y-%m-%d')} ~ {df_videos['published_date'].max().strftime('%Y-%m-%d')}\n",
    "  â€¢ ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: {(df_videos['title_length'].sum() + df_videos['desc_length'].sum() + df_comments['text_length'].sum()):,} ê¸€ì\n",
    "\n",
    "ğŸ† ì£¼ìš” ë°œê²¬ì‚¬í•­\n",
    "{'â”€'*100}\n",
    "\n",
    "1. ê°€ì¥ í™œë°œí•œ ì±„ë„\n",
    "   â†’ {top_channels.index[0]} ({top_channels.values[0]}ê°œ ì˜ìƒ)\n",
    "   â†’ í‰ê·  ì¡°íšŒìˆ˜: {df_videos[df_videos['channel_title']==top_channels.index[0]]['view_count'].mean():,.0f}\n",
    "\n",
    "2. ê°€ì¥ ì¸ê¸°ìˆëŠ” ê²€ìƒ‰ í‚¤ì›Œë“œ\n",
    "   â†’ {search_keyword_stats.index[0]}\n",
    "   â†’ ì˜ìƒ ìˆ˜: {search_keyword_stats.loc[search_keyword_stats.index[0], 'video_count']:.0f}ê°œ\n",
    "\n",
    "3. í‰ê·  ì°¸ì—¬ë„\n",
    "   â†’ ì¡°íšŒìˆ˜: {df_videos['view_count'].mean():,.0f} (ì¤‘ì•™ê°’: {df_videos['view_count'].median():,.0f})\n",
    "   â†’ ì¢‹ì•„ìš”: {df_videos['like_count'].mean():,.0f} (ì¤‘ì•™ê°’: {df_videos['like_count'].median():,.0f})\n",
    "   â†’ ëŒ“ê¸€: {df_videos_merged['comment_count'].mean():.1f} (ì¤‘ì•™ê°’: {df_videos_merged['comment_count'].median():.1f})\n",
    "\n",
    "4. ìµœê³  ì¡°íšŒìˆ˜ ì˜ìƒ\n",
    "   â†’ {df_videos.loc[df_videos['view_count'].idxmax(), 'title']}\n",
    "   â†’ ì¡°íšŒìˆ˜: {df_videos['view_count'].max():,}\n",
    "   â†’ ì±„ë„: {df_videos.loc[df_videos['view_count'].idxmax(), 'channel_title']}\n",
    "\n",
    "5. ê°€ì¥ ë§ì€ ëŒ“ê¸€ì„ ë°›ì€ ì˜ìƒ\n",
    "   â†’ {df_videos_merged.loc[df_videos_merged['comment_count'].idxmax(), 'title']}\n",
    "   â†’ ëŒ“ê¸€: {df_videos_merged['comment_count'].max():.0f}ê°œ\n",
    "\n",
    "ğŸ“ˆ ì‹œê°„ëŒ€ë³„ íŠ¸ë Œë“œ\n",
    "{'â”€'*100}\n",
    "  â€¢ ê°€ì¥ í™œë°œí•œ ë‚ : {daily_videos.loc[daily_videos['count'].idxmax(), 'published_day']} ({daily_videos['count'].max()}ê°œ ì˜ìƒ)\n",
    "  â€¢ ì¼í‰ê·  ì—…ë¡œë“œ: {daily_videos['count'].mean():.1f}ê°œ\n",
    "\n",
    "ğŸ’¬ ëŒ“ê¸€ ë¶„ì„\n",
    "{'â”€'*100}\n",
    "  â€¢ í‰ê·  ëŒ“ê¸€ ê¸¸ì´: {df_comments['text_length'].mean():.0f}ì\n",
    "  â€¢ ê°€ì¥ ì¢‹ì•„ìš” ë§ì€ ëŒ“ê¸€: {df_comments['like_count'].max()}ê°œ\n",
    "  â€¢ ë‹µê¸€ ìˆëŠ” ëŒ“ê¸€: {len(df_comments[df_comments['reply_count'] > 0])}ê°œ ({len(df_comments[df_comments['reply_count'] > 0])/len(df_comments)*100:.1f}%)\n",
    "\n",
    "ğŸ”‘ í•µì‹¬ í‚¤ì›Œë“œ TOP 5\n",
    "{'â”€'*100}\n",
    "\"\"\"\n",
    "\n",
    "for i, (keyword, count) in enumerate(keyword_counts.most_common(5), 1):\n",
    "    insights_report += f\"  {i}. {keyword}: {count}ê°œ ({count/len(df_videos)*100:.1f}%)\\n\"\n",
    "\n",
    "insights_report += f\"\"\"\n",
    "ğŸ’¡ ì¸ì‚¬ì´íŠ¸ & ê²°ë¡ \n",
    "{'â”€'*100}\n",
    "\n",
    "1. ì½˜í…ì¸  ì§‘ì¤‘ë„\n",
    "   â€¢ October 2025 í¬ë˜ì‹œëŠ” {len(df_videos)}ê°œì˜ ì˜ìƒì„ ìƒì„±í–ˆìœ¼ë©°,\n",
    "     ì´ëŠ” crypto ì»¤ë®¤ë‹ˆí‹°ì˜ ë†’ì€ ê´€ì‹¬ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "   \n",
    "2. ì±„ë„ ë‹¤ì–‘ì„±\n",
    "   â€¢ {df_videos['channel_title'].nunique()}ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ì±„ë„ì´ ì°¸ì—¬\n",
    "   â€¢ ìƒìœ„ 5ê°œ ì±„ë„ì´ {top_channels.head(5).sum()}ê°œ ({top_channels.head(5).sum()/len(df_videos)*100:.1f}%)ì˜ ì½˜í…ì¸  ìƒì‚°\n",
    "\n",
    "3. ì°¸ì—¬ë„ íŒ¨í„´\n",
    "   â€¢ ì¢‹ì•„ìš”ìœ¨(Like Ratio) ì¤‘ì•™ê°’: {df_videos['like_ratio'].median():.2f}%\n",
    "   â€¢ ë†’ì€ ì°¸ì—¬ë„ëŠ” ì»¤ë®¤ë‹ˆí‹°ì˜ ê°•í•œ ë°˜ì‘ì„ ì‹œì‚¬\n",
    "\n",
    "4. í‚¤ì›Œë“œ ì¸ì‚¬ì´íŠ¸\n",
    "   â€¢ '{keyword_counts.most_common(1)[0][0]}'ê°€ ê°€ì¥ ìì£¼ ì–¸ê¸‰ë¨ ({keyword_counts.most_common(1)[0][1]}íšŒ)\n",
    "   â€¢ 'Crash', 'Liquidation', 'October' ë“±ì˜ í‚¤ì›Œë“œê°€ ì§€ë°°ì \n",
    "\n",
    "5. ì‹œê°„ì  ë¶„í¬\n",
    "   â€¢ íŠ¹ì • ë‚ ì§œ({daily_videos.loc[daily_videos['count'].idxmax(), 'published_day']})ì— \n",
    "     ì½˜í…ì¸  ìƒì‚°ì´ ì§‘ì¤‘ë¨ â†’ ì´ë²¤íŠ¸ì˜ ì¦‰ê°ì  ì˜í–¥\n",
    "\n",
    "{'='*100}\n",
    "ë¦¬í¬íŠ¸ ìƒì„± ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "{'='*100}\n",
    "\"\"\"\n",
    "\n",
    "# ë¦¬í¬íŠ¸ ì €ì¥\n",
    "report_file = f'{OUTPUT_DIR}/INSIGHTS_REPORT.txt'\n",
    "with open(report_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(insights_report)\n",
    "\n",
    "print(insights_report)\n",
    "print(f\"\\nğŸ’¾ ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸: {report_file}\")\n",
    "\n",
    "# ìµœì¢… ìš”ì•½\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ìƒì„±ëœ íŒŒì¼\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "viz_files = [\n",
    "    \"01_channel_analysis.png\",\n",
    "    \"02_time_trends.png\",\n",
    "    \"03_wordclouds.png\",\n",
    "    \"04_engagement_analysis.png\",\n",
    "    \"05_keyword_analysis.png\",\n",
    "    \"06_search_keyword_performance.png\",\n",
    "    \"07_comprehensive_dashboard.png\",\n",
    "    \"INSIGHTS_REPORT.txt\"\n",
    "]\n",
    "\n",
    "for i, f in enumerate(viz_files, 1):\n",
    "    print(f\"  {i}. {f}\")\n",
    "\n",
    "print(f\"\\nğŸ“‚ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}\")\n",
    "print(\"\\nâœ… ëª¨ë“  ë¶„ì„ ì™„ë£Œ! íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b0e0649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì„¤ì • ê¸°ê°„: 20250901 ~ 20251031\n",
      "âœ… ì €ì¥ ê²½ë¡œ: C:\\junwoo\\AI_Project_01_Team6\\data\\Youtube_data\\youtube_data_collection_final\n",
      "\n",
      "[1/4] ì˜ìƒ ê²€ìƒ‰ ì‹œì‘ (Strict Date Filtering)...\n",
      "   -> ì˜ìƒ 286ê°œ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ. ìƒì„¸ í†µê³„ ì¡°íšŒ ì¤‘...\n",
      "\n",
      "[2/4] ëŒ“ê¸€ ìˆ˜ì§‘ ì‹œì‘ (ëŒ€ìƒ ì˜ìƒ 286ê°œ)...\n",
      "   -> ì§„í–‰ë¥ : 0/286 ì˜ìƒ ì™„ë£Œ\n",
      "   -> ì§„í–‰ë¥ : 20/286 ì˜ìƒ ì™„ë£Œ\n",
      "   -> ì§„í–‰ë¥ : 40/286 ì˜ìƒ ì™„ë£Œ\n",
      "   -> ì§„í–‰ë¥ : 60/286 ì˜ìƒ ì™„ë£Œ\n",
      "   -> ì§„í–‰ë¥ : 80/286 ì˜ìƒ ì™„ë£Œ\n",
      "   -> ì§„í–‰ë¥ : 100/286 ì˜ìƒ ì™„ë£Œ\n",
      "   -> ì§„í–‰ë¥ : 120/286 ì˜ìƒ ì™„ë£Œ\n",
      "   -> ì§„í–‰ë¥ : 140/286 ì˜ìƒ ì™„ë£Œ\n",
      "   -> ì§„í–‰ë¥ : 160/286 ì˜ìƒ ì™„ë£Œ\n",
      "   -> ì§„í–‰ë¥ : 180/286 ì˜ìƒ ì™„ë£Œ\n",
      "   -> ì§„í–‰ë¥ : 200/286 ì˜ìƒ ì™„ë£Œ\n",
      "   -> ì§„í–‰ë¥ : 220/286 ì˜ìƒ ì™„ë£Œ\n",
      "   -> ì§„í–‰ë¥ : 240/286 ì˜ìƒ ì™„ë£Œ\n",
      "\n",
      "[3/4] ìë§‰ ìˆ˜ì§‘ ì‹œì‘...\n",
      "\n",
      "[4/4] ë°ì´í„° ë³‘í•© ë° ì •ë ¬ ì¤‘...\n",
      "\n",
      "============================================================\n",
      "âœ… ìˆ˜ì§‘ ë° ì €ì¥ ì™„ë£Œ!\n",
      "ğŸ“‚ íŒŒì¼ ê²½ë¡œ: C:\\junwoo\\AI_Project_01_Team6\\data\\Youtube_data\\youtube_data_collection_final\\YouTube_Integrated_20250901_20251031_20260203_090730.csv\n",
      "ğŸ“Š ì´ ë°ì´í„°: 5313í–‰\n",
      "ğŸ“… ë‚ ì§œ ë²”ìœ„: 20250901 ~ 20251031\n",
      "============================================================\n",
      "      STD_DATE platform     type  \\\n",
      "1527  20250901  YouTube  comment   \n",
      "1540  20250901  YouTube  comment   \n",
      "1541  20250901  YouTube  comment   \n",
      "1542  20250901  YouTube  comment   \n",
      "1543  20250901  YouTube  comment   \n",
      "\n",
      "                                                content  engagement  \\\n",
      "1527  ğŸ’ Join Premium: https://the-bitcoin-strategy.c...           2   \n",
      "1540  Thanks again for going into this strategy in s...           1   \n",
      "1541                Smart man, smart man in deed ğŸ‘ŒğŸ»ğŸ˜‰ GG           0   \n",
      "1542  Selling into bullish divergence on the RSI. Bo...           2   \n",
      "1543  None if this takes into consideration the 30% ...           0   \n",
      "\n",
      "                   author                                          url  \\\n",
      "1527     @BitcoinStrategy  https://www.youtube.com/watch?v=RN1CTwBj2uU   \n",
      "1540      @martinfell9126  https://www.youtube.com/watch?v=RN1CTwBj2uU   \n",
      "1541  @theblockchainclub1  https://www.youtube.com/watch?v=RN1CTwBj2uU   \n",
      "1542   @mindlessfatemusic  https://www.youtube.com/watch?v=RN1CTwBj2uU   \n",
      "1543       @ToddAdams-q3f  https://www.youtube.com/watch?v=RN1CTwBj2uU   \n",
      "\n",
      "             original_date                          id  \n",
      "1527  2025-09-01T10:31:04Z  UgwGXwKyFGMRqKE3BsB4AaABAg  \n",
      "1540  2025-09-01T22:46:30Z  Ugw7V44aqGJWD9-PS2x4AaABAg  \n",
      "1541  2025-09-01T21:40:44Z  UgzEAyawbhwgP9dyP254AaABAg  \n",
      "1542  2025-09-01T21:25:59Z  UgwHFpOViwGlAasJ1MB4AaABAg  \n",
      "1543  2025-09-01T21:23:00Z  UgxeKzewRmPR1jQRPvR4AaABAg  \n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "[Strict Mode] YouTube Data Collector & Time-Series Integrator\n",
    "Target Period: 2025-09-01 ~ 2025-10-31 (Strict)\n",
    "Output Schema: [STD_DATE(PK), type, platform, content, author, engagement, url, id]\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================================================\n",
    "# [í•„ìˆ˜ ì„¤ì •] API í‚¤ ì„¤ì •\n",
    "# ============================================================================\n",
    "API_KEY = 'AIzaSyDj4y7cahjxnvq3ffSQHelAGjEWhea65JE'  # <--- ì—¬ê¸°ì— API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”\n",
    "\n",
    "# ============================================================================\n",
    "# ê¸°ê°„ ë° ìˆ˜ì§‘ ëª©í‘œ\n",
    "# ============================================================================\n",
    "# API ìš”ì²­ìš© (UTC ê¸°ì¤€)\n",
    "START_DATE_API = \"2025-09-01T00:00:00Z\"\n",
    "END_DATE_API   = \"2025-10-31T23:59:59Z\"\n",
    "\n",
    "# CSV í•„í„°ë§ìš© (YYYYMMDD ë¬¸ìì—´) - ì´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ\n",
    "TARGET_DATE_START = \"20250901\"\n",
    "TARGET_DATE_END   = \"20251031\"\n",
    "\n",
    "CONFIG = {\n",
    "    'target_videos': 500,          # ëª©í‘œ ì˜ìƒ ê°œìˆ˜\n",
    "    'target_comments': 5000,       # ëª©í‘œ ëŒ“ê¸€ ê°œìˆ˜\n",
    "    'max_comments_per_video': 50,  # ì˜ìƒë‹¹ ìˆ˜ì§‘í•  ìµœëŒ€ ëŒ“ê¸€ ìˆ˜\n",
    "    'target_captions': 3000,       # ëª©í‘œ ìë§‰ ìˆ˜\n",
    "    'output_dir': r'C:\\junwoo\\AI_Project_01_Team6\\data\\Youtube_data\\youtube_data_collection_final'\n",
    "}\n",
    "\n",
    "# ê²€ìƒ‰ í‚¤ì›Œë“œ (í¬ë¦½í†  í¬ë˜ì‹œ ê´€ë ¨)\n",
    "KEYWORDS = [\n",
    "    'crypto crash', 'bitcoin crash', 'October 2025 market', \n",
    "    'â‚©27.764.45 ($19) billion liquidation', 'panic selling crypto', \n",
    "    'Binance liquidation', 'Ethereum dump', 'coin market crash'\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# 1. ì´ˆê¸°í™” ë° ìœ í‹¸ë¦¬í‹°\n",
    "# ============================================================================\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "if API_KEY == 'YOUR_YOUTUBE_API_KEY_HERE':\n",
    "    print(\"âŒ [ì˜¤ë¥˜] API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì½”ë“œ ìƒë‹¨ì˜ API_KEYë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.\")\n",
    "    exit(1)\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "print(f\"âœ… ì„¤ì • ê¸°ê°„: {TARGET_DATE_START} ~ {TARGET_DATE_END}\")\n",
    "print(f\"âœ… ì €ì¥ ê²½ë¡œ: {CONFIG['output_dir']}\")\n",
    "\n",
    "def iso_to_yyyymmdd(iso_string):\n",
    "    \"\"\"\n",
    "    ISO 8601 ë‚ ì§œ ë¬¸ìì—´(ì˜ˆ: 2025-09-01T12:00:00Z)ì„ \n",
    "    YYYYMMDD í˜•ì‹(ì˜ˆ: 20250901)ìœ¼ë¡œ ë³€í™˜\n",
    "    \"\"\"\n",
    "    if not iso_string: return None\n",
    "    # ë‚ ì§œ ë¶€ë¶„ë§Œ ì˜ë¼ì„œ í•˜ì´í”ˆ ì œê±° (ê°€ì¥ ì•ˆì „í•˜ê³  ë¹ ë¦„)\n",
    "    return iso_string[:10].replace('-', '')\n",
    "\n",
    "# ============================================================================\n",
    "# 2. ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ (API ìš”ì²­ ì‹œ ê¸°ê°„ í•„í„° ì ìš©)\n",
    "# ============================================================================\n",
    "\n",
    "def collect_videos():\n",
    "    \"\"\"ì˜ìƒ ìˆ˜ì§‘: API ë ˆë²¨ì—ì„œ ë‚ ì§œ í•„í„°ë§ ì ìš©\"\"\"\n",
    "    collected_videos = []\n",
    "    video_ids = set()\n",
    "    \n",
    "    print(\"\\n[1/4] ì˜ìƒ ê²€ìƒ‰ ì‹œì‘ (Strict Date Filtering)...\")\n",
    "    \n",
    "    for keyword in KEYWORDS:\n",
    "        if len(collected_videos) >= CONFIG['target_videos']: break\n",
    "        \n",
    "        try:\n",
    "            request = youtube.search().list(\n",
    "                part='snippet',\n",
    "                q=keyword,\n",
    "                type='video',\n",
    "                maxResults=50,\n",
    "                publishedAfter=START_DATE_API,  # API í•„í„° 1\n",
    "                publishedBefore=END_DATE_API,   # API í•„í„° 2\n",
    "                order='relevance'\n",
    "            )\n",
    "            response = request.execute()\n",
    "            \n",
    "            for item in response.get('items', []):\n",
    "                vid = item['id']['videoId']\n",
    "                if vid in video_ids: continue\n",
    "                \n",
    "                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
    "                pub_date = item['snippet']['publishedAt']\n",
    "                std_date = iso_to_yyyymmdd(pub_date)\n",
    "                \n",
    "                # [ì´ì¤‘ ê²€ì¦] íŒŒì´ì¬ ë ˆë²¨ì—ì„œ ë‚ ì§œ ë²”ìœ„ ì¬í™•ì¸\n",
    "                if not (TARGET_DATE_START <= std_date <= TARGET_DATE_END):\n",
    "                    continue\n",
    "\n",
    "                video_ids.add(vid)\n",
    "                collected_videos.append({\n",
    "                    'id': vid,\n",
    "                    'STD_DATE': std_date, # Key ê°’ ë¯¸ë¦¬ ìƒì„±\n",
    "                    'original_date': pub_date,\n",
    "                    'content': f\"{item['snippet']['title']} || {item['snippet']['description']}\",\n",
    "                    'author': item['snippet']['channelTitle'],\n",
    "                    'url': f\"https://www.youtube.com/watch?v={vid}\",\n",
    "                    'type': 'video'\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ê²€ìƒ‰ ì˜¤ë¥˜ ({keyword}): {e}\")\n",
    "            \n",
    "    # ì˜ìƒ í†µê³„(ì¡°íšŒìˆ˜) ì¶”ê°€ ìˆ˜ì§‘\n",
    "    print(f\"   -> ì˜ìƒ {len(collected_videos)}ê°œ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ. ìƒì„¸ í†µê³„ ì¡°íšŒ ì¤‘...\")\n",
    "    \n",
    "    # 50ê°œì”© ë°°ì¹˜ ì²˜ë¦¬\n",
    "    for i in range(0, len(collected_videos), 50):\n",
    "        batch = collected_videos[i:i+50]\n",
    "        ids = [v['id'] for v in batch]\n",
    "        try:\n",
    "            stats_req = youtube.videos().list(part='statistics', id=','.join(ids)).execute()\n",
    "            stats_map = {item['id']: item['statistics'] for item in stats_req.get('items', [])}\n",
    "            \n",
    "            for v in batch:\n",
    "                if v['id'] in stats_map:\n",
    "                    # ì¡°íšŒìˆ˜ë¥¼ engagementë¡œ ì‚¬ìš©\n",
    "                    v['engagement'] = int(stats_map[v['id']].get('viewCount', 0))\n",
    "                else:\n",
    "                    v['engagement'] = 0\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    return collected_videos\n",
    "\n",
    "def collect_comments(video_list):\n",
    "    \"\"\"ëŒ“ê¸€ ìˆ˜ì§‘: ìˆ˜ì§‘ëœ ì˜ìƒë“¤ì— ë‹¬ë¦° ëŒ“ê¸€ ìˆ˜ì§‘\"\"\"\n",
    "    collected_comments = []\n",
    "    print(f\"\\n[2/4] ëŒ“ê¸€ ìˆ˜ì§‘ ì‹œì‘ (ëŒ€ìƒ ì˜ìƒ {len(video_list)}ê°œ)...\")\n",
    "    \n",
    "    for i, video in enumerate(video_list):\n",
    "        if len(collected_comments) >= CONFIG['target_comments']: break\n",
    "        if i % 20 == 0: print(f\"   -> ì§„í–‰ë¥ : {i}/{len(video_list)} ì˜ìƒ ì™„ë£Œ\")\n",
    "        \n",
    "        try:\n",
    "            req = youtube.commentThreads().list(\n",
    "                part='snippet',\n",
    "                videoId=video['id'],\n",
    "                maxResults=CONFIG['max_comments_per_video'],\n",
    "                textFormat='plainText'\n",
    "            )\n",
    "            resp = req.execute()\n",
    "            \n",
    "            for item in resp.get('items', []):\n",
    "                snippet = item['snippet']['topLevelComment']['snippet']\n",
    "                pub_date = snippet['publishedAt']\n",
    "                std_date = iso_to_yyyymmdd(pub_date)\n",
    "                \n",
    "                # [ì´ì¤‘ ê²€ì¦] ëŒ“ê¸€ ì‘ì„±ì¼ë„ ê¸°ê°„ ë‚´ì—¬ì•¼ í•¨\n",
    "                if not (TARGET_DATE_START <= std_date <= TARGET_DATE_END):\n",
    "                    continue\n",
    "                    \n",
    "                collected_comments.append({\n",
    "                    'id': item['id'],\n",
    "                    'STD_DATE': std_date,\n",
    "                    'original_date': pub_date,\n",
    "                    'content': snippet['textDisplay'],\n",
    "                    'author': snippet['authorDisplayName'],\n",
    "                    'engagement': int(snippet['likeCount']), # ì¢‹ì•„ìš”ë¥¼ engagementë¡œ\n",
    "                    'url': video['url'], # ëŒ“ê¸€ì€ ì˜ìƒ URL ê³µìœ \n",
    "                    'type': 'comment'\n",
    "                })\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return collected_comments\n",
    "\n",
    "def collect_captions(video_list):\n",
    "    \"\"\"ìë§‰ ìˆ˜ì§‘: ì˜ìƒ ë‚ ì§œë¥¼ ìƒì†ë°›ìŒ\"\"\"\n",
    "    collected_captions = []\n",
    "    print(f\"\\n[3/4] ìë§‰ ìˆ˜ì§‘ ì‹œì‘...\")\n",
    "    \n",
    "    for i, video in enumerate(video_list):\n",
    "        if len(collected_captions) >= CONFIG['target_captions']: break\n",
    "        \n",
    "        try:\n",
    "            # í•œêµ­ì–´ -> ì˜ì–´ ìˆœ ì‹œë„\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video['id'], languages=['ko', 'en'])\n",
    "            \n",
    "            # ìë§‰ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸° (ë„ˆë¬´ ì˜ê²Œ ìª¼ê°œì§€ ì•Šë„ë¡ 5ë¬¸ì¥ì”© ë³‘í•©í•˜ê±°ë‚˜ ì „ì²´ ë³‘í•©)\n",
    "            full_text = \" \".join([t['text'] for t in transcript])\n",
    "            \n",
    "            # ìë§‰ì€ ë‚ ì§œê°€ ì—†ìœ¼ë¯€ë¡œ ì˜ìƒì˜ ë‚ ì§œ(STD_DATE)ë¥¼ ê·¸ëŒ€ë¡œ ìƒì†ë°›ìŒ (ì¤‘ìš”)\n",
    "            collected_captions.append({\n",
    "                'id': video['id'],\n",
    "                'STD_DATE': video['STD_DATE'], # ì˜ìƒ ë‚ ì§œ ìƒì†\n",
    "                'original_date': video['original_date'],\n",
    "                'content': full_text[:3000], # ì—‘ì…€ ì…€ ì œí•œ ê³ ë ¤í•˜ì—¬ ê¸¸ì´ ì œí•œ\n",
    "                'author': 'transcript',\n",
    "                'engagement': 0,\n",
    "                'url': video['url'],\n",
    "                'type': 'caption'\n",
    "            })\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return collected_captions\n",
    "\n",
    "# ============================================================================\n",
    "# 3. ë©”ì¸ ì‹¤í–‰ ë° ì €ì¥\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    # 1. ë°ì´í„° ìˆ˜ì§‘\n",
    "    videos = collect_videos()\n",
    "    comments = collect_comments(videos)\n",
    "    captions = collect_captions(videos)\n",
    "    \n",
    "    # 2. ë°ì´í„° í†µí•©\n",
    "    print(\"\\n[4/4] ë°ì´í„° ë³‘í•© ë° ì •ë ¬ ì¤‘...\")\n",
    "    all_data = videos + comments + captions\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # 3. í•„ìˆ˜ ì»¬ëŸ¼ ì •ë¦¬ ë° ìˆœì„œ ê°•ì œ (íŒ€ì› ê³µìœ ìš© Schema)\n",
    "    # ë°˜ë“œì‹œ STD_DATEê°€ ë§¨ ì•ì— ì˜¤ë„ë¡ í•¨\n",
    "    columns_order = [\n",
    "        'STD_DATE',   # YYYYMMDD (Key)\n",
    "        'platform',   # 'YouTube' ê³ ì •\n",
    "        'type',       # video / comment / caption\n",
    "        'content',    # í…ìŠ¤íŠ¸ ë‚´ìš©\n",
    "        'engagement', # ì¡°íšŒìˆ˜ or ì¢‹ì•„ìš”\n",
    "        'author',     # ì‘ì„±ì\n",
    "        'url',        # ë§í¬\n",
    "        'original_date', # ì›ë³¸ ì‹œê°„\n",
    "        'id'          # ê³ ìœ  ID\n",
    "    ]\n",
    "    \n",
    "    # í”Œë«í¼ ì»¬ëŸ¼ ì¶”ê°€\n",
    "    df['platform'] = 'YouTube'\n",
    "    \n",
    "    # ì»¬ëŸ¼ ìˆœì„œ ì ìš© (ì—†ëŠ” ì»¬ëŸ¼ì€ ìƒì„±)\n",
    "    for col in columns_order:\n",
    "        if col not in df.columns:\n",
    "            df[col] = ''\n",
    "            \n",
    "    df = df[columns_order]\n",
    "    \n",
    "    # 4. ìµœì¢… ì •ë ¬ (ë‚ ì§œìˆœ -> íƒ€ì…ìˆœ)\n",
    "    df = df.sort_values(by=['STD_DATE', 'type'])\n",
    "    \n",
    "    # 5. íŒŒì¼ ì €ì¥\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f\"YouTube_Integrated_{TARGET_DATE_START}_{TARGET_DATE_END}_{timestamp}.csv\"\n",
    "    save_path = os.path.join(CONFIG['output_dir'], filename)\n",
    "    \n",
    "    df.to_csv(save_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"âœ… ìˆ˜ì§‘ ë° ì €ì¥ ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ“‚ íŒŒì¼ ê²½ë¡œ: {save_path}\")\n",
    "    print(f\"ğŸ“Š ì´ ë°ì´í„°: {len(df)}í–‰\")\n",
    "    print(f\"ğŸ“… ë‚ ì§œ ë²”ìœ„: {df['STD_DATE'].min()} ~ {df['STD_DATE'].max()}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\n",
    "    print(df.head())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f380c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²˜ë¦¬ ì™„ë£Œ: 5313 -> 5280 í–‰\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë“œ\n",
    "file_path = 'C:\\junwoo\\AI_Project_01_Team6\\data\\Youtube_data\\youtube_data_collection_final\\YouTube_Integrated_20250901_20251031_20260203_090730.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 2. ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
    "def clean_content(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # URL ì œê±°\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # ë¶ˆí•„ìš”í•œ í™ë³´ì„± ë¬¸êµ¬ íŒ¨í„´ (êµ¬ë…, ì¢‹ì•„ìš”, ë§í¬ ìœ ë„ ë“±)\n",
    "    useless_patterns = [\n",
    "        r'join premium:?', r'public telegram:?', r'trading platform:?',\n",
    "        r'subscribe to (my|our) channel', r'don\\'t forget to like',\n",
    "        r'link in (the )?description', r'link in bio', r'follow (me|us) on',\n",
    "        r'business inquiries:', r'social media:', r'contact:', \n",
    "        r'not financial advice', r'sign up', r'click here'\n",
    "    ]\n",
    "    \n",
    "    for pattern in useless_patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "        \n",
    "    # ê³µë°± ì •ë¦¬\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# 3. ì „ì²˜ë¦¬ ì ìš©\n",
    "df['content'] = df['content'].apply(clean_content)\n",
    "\n",
    "# 4. ë‚´ìš©ì´ ì—†ê±°ë‚˜ ë„ˆë¬´ ì§§ì€(ì˜ë¯¸ ì—†ëŠ”) í–‰ ì‚­ì œ\n",
    "df_clean = df[df['content'].str.len() > 1].copy()\n",
    "\n",
    "# 5. ì €ì¥\n",
    "df_clean.to_csv('YouTube_Cleaned_Data.csv', index=False, encoding='utf-8-sig')\n",
    "print(f\"ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df)} -> {len(df_clean)} í–‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c789b372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ íŒŒì¼ ë¡œë“œ ì¤‘: C:\\junwoo\\AI_Project_01_Team6\\data\\Youtube_data\\YouTube_Cleaned_Data.csv\n",
      "   -> ì›ë³¸ ë°ì´í„°: 5280í–‰\n",
      "\n",
      "ğŸ§¹ íŠ¹ìˆ˜ë¬¸ì ì œê±° ì‘ì—… ì‹œì‘...\n",
      "   -> ì „ì²˜ë¦¬ í›„ ë°ì´í„°: 5180í–‰ (ì‚­ì œëœ í–‰: 100ê°œ)\n",
      "\n",
      "==================================================\n",
      "âœ… ìµœì¢… ì „ì²˜ë¦¬ ì™„ë£Œ!\n",
      "ğŸ“‚ ì €ì¥ëœ íŒŒì¼: C:\\junwoo\\AI_Project_01_Team6\\data\\Youtube_data\\YouTube_Final_NoSpecialChars.csv\n",
      "==================================================\n",
      "\n",
      "[ì „ì²˜ë¦¬ ê²°ê³¼ ìƒ˜í”Œ]\n",
      "    STD_DATE     type                                            content\n",
      "1   20250901  comment  Thanks again for going into this strategy in s...\n",
      "2   20250901  comment                     Smart man smart man in deed GG\n",
      "3   20250901  comment  Selling into bullish divergence on the RSI Bol...\n",
      "4   20250901  comment  None if this takes into consideration the 30 y...\n",
      "5   20250901  comment  Hi if for some reason the BTC price goes up by...\n",
      "6   20250901  comment                                  See u btc 101000k\n",
      "7   20250901  comment  Theres a solid chance you just sold near the b...\n",
      "8   20250901  comment  Ive wanted to join your premium members but I ...\n",
      "9   20250901  comment                                   Gerhards wobbled\n",
      "10  20250901  comment  you really believe Bitcoin cant jump to 112k i...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# 1. íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "# ============================================================================\n",
    "# ì´ë¯¸ 1ì°¨ ì „ì²˜ë¦¬ê°€ ì™„ë£Œëœ íŒŒì¼ ê²½ë¡œ\n",
    "INPUT_FILE = r'C:\\junwoo\\AI_Project_01_Team6\\data\\Youtube_data\\YouTube_Cleaned_Data.csv'\n",
    "# ìµœì¢… ê²°ê³¼ ì €ì¥ ê²½ë¡œ\n",
    "OUTPUT_FILE = r'C:\\junwoo\\AI_Project_01_Team6\\data\\Youtube_data\\YouTube_Final_NoSpecialChars.csv'\n",
    "\n",
    "# ============================================================================\n",
    "# 2. ë°ì´í„° ë¡œë“œ\n",
    "# ============================================================================\n",
    "if os.path.exists(INPUT_FILE):\n",
    "    print(f\"ğŸ“‚ íŒŒì¼ ë¡œë“œ ì¤‘: {INPUT_FILE}\")\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    print(f\"   -> ì›ë³¸ ë°ì´í„°: {len(df)}í–‰\")\n",
    "else:\n",
    "    print(f\"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\\nê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: {INPUT_FILE}\")\n",
    "    exit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. íŠ¹ìˆ˜ë¬¸ì ì œê±° í•¨ìˆ˜ (Regex í™œìš©)\n",
    "# ============================================================================\n",
    "def remove_special_characters(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # [ì •ê·œí‘œí˜„ì‹ ì„¤ëª…]\n",
    "    # [^ ... ] : ëŒ€ê´„í˜¸ ì•ˆì˜ ë¬¸ìë“¤ì„ \"ì œì™¸í•œ\" ëª¨ë“  ê²ƒì„ ì°¾ìŒ\n",
    "    # ê°€-í£    : í•œê¸€ ì „ì²´\n",
    "    # a-zA-Z   : ì˜ë¬¸ ëŒ€ì†Œë¬¸ì\n",
    "    # 0-9      : ìˆ«ì\n",
    "    # \\s       : ê³µë°±(ìŠ¤í˜ì´ìŠ¤, íƒ­ ë“±)\n",
    "    # => ì¦‰, í•œê¸€/ì˜ì–´/ìˆ«ì/ê³µë°±ì´ \"ì•„ë‹Œ\" ëª¨ë“  ë¬¸ì(íŠ¹ìˆ˜ë¬¸ì, ì´ëª¨ì§€ ë“±)ë¥¼ ì œê±°('')\n",
    "    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    # ë‹¤ì¤‘ ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ì¤„ì„ (ì˜ˆ: \"ì•ˆë…•    í•˜ì„¸ìš”\" -> \"ì•ˆë…• í•˜ì„¸ìš”\")\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"\\nğŸ§¹ íŠ¹ìˆ˜ë¬¸ì ì œê±° ì‘ì—… ì‹œì‘...\")\n",
    "\n",
    "# í•¨ìˆ˜ ì ìš©\n",
    "df['content'] = df['content'].apply(remove_special_characters)\n",
    "\n",
    "# ============================================================================\n",
    "# 4. ë¹ˆ ë°ì´í„° ì •ë¦¬ ë° ì €ì¥\n",
    "# ============================================================================\n",
    "\n",
    "# íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ ë‚´ìš©ì´ í…… ë¹„ê²Œ ëœ í–‰ ì‚­ì œ (ì˜ˆ: \"!!! ???\" -> \"\")\n",
    "df_final = df[df['content'].str.strip().astype(bool)].copy()\n",
    "\n",
    "print(f\"   -> ì „ì²˜ë¦¬ í›„ ë°ì´í„°: {len(df_final)}í–‰ (ì‚­ì œëœ í–‰: {len(df) - len(df_final)}ê°œ)\")\n",
    "\n",
    "# ìµœì¢… ì €ì¥\n",
    "df_final.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"âœ… ìµœì¢… ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“‚ ì €ì¥ëœ íŒŒì¼: {OUTPUT_FILE}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°\n",
    "print(\"\\n[ì „ì²˜ë¦¬ ê²°ê³¼ ìƒ˜í”Œ]\")\n",
    "print(df_final[['STD_DATE', 'type', 'content']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "542dacb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ë°ì´í„° í¬ê¸°: (5180, 9)\n",
      "================================================================================\n",
      "    STD_DATE platform     type                                            content  engagement               author                                          url         original_date                          id\n",
      "0   20250901  YouTube  comment  Thanks again for going into this strategy in s...           1      @martinfell9126  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T22:46:30Z  Ugw7V44aqGJWD9-PS2x4AaABAg\n",
      "1   20250901  YouTube  comment                     Smart man smart man in deed GG           0  @theblockchainclub1  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T21:40:44Z  UgzEAyawbhwgP9dyP254AaABAg\n",
      "2   20250901  YouTube  comment  Selling into bullish divergence on the RSI Bol...           2   @mindlessfatemusic  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T21:25:59Z  UgwHFpOViwGlAasJ1MB4AaABAg\n",
      "3   20250901  YouTube  comment  None if this takes into consideration the 30 y...           0       @ToddAdams-q3f  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T21:23:00Z  UgxeKzewRmPR1jQRPvR4AaABAg\n",
      "4   20250901  YouTube  comment  Hi if for some reason the BTC price goes up by...           0             @uds2kor  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T20:32:02Z  Ugy51jQC20Wjwr2uF754AaABAg\n",
      "5   20250901  YouTube  comment                                  See u btc 101000k           0        @miggystv9431  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T20:06:53Z  Ugwli9k17RFz7_Qskv54AaABAg\n",
      "6   20250901  YouTube  comment  Theres a solid chance you just sold near the b...           6   @1stCommentChillin  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T17:22:22Z  Ugw1H9cpqadd6fwSG3h4AaABAg\n",
      "7   20250901  YouTube  comment  Ive wanted to join your premium members but I ...           1        @tommydoublec  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T17:13:02Z  UgzBZ3URw9MpC5hUoIp4AaABAg\n",
      "8   20250901  YouTube  comment                                   Gerhards wobbled           2           @CinnaPepe  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T16:25:31Z  UgxtRPCfkI4aJoA3Qkd4AaABAg\n",
      "9   20250901  YouTube  comment  you really believe Bitcoin cant jump to 112k i...           1     @cathyclinch8725  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T16:06:54Z  UgwhW0Hz1boNnGjL4xZ4AaABAg\n",
      "10  20250901  YouTube  comment                                      show receipts           0            @ctcrnitv  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T15:47:47Z  Ugwx56vPg9hq_cW7Nj94AaABAg\n",
      "11  20250901  YouTube  comment                                           115 days           0          @tombijl379  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T15:35:11Z  UgybqcBKgf2lP8tD7RR4AaABAg\n",
      "12  20250901  YouTube  comment                           Next week All in Bitcoin           2   @prasannashakya710  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T14:56:42Z  Ugz1NlqwGSruDtfALQ14AaABAg\n",
      "13  20250901  YouTube  comment  Never sell ur Bitcoin do dollarcostaverage ins...           4              @CellDE  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T14:52:09Z  Ugzk4xZ3NT_0ob7Aq1V4AaABAg\n",
      "14  20250901  YouTube  comment  This dude is a different kind of special Shows...          13        @7Investments  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T14:47:19Z  UgykocZ3jTDbwjy_f5V4AaABAg\n",
      "15  20250901  YouTube  comment                           make a video about jager           0   @josephanthony5664  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T14:32:07Z  UgyBd4BUCPWOmZsmzVl4AaABAg\n",
      "16  20250901  YouTube  comment                                           You sold           0         @hodlfantasy  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T14:30:54Z  UgxEn1c_o0DhANbI2_94AaABAg\n",
      "17  20250901  YouTube  comment           ETH GONNABE NEW GENARATION NEXT BTC SOON           0         @salsabil887  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T14:29:07Z  UgyySnOFfHmJ1HOsdul4AaABAg\n",
      "18  20250901  YouTube  comment  Its been below the 120day 4 times during this ...           0           @digimat77  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T14:00:58Z  UgyVY-KGgVVUK0yJCRF4AaABAg\n",
      "19  20250901  YouTube  comment  This guy keeps it real you dont sell Bitcoin b...           1       @ashershah5352  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T13:53:41Z  Ugyl3BLxD6VdJi7ZfWl4AaABAg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STD_DATE</th>\n",
       "      <th>platform</th>\n",
       "      <th>type</th>\n",
       "      <th>content</th>\n",
       "      <th>engagement</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>original_date</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20250901</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>comment</td>\n",
       "      <td>Thanks again for going into this strategy in s...</td>\n",
       "      <td>1</td>\n",
       "      <td>@martinfell9126</td>\n",
       "      <td>https://www.youtube.com/watch?v=RN1CTwBj2uU</td>\n",
       "      <td>2025-09-01T22:46:30Z</td>\n",
       "      <td>Ugw7V44aqGJWD9-PS2x4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20250901</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>comment</td>\n",
       "      <td>Smart man smart man in deed GG</td>\n",
       "      <td>0</td>\n",
       "      <td>@theblockchainclub1</td>\n",
       "      <td>https://www.youtube.com/watch?v=RN1CTwBj2uU</td>\n",
       "      <td>2025-09-01T21:40:44Z</td>\n",
       "      <td>UgzEAyawbhwgP9dyP254AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20250901</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>comment</td>\n",
       "      <td>Selling into bullish divergence on the RSI Bol...</td>\n",
       "      <td>2</td>\n",
       "      <td>@mindlessfatemusic</td>\n",
       "      <td>https://www.youtube.com/watch?v=RN1CTwBj2uU</td>\n",
       "      <td>2025-09-01T21:25:59Z</td>\n",
       "      <td>UgwHFpOViwGlAasJ1MB4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20250901</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>comment</td>\n",
       "      <td>None if this takes into consideration the 30 y...</td>\n",
       "      <td>0</td>\n",
       "      <td>@ToddAdams-q3f</td>\n",
       "      <td>https://www.youtube.com/watch?v=RN1CTwBj2uU</td>\n",
       "      <td>2025-09-01T21:23:00Z</td>\n",
       "      <td>UgxeKzewRmPR1jQRPvR4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20250901</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>comment</td>\n",
       "      <td>Hi if for some reason the BTC price goes up by...</td>\n",
       "      <td>0</td>\n",
       "      <td>@uds2kor</td>\n",
       "      <td>https://www.youtube.com/watch?v=RN1CTwBj2uU</td>\n",
       "      <td>2025-09-01T20:32:02Z</td>\n",
       "      <td>Ugy51jQC20Wjwr2uF754AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20250901</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>comment</td>\n",
       "      <td>See u btc 101000k</td>\n",
       "      <td>0</td>\n",
       "      <td>@miggystv9431</td>\n",
       "      <td>https://www.youtube.com/watch?v=RN1CTwBj2uU</td>\n",
       "      <td>2025-09-01T20:06:53Z</td>\n",
       "      <td>Ugwli9k17RFz7_Qskv54AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20250901</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>comment</td>\n",
       "      <td>Theres a solid chance you just sold near the b...</td>\n",
       "      <td>6</td>\n",
       "      <td>@1stCommentChillin</td>\n",
       "      <td>https://www.youtube.com/watch?v=RN1CTwBj2uU</td>\n",
       "      <td>2025-09-01T17:22:22Z</td>\n",
       "      <td>Ugw1H9cpqadd6fwSG3h4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20250901</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>comment</td>\n",
       "      <td>Ive wanted to join your premium members but I ...</td>\n",
       "      <td>1</td>\n",
       "      <td>@tommydoublec</td>\n",
       "      <td>https://www.youtube.com/watch?v=RN1CTwBj2uU</td>\n",
       "      <td>2025-09-01T17:13:02Z</td>\n",
       "      <td>UgzBZ3URw9MpC5hUoIp4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20250901</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>comment</td>\n",
       "      <td>Gerhards wobbled</td>\n",
       "      <td>2</td>\n",
       "      <td>@CinnaPepe</td>\n",
       "      <td>https://www.youtube.com/watch?v=RN1CTwBj2uU</td>\n",
       "      <td>2025-09-01T16:25:31Z</td>\n",
       "      <td>UgxtRPCfkI4aJoA3Qkd4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20250901</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>comment</td>\n",
       "      <td>you really believe Bitcoin cant jump to 112k i...</td>\n",
       "      <td>1</td>\n",
       "      <td>@cathyclinch8725</td>\n",
       "      <td>https://www.youtube.com/watch?v=RN1CTwBj2uU</td>\n",
       "      <td>2025-09-01T16:06:54Z</td>\n",
       "      <td>UgwhW0Hz1boNnGjL4xZ4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20250901</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>comment</td>\n",
       "      <td>show receipts</td>\n",
       "      <td>0</td>\n",
       "      <td>@ctcrnitv</td>\n",
       "      <td>https://www.youtube.com/watch?v=RN1CTwBj2uU</td>\n",
       "      <td>2025-09-01T15:47:47Z</td>\n",
       "      <td>Ugwx56vPg9hq_cW7Nj94AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20250901</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>comment</td>\n",
       "      <td>115 days</td>\n",
       "      <td>0</td>\n",
       "      <td>@tombijl379</td>\n",
       "      <td>https://www.youtube.com/watch?v=RN1CTwBj2uU</td>\n",
       "      <td>2025-09-01T15:35:11Z</td>\n",
       "      <td>UgybqcBKgf2lP8tD7RR4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20250901</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>comment</td>\n",
       "      <td>Next week All in Bitcoin</td>\n",
       "      <td>2</td>\n",
       "      <td>@prasannashakya710</td>\n",
       "      <td>https://www.youtube.com/watch?v=RN1CTwBj2uU</td>\n",
       "      <td>2025-09-01T14:56:42Z</td>\n",
       "      <td>Ugz1NlqwGSruDtfALQ14AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20250901</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>comment</td>\n",
       "      <td>Never sell ur Bitcoin do dollarcostaverage ins...</td>\n",
       "      <td>4</td>\n",
       "      <td>@CellDE</td>\n",
       "      <td>https://www.youtube.com/watch?v=RN1CTwBj2uU</td>\n",
       "      <td>2025-09-01T14:52:09Z</td>\n",
       "      <td>Ugzk4xZ3NT_0ob7Aq1V4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20250901</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>comment</td>\n",
       "      <td>This dude is a different kind of special Shows...</td>\n",
       "      <td>13</td>\n",
       "      <td>@7Investments</td>\n",
       "      <td>https://www.youtube.com/watch?v=RN1CTwBj2uU</td>\n",
       "      <td>2025-09-01T14:47:19Z</td>\n",
       "      <td>UgykocZ3jTDbwjy_f5V4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20250901</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>comment</td>\n",
       "      <td>make a video about jager</td>\n",
       "      <td>0</td>\n",
       "      <td>@josephanthony5664</td>\n",
       "      <td>https://www.youtube.com/watch?v=RN1CTwBj2uU</td>\n",
       "      <td>2025-09-01T14:32:07Z</td>\n",
       "      <td>UgyBd4BUCPWOmZsmzVl4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20250901</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>comment</td>\n",
       "      <td>You sold</td>\n",
       "      <td>0</td>\n",
       "      <td>@hodlfantasy</td>\n",
       "      <td>https://www.youtube.com/watch?v=RN1CTwBj2uU</td>\n",
       "      <td>2025-09-01T14:30:54Z</td>\n",
       "      <td>UgxEn1c_o0DhANbI2_94AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20250901</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>comment</td>\n",
       "      <td>ETH GONNABE NEW GENARATION NEXT BTC SOON</td>\n",
       "      <td>0</td>\n",
       "      <td>@salsabil887</td>\n",
       "      <td>https://www.youtube.com/watch?v=RN1CTwBj2uU</td>\n",
       "      <td>2025-09-01T14:29:07Z</td>\n",
       "      <td>UgyySnOFfHmJ1HOsdul4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20250901</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>comment</td>\n",
       "      <td>Its been below the 120day 4 times during this ...</td>\n",
       "      <td>0</td>\n",
       "      <td>@digimat77</td>\n",
       "      <td>https://www.youtube.com/watch?v=RN1CTwBj2uU</td>\n",
       "      <td>2025-09-01T14:00:58Z</td>\n",
       "      <td>UgyVY-KGgVVUK0yJCRF4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20250901</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>comment</td>\n",
       "      <td>This guy keeps it real you dont sell Bitcoin b...</td>\n",
       "      <td>1</td>\n",
       "      <td>@ashershah5352</td>\n",
       "      <td>https://www.youtube.com/watch?v=RN1CTwBj2uU</td>\n",
       "      <td>2025-09-01T13:53:41Z</td>\n",
       "      <td>Ugyl3BLxD6VdJi7ZfWl4AaABAg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    STD_DATE platform     type                                            content  engagement               author                                          url         original_date                          id\n",
       "0   20250901  YouTube  comment  Thanks again for going into this strategy in s...           1      @martinfell9126  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T22:46:30Z  Ugw7V44aqGJWD9-PS2x4AaABAg\n",
       "1   20250901  YouTube  comment                     Smart man smart man in deed GG           0  @theblockchainclub1  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T21:40:44Z  UgzEAyawbhwgP9dyP254AaABAg\n",
       "2   20250901  YouTube  comment  Selling into bullish divergence on the RSI Bol...           2   @mindlessfatemusic  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T21:25:59Z  UgwHFpOViwGlAasJ1MB4AaABAg\n",
       "3   20250901  YouTube  comment  None if this takes into consideration the 30 y...           0       @ToddAdams-q3f  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T21:23:00Z  UgxeKzewRmPR1jQRPvR4AaABAg\n",
       "4   20250901  YouTube  comment  Hi if for some reason the BTC price goes up by...           0             @uds2kor  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T20:32:02Z  Ugy51jQC20Wjwr2uF754AaABAg\n",
       "5   20250901  YouTube  comment                                  See u btc 101000k           0        @miggystv9431  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T20:06:53Z  Ugwli9k17RFz7_Qskv54AaABAg\n",
       "6   20250901  YouTube  comment  Theres a solid chance you just sold near the b...           6   @1stCommentChillin  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T17:22:22Z  Ugw1H9cpqadd6fwSG3h4AaABAg\n",
       "7   20250901  YouTube  comment  Ive wanted to join your premium members but I ...           1        @tommydoublec  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T17:13:02Z  UgzBZ3URw9MpC5hUoIp4AaABAg\n",
       "8   20250901  YouTube  comment                                   Gerhards wobbled           2           @CinnaPepe  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T16:25:31Z  UgxtRPCfkI4aJoA3Qkd4AaABAg\n",
       "9   20250901  YouTube  comment  you really believe Bitcoin cant jump to 112k i...           1     @cathyclinch8725  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T16:06:54Z  UgwhW0Hz1boNnGjL4xZ4AaABAg\n",
       "10  20250901  YouTube  comment                                      show receipts           0            @ctcrnitv  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T15:47:47Z  Ugwx56vPg9hq_cW7Nj94AaABAg\n",
       "11  20250901  YouTube  comment                                           115 days           0          @tombijl379  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T15:35:11Z  UgybqcBKgf2lP8tD7RR4AaABAg\n",
       "12  20250901  YouTube  comment                           Next week All in Bitcoin           2   @prasannashakya710  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T14:56:42Z  Ugz1NlqwGSruDtfALQ14AaABAg\n",
       "13  20250901  YouTube  comment  Never sell ur Bitcoin do dollarcostaverage ins...           4              @CellDE  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T14:52:09Z  Ugzk4xZ3NT_0ob7Aq1V4AaABAg\n",
       "14  20250901  YouTube  comment  This dude is a different kind of special Shows...          13        @7Investments  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T14:47:19Z  UgykocZ3jTDbwjy_f5V4AaABAg\n",
       "15  20250901  YouTube  comment                           make a video about jager           0   @josephanthony5664  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T14:32:07Z  UgyBd4BUCPWOmZsmzVl4AaABAg\n",
       "16  20250901  YouTube  comment                                           You sold           0         @hodlfantasy  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T14:30:54Z  UgxEn1c_o0DhANbI2_94AaABAg\n",
       "17  20250901  YouTube  comment           ETH GONNABE NEW GENARATION NEXT BTC SOON           0         @salsabil887  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T14:29:07Z  UgyySnOFfHmJ1HOsdul4AaABAg\n",
       "18  20250901  YouTube  comment  Its been below the 120day 4 times during this ...           0           @digimat77  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T14:00:58Z  UgyVY-KGgVVUK0yJCRF4AaABAg\n",
       "19  20250901  YouTube  comment  This guy keeps it real you dont sell Bitcoin b...           1       @ashershah5352  https://www.youtube.com/watch?v=RN1CTwBj2uU  2025-09-01T13:53:41Z  Ugyl3BLxD6VdJi7ZfWl4AaABAg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ (ìƒì„±ëœ ìµœì¢… íŒŒì¼)\n",
    "file_path = r'C:\\junwoo\\AI_Project_01_Team6\\data\\Youtube_data\\YouTube_Final_NoSpecialChars.csv'\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 1. í„°ë¯¸ë„ì—ì„œ í…ìŠ¤íŠ¸ í‘œë¡œ ë³´ê¸°\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ë°ì´í„° í¬ê¸°: {df.shape}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ëª¨ë“  ì—´ì´ ì˜ë¦¬ì§€ ì•Šê²Œ ì„¤ì • í›„ ì¶œë ¥\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    pd.set_option('display.max_colwidth', 50) # ë‚´ìš©ì´ ê¸¸ë©´ 50ìì—ì„œ ìë¦„\n",
    "    \n",
    "    print(df.head(20)) # ìƒìœ„ 20ê°œ ì¶œë ¥\n",
    "    \n",
    "    # 2. (Jupyter í™˜ê²½ì¸ ê²½ìš°) ì¸í„°ë™í‹°ë¸Œ í‘œë¡œ ë³´ê¸°\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(df.head(20))\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print(\"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11be7929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...\n",
      "   - Total Data: 7,626í–‰ (í”Œë«í¼: <StringArray>\n",
      "['YouTube', 'Reddit', 'News', 'Medium', 'Substack', 'X (Twitter)', 'BitcoinTalk']\n",
      "Length: 7, dtype: str)\n",
      "   - YouTube Final: 5,180í–‰\n",
      "\n",
      "ğŸ”— ë°ì´í„° ë³‘í•© ì¤‘...\n",
      "ğŸ§¹ ë°ì´í„° ì •ì œ ì¤‘ (ì¤‘ë³µ ì œê±°)...\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ ìµœì¢… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!\n",
      "ğŸ“‚ ì €ì¥ ê²½ë¡œ: C:\\junwoo\\AI_Project_01_Team6\\data\\Youtube_data\\FINAL_TEAM_DATASET_FULL.csv\n",
      "ğŸ“Š ì´ ë°ì´í„°: 6,382í–‰\n",
      "ğŸ“… ê¸°ê°„: 20250901 ~ 20251031\n",
      "============================================================\n",
      "\n",
      "[í”Œë«í¼ë³„ ë°ì´í„° ê°œìˆ˜]\n",
      "platform\n",
      "YouTube        5173\n",
      "X (Twitter)     464\n",
      "Reddit          350\n",
      "News            133\n",
      "Medium          126\n",
      "Substack         98\n",
      "BitcoinTalk      38\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# 1. íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "# ============================================================================\n",
    "BASE_DIR = r'C:\\junwoo\\AI_Project_01_Team6\\data\\Youtube_data'\n",
    "\n",
    "FILE_TOTAL = os.path.join(BASE_DIR, 'Total_Data.csv')       # Reddit, X, News ë“± í¬í•¨ëœ í†µí•© íŒŒì¼\n",
    "FILE_YOUTUBE = os.path.join(BASE_DIR, 'YouTube_Final.csv')  # ì •ì œëœ ìœ íŠœë¸Œ íŒŒì¼\n",
    "OUTPUT_FILE = os.path.join(BASE_DIR, 'FINAL_TEAM_DATASET_FULL.csv')\n",
    "\n",
    "# ============================================================================\n",
    "# 2. ë°ì´í„° ë¡œë“œ ë° ë³‘í•©\n",
    "# ============================================================================\n",
    "print(\"ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "\n",
    "# íŒŒì¼ ì½ê¸° (ì¸ì½”ë”© ì—ëŸ¬ ë°©ì§€ìš© ì˜µì…˜ ì¶”ê°€)\n",
    "df_total = pd.read_csv(FILE_TOTAL, low_memory=False)\n",
    "df_youtube = pd.read_csv(FILE_YOUTUBE, low_memory=False)\n",
    "\n",
    "print(f\"   - Total Data: {len(df_total):,}í–‰ (í”Œë«í¼: {df_total['platform'].unique()})\")\n",
    "print(f\"   - YouTube Final: {len(df_youtube):,}í–‰\")\n",
    "\n",
    "# ë‹¨ìˆœ ë³‘í•© (Concat)\n",
    "print(\"\\nğŸ”— ë°ì´í„° ë³‘í•© ì¤‘...\")\n",
    "merged_df = pd.concat([df_total, df_youtube], ignore_index=True)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. ë°ì´í„° ì •ì œ (ì¤‘ë³µ ì œê±° ë° í˜•ì‹ í†µì¼)\n",
    "# ============================================================================\n",
    "print(\"ğŸ§¹ ë°ì´í„° ì •ì œ ì¤‘ (ì¤‘ë³µ ì œê±°)...\")\n",
    "\n",
    "# 1. ì»¬ëŸ¼ í˜•ì‹ ì•ˆì „í™” (ë¬¸ìì—´ ë³€í™˜)\n",
    "merged_df['STD_DATE'] = merged_df['STD_DATE'].astype(str).str.split('.').str[0] # 20250901.0 -> 20250901\n",
    "merged_df['content'] = merged_df['content'].fillna('').astype(str)\n",
    "merged_df['id'] = merged_df['id'].fillna('').astype(str)\n",
    "\n",
    "# 2. ì°¸ì—¬ë„(Engagement) ìˆ«ì ë³€í™˜ (ì •ë ¬ ë° ì¤‘ë³µ ì œê±° ê¸°ì¤€)\n",
    "merged_df['engagement'] = pd.to_numeric(merged_df['engagement'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# 3. ì¤‘ë³µ ì œê±° (í•µì‹¬ ë¡œì§)\n",
    "# (1) engagement(ì°¸ì—¬ë„)ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬ -> ì¤‘ë³µ ì‹œ ì¸ê¸° ìˆëŠ” ë°ì´í„°ë¥¼ ì‚´ë¦¼\n",
    "merged_df = merged_df.sort_values(by='engagement', ascending=False)\n",
    "\n",
    "# (2) ID ê¸°ì¤€ ì¤‘ë³µ ì œê±° (í”Œë«í¼ë³„ë¡œ IDê°€ ê²¹ì¹  ìˆ˜ ìˆìœ¼ë¯€ë¡œ í”Œë«í¼+ID ì¡°í•© í™•ì¸)\n",
    "#     ë‹¨, ìœ íŠœë¸Œ ë“±ì—ì„œ IDê°€ í™•ì‹¤í•˜ë‹¤ë©´ IDë§Œìœ¼ë¡œë„ ì¶©ë¶„í•˜ì§€ë§Œ ì•ˆì „í•˜ê²Œ í”Œë«í¼ í¬í•¨\n",
    "merged_df = merged_df.drop_duplicates(subset=['platform', 'id'], keep='first')\n",
    "\n",
    "# (3) ë‚´ìš© ê¸°ì¤€ ì¤‘ë³µ ì œê±° (IDê°€ ì—†ê±°ë‚˜ ë‹¬ë¼ë„, ë‚ ì§œ/ë‚´ìš©/í”Œë«í¼ì´ ê°™ìœ¼ë©´ ì¤‘ë³µ)\n",
    "merged_df = merged_df.drop_duplicates(subset=['STD_DATE', 'platform', 'content'], keep='first')\n",
    "\n",
    "# 4. ë‚ ì§œ í•„í„°ë§ (20250901 ~ 20251031) - í”„ë¡œì íŠ¸ ë²”ìœ„ ì™¸ ë°ì´í„° ì‚­ì œ\n",
    "merged_df = merged_df[\n",
    "    (merged_df['STD_DATE'] >= '20250901') & \n",
    "    (merged_df['STD_DATE'] <= '20251031')\n",
    "]\n",
    "\n",
    "# 5. ìµœì¢… ì •ë ¬: ë‚ ì§œ -> í”Œë«í¼ -> ì°¸ì—¬ë„\n",
    "merged_df = merged_df.sort_values(by=['STD_DATE', 'platform', 'engagement'], ascending=[True, True, False])\n",
    "\n",
    "# ============================================================================\n",
    "# 4. ìµœì¢… ì €ì¥\n",
    "# ============================================================================\n",
    "# í•„ìˆ˜ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ ì €ì¥\n",
    "target_cols = ['STD_DATE', 'platform', 'type', 'content', 'engagement', 'author', 'url', 'original_date', 'id']\n",
    "merged_df = merged_df[target_cols]\n",
    "\n",
    "merged_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ‰ ìµœì¢… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“‚ ì €ì¥ ê²½ë¡œ: {OUTPUT_FILE}\")\n",
    "print(f\"ğŸ“Š ì´ ë°ì´í„°: {len(merged_df):,}í–‰\")\n",
    "print(f\"ğŸ“… ê¸°ê°„: {merged_df['STD_DATE'].min()} ~ {merged_df['STD_DATE'].max()}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# í”Œë«í¼ë³„ ë¶„í¬ í™•ì¸\n",
    "print(\"\\n[í”Œë«í¼ë³„ ë°ì´í„° ê°œìˆ˜]\")\n",
    "print(merged_df['platform'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-project-01-team6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
