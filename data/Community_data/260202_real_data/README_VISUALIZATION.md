# October 2025 Crypto Crash - ì»¤ë®¤ë‹ˆí‹° ë°ì´í„° ì‹œê°í™”

## ğŸ“Š ë°ì´í„°ì…‹ ê°œìš”
- **ì´ ë ˆì½”ë“œ**: 145ê°œ ì‹¤ì œ ì»¤ë®¤ë‹ˆí‹° í¬ìŠ¤íŠ¸/íŠ¸ìœ—
- **í”Œë«í¼**: X (Twitter) 83ê°œ, Reddit 62ê°œ
- **ë°ì´í„° ì†ŒìŠ¤**: 100% ì‹¤ì œ URL (í•©ì„± 0%)
- **ìˆ˜ì§‘ ê¸°ê°„**: 2025ë…„ 9ì›” ~ 11ì›”
- **ì£¼ì œ**: October 2025 $19B ì²­ì‚° ì´ë²¤íŠ¸

## ğŸ“ íŒŒì¼ êµ¬ì¡°
```
outputs/
â”œâ”€â”€ FINAL_COMMUNITY_DATASET_145.csv         # ë©”ì¸ ë°ì´í„°ì…‹
â”œâ”€â”€ FINAL_COMMUNITY_DATASET_145.json        # JSON í˜•ì‹
â”œâ”€â”€ dataset_statistics.json                 # í†µê³„ ìš”ì•½
â”œâ”€â”€ comprehensive_visualization.py          # ì‹œê°í™” ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ README_VISUALIZATION.md                 # ì´ ë¬¸ì„œ
```

## ğŸš€ VSCodeì—ì„œ ì‹¤í–‰í•˜ê¸°

### 1. í™˜ê²½ ì¤€ë¹„
í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ:
```bash
# UV í™˜ê²½ í™œì„±í™” (ì´ë¯¸ ì„¤ì¹˜ëœ ê²½ìš°)
uv sync

# ë˜ëŠ” í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì—†ë‹¤ë©´
uv pip install pandas numpy matplotlib seaborn wordcloud networkx scikit-learn textblob vadersentiment koreanize-matplotlib
```

### 2. ë°ì´í„° íŒŒì¼ ìœ„ì¹˜ í™•ì¸
ì‹œê°í™” ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒ íŒŒì¼ì´ **ê°™ì€ ë””ë ‰í† ë¦¬**ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤:
- `FINAL_COMMUNITY_DATASET_145.csv`
- `comprehensive_visualization.py`

**ë°©ë²• 1: íŒŒì¼ ì´ë™**
```bash
# outputs í´ë”ë¡œ ì´ë™
cd path/to/outputs

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python comprehensive_visualization.py
```

**ë°©ë²• 2: ê²½ë¡œ ìˆ˜ì •**
ìŠ¤í¬ë¦½íŠ¸ ë‚´ 15ë²ˆ ì¤„ì„ ìˆ˜ì •:
```python
# ë³€ê²½ ì „
df = pd.read_csv('FINAL_COMMUNITY_DATASET_145.csv')

# ë³€ê²½ í›„ (ì ˆëŒ€ ê²½ë¡œ ë˜ëŠ” ìƒëŒ€ ê²½ë¡œ)
df = pd.read_csv('outputs/FINAL_COMMUNITY_DATASET_145.csv')
```

### 3. ì‹¤í–‰
```bash
# Pythonìœ¼ë¡œ ì§ì ‘ ì‹¤í–‰
python comprehensive_visualization.py

# ë˜ëŠ” UVë¡œ ì‹¤í–‰
uv run python comprehensive_visualization.py
```

### 4. ê²°ê³¼ í™•ì¸
ê°™ì€ ë””ë ‰í† ë¦¬ì— 10ê°œì˜ PNG íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤:
1. `01_platform_distribution.png` - í”Œë«í¼ ë¶„í¬
2. `02_category_distribution.png` - ì¹´í…Œê³ ë¦¬ ë¶„í¬
3. `03_sentiment_analysis.png` - ê°ì • ë¶„ì„
4. `04_time_period_distribution.png` - ì‹œê°„ëŒ€ ë¶„í¬
5. `05_influence_score_analysis.png` - ì˜í–¥ë ¥ ë¶„ì„
6. `06_wordclouds.png` - ì›Œë“œí´ë¼ìš°ë“œ
7. `07_keyword_heatmap.png` - í‚¤ì›Œë“œ íˆíŠ¸ë§µ
8. `08_keyword_cooccurrence.png` - í‚¤ì›Œë“œ ê³µë™ ì¶œí˜„
9. `09_network_graph.png` - ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„
10. `10_comprehensive_dashboard.png` - ì¢…í•© ëŒ€ì‹œë³´ë“œ

## ğŸ“ˆ ìƒì„±ë˜ëŠ” ì‹œê°í™”

### 1. **í”Œë«í¼ ë¶„í¬** (íŒŒì´ ì°¨íŠ¸ + ë°” ì°¨íŠ¸)
- X vs Reddit ë¹„ìœ¨
- ê° í”Œë«í¼ë³„ í¬ìŠ¤íŠ¸ ìˆ˜

### 2. **ì¹´í…Œê³ ë¦¬ ë¶„í¬** (ìˆ˜í‰ ë°” ì°¨íŠ¸)
- 10ê°œ ì¹´í…Œê³ ë¦¬ë³„ í¬ìŠ¤íŠ¸ ìˆ˜
- ë°±ë¶„ìœ¨ í‘œì‹œ

### 3. **ê°ì • ë¶„ì„** (ë„ë„› ì°¨íŠ¸ + ìŠ¤íƒ ë°”)
- 5ê°€ì§€ ê°ì • (Very_Negative, Negative, Neutral, Mixed, Positive)
- í”Œë«í¼ë³„ ê°ì • ë¹„êµ

### 4. **ì‹œê°„ëŒ€ ë¶„ì„** (ìˆ˜í‰ ë°” ì°¨íŠ¸)
- October 10, October 11, October 2025 ë“±
- í¬ë˜ì‹œ ì „í›„ ë¶„í¬

### 5. **ì˜í–¥ë ¥ ì ìˆ˜ ë¶„ì„** (4ê°œ ì„œë¸Œí”Œë¡¯)
- ì „ì²´ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
- í”Œë«í¼ë³„ ë°•ìŠ¤í”Œë¡¯
- ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ì˜í–¥ë ¥
- ê°ì •ë³„ í‰ê·  ì˜í–¥ë ¥

### 6. **ì›Œë“œí´ë¼ìš°ë“œ** (4ê°œ)
- ì „ì²´ í¬ìŠ¤íŠ¸
- X (Twitter) ì „ìš©
- Reddit ì „ìš©
- ë¶€ì • ê°ì • í¬ìŠ¤íŠ¸

### 7. **í‚¤ì›Œë“œ íˆíŠ¸ë§µ** (2ê°œ)
- í”Œë«í¼ë³„ í‚¤ì›Œë“œ ë¹ˆë„
- ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë¹ˆë„ (ìƒìœ„ 8ê°œ)

### 8. **í‚¤ì›Œë“œ ê³µë™ ì¶œí˜„ íˆíŠ¸ë§µ**
- 15ê°œ í‚¤ì›Œë“œ ê°„ ë™ì‹œ ì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤
- ì–´ë–¤ í‚¤ì›Œë“œê°€ í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ”ì§€ ë¶„ì„

### 9. **ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„**
- ìƒìœ„ 15ëª… ì‘ì„±ìì™€ ì¹´í…Œê³ ë¦¬ ê´€ê³„
- íŒŒë€ìƒ‰ ë…¸ë“œ: ì‘ì„±ì
- ì£¼í™©ìƒ‰ ë…¸ë“œ: ì¹´í…Œê³ ë¦¬
- ì—£ì§€ ë‘ê»˜: í¬ìŠ¤íŠ¸ ìˆ˜

### 10. **ì¢…í•© ëŒ€ì‹œë³´ë“œ**
- 7ê°œ ì„œë¸Œí”Œë¡¯ì— ëª¨ë“  í•µì‹¬ ì •ë³´
- í•œ ëˆˆì— ë³´ëŠ” ë°ì´í„° ìš”ì•½
- í†µê³„ ì •ë³´ ë°•ìŠ¤

## ğŸ¨ ì»¤ìŠ¤í„°ë§ˆì´ì§•

### ìƒ‰ìƒ ë³€ê²½
```python
# í”Œë«í¼ ìƒ‰ìƒ (45-46ë²ˆ ì¤„)
colors = ['#1DA1F2', '#FF4500']  # Twitter blue, Reddit orange

# ê°ì • ìƒ‰ìƒ (84-86ë²ˆ ì¤„)
colors_sent = {'Very_Negative': '#d62728', 'Negative': '#ff7f0e', 
               'Neutral': '#7f7f7f', 'Mixed': '#bcbd22', 'Positive': '#2ca02c'}
```

### í•´ìƒë„ ë³€ê²½
```python
# ê° plt.savefig() í˜¸ì¶œì—ì„œ dpi ì¡°ì •
plt.savefig('filename.png', dpi=300, bbox_inches='tight')  # í˜„ì¬: 300 DPI

# ë” ë†’ì€ í•´ìƒë„: dpi=600
# ë¹ ë¥¸ ë¯¸ë¦¬ë³´ê¸°: dpi=150
```

### ì›Œë“œí´ë¼ìš°ë“œ ë‹¨ì–´ ìˆ˜ ì¡°ì •
```python
# 175ë²ˆ ì¤„ ê·¼ì²˜
wc_all = WordCloud(width=800, height=400, background_color='white', 
                   stopwords=stopwords, colormap='viridis', 
                   max_words=100,  # ì—¬ê¸°ë¥¼ ë³€ê²½ (í˜„ì¬ 100ê°œ)
                   relative_scaling=0.5).generate(all_text)
```

### ë¶ˆìš©ì–´ ì¶”ê°€
```python
# 168-173ë²ˆ ì¤„
stopwords = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
                'of', 'with', 'by', 'from', 'up', 'about', 'into', 'through', 'during',
                'is', 'was', 'are', 'were', 'been', 'be', 'have', 'has', 'had', 'do',
                'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might',
                'can', 'this', 'that', 'these', 'those', 'it', 'its', 'as'])

# ì—¬ê¸°ì— ì¶”ê°€:
stopwords.add('your_word')
```

## ğŸ”§ ë¬¸ì œ í•´ê²°

### í•œê¸€ í°íŠ¸ ì˜¤ë¥˜
```bash
# koreanize-matplotlib ì„¤ì¹˜
uv pip install koreanize-matplotlib

# ë˜ëŠ” ìˆ˜ë™ìœ¼ë¡œ í°íŠ¸ ì§€ì •
plt.rcParams['font.family'] = 'NanumGothic'  # ë˜ëŠ” ë‹¤ë¥¸ í•œê¸€ í°íŠ¸
```

### ë©”ëª¨ë¦¬ ë¶€ì¡±
í° ë°ì´í„°ì…‹ì—ì„œ ë©”ëª¨ë¦¬ ë¬¸ì œê°€ ë°œìƒí•˜ë©´:
```python
# ê° ì‹œê°í™” í›„ ë©”ëª¨ë¦¬ í•´ì œ
plt.close('all')
import gc
gc.collect()
```

### íŒŒì¼ ê²½ë¡œ ì˜¤ë¥˜
Windowsì—ì„œ:
```python
df = pd.read_csv(r'C:\path\to\FINAL_COMMUNITY_DATASET_145.csv')
```

Mac/Linuxì—ì„œ:
```python
df = pd.read_csv('/path/to/FINAL_COMMUNITY_DATASET_145.csv')
```

## ğŸ“Š ë°ì´í„° ì»¬ëŸ¼ ì„¤ëª…

| ì»¬ëŸ¼ëª… | íƒ€ì… | ì„¤ëª… |
|--------|------|------|
| `platform` | str | í”Œë«í¼ (X ë˜ëŠ” Reddit) |
| `url` | str | ì›ë³¸ URL |
| `author` | str | ì‘ì„±ì (@username ë˜ëŠ” r/subreddit) |
| `title` | str | í¬ìŠ¤íŠ¸/íŠ¸ìœ— ì œëª© |
| `category` | str | ì¹´í…Œê³ ë¦¬ (10ê°œ) |
| `sentiment` | str | ê°ì • (5ê°œ) |
| `influence_score` | float | ì˜í–¥ë ¥ ì ìˆ˜ (0-60) |
| `time_period` | str | ì‹œê°„ëŒ€ ë¶„ë¥˜ |
| `kw_*` | bool | í‚¤ì›Œë“œ í”Œë˜ê·¸ (15ê°œ) |
| `source_detail` | str | ìƒì„¸ ì†ŒìŠ¤ ì •ë³´ |

## ğŸ’¡ ì¶”ê°€ ë¶„ì„ ì•„ì´ë””ì–´

### Jupyter Notebookì—ì„œ ì¸í„°ë™í‹°ë¸Œ ë¶„ì„
```python
import pandas as pd
import plotly.express as px

df = pd.read_csv('FINAL_COMMUNITY_DATASET_145.csv')

# ì¸í„°ë™í‹°ë¸Œ ì‚°ì ë„
fig = px.scatter(df, x='influence_score', y='category', 
                color='sentiment', hover_data=['title', 'author'],
                title='Influence Score by Category')
fig.show()
```

### ì‹œê³„ì—´ ë¶„ì„ (ë‚ ì§œ ì¶”ê°€ ì‹œ)
```python
# ë§Œì•½ ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆë‹¤ë©´
df['date'] = pd.to_datetime(df['date'])
daily_counts = df.groupby('date').size()
daily_counts.plot(kind='line', figsize=(12, 6))
```

### Topic Modeling (LDA)
```python
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(max_features=100, stop_words='english')
X = vectorizer.fit_transform(df['title'])

lda = LatentDirichletAllocation(n_components=5, random_state=42)
lda.fit(X)

# í† í”½ë³„ ì£¼ìš” ë‹¨ì–´
for idx, topic in enumerate(lda.components_):
    print(f"Topic {idx}:", [vectorizer.get_feature_names_out()[i] 
                           for i in topic.argsort()[-10:]])
```

## ğŸ“ ì°¸ê³ ì‚¬í•­
- ëª¨ë“  ì‹œê°í™”ëŠ” 300 DPI ê³ í•´ìƒë„ë¡œ ì €ì¥
- ìƒ‰ìƒì€ colorblind-friendly íŒ”ë ˆíŠ¸ ì‚¬ìš©
- ê·¸ë¦¬ë“œì™€ ë ˆì´ë¸”ë¡œ ê°€ë…ì„± ìµœì í™”
- ê° ì°¨íŠ¸ì— ëª…í™•í•œ ì œëª©ê³¼ ë²”ë¡€ í¬í•¨

## ğŸ¤ ê¸°ì—¬
ê°œì„  ì‚¬í•­ì´ë‚˜ ë²„ê·¸ ë¦¬í¬íŠ¸ëŠ” ì´ìŠˆë¡œ ì œì¶œí•´ì£¼ì„¸ìš”!

## ğŸ“„ ë¼ì´ì„ ìŠ¤
ì´ í”„ë¡œì íŠ¸ì˜ ë°ì´í„°ëŠ” ê³µê°œ ì†ŒìŠ¤(X, Reddit)ì—ì„œ ìˆ˜ì§‘ë˜ì—ˆìœ¼ë©°, êµìœ¡ ë° ì—°êµ¬ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.
