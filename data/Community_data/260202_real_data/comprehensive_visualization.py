#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
October 2025 Crypto Crash - ì»¤ë®¤ë‹ˆí‹° ë°ì´í„° ì¢…í•© ì‹œê°í™”
145ê°œ ì‹¤ì œ URL ê¸°ë°˜ ë¹„ì •í˜• ë°ì´í„° ë¶„ì„

Required packages (from pyproject.toml):
- pandas, numpy, matplotlib, seaborn
- wordcloud, networkx
- scikit-learn, textblob, vadersentiment
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from collections import Counter
import networkx as nx
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from textblob import TextBlob
from vadersentiment.vaderSentiment import SentimentIntensityAnalyzer
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì • (koreanize-matplotlib ì‚¬ìš©)
try:
    import koreanize_matplotlib
    koreanize_matplotlib.matplotlib_settings()
except:
    plt.rcParams['font.family'] = 'DejaVu Sans'

# ìŠ¤íƒ€ì¼ ì„¤ì •
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("=" * 100)
print("ì»¤ë®¤ë‹ˆí‹° ë°ì´í„° ì¢…í•© ì‹œê°í™” ì‹œì‘")
print("=" * 100)

# ============================================================================
# 1. ë°ì´í„° ë¡œë“œ
# ============================================================================
print("\n[1/12] ë°ì´í„° ë¡œë“œ ì¤‘...")

df = pd.read_csv('FINAL_COMMUNITY_DATASET_145.csv')
print(f"âœ… {len(df)}ê°œ ë ˆì½”ë“œ ë¡œë“œ ì™„ë£Œ")
print(f"   ì»¬ëŸ¼: {len(df.columns)}ê°œ")
print(f"   í”Œë«í¼: X {len(df[df['platform']=='X'])}ê°œ, Reddit {len(df[df['platform']=='Reddit'])}ê°œ")

# ============================================================================
# 2. ê¸°ë³¸ í†µê³„ í™•ì¸
# ============================================================================
print("\n[2/12] ì›ë³¸ ë°ì´í„° í™•ì¸...")

# ìƒìœ„ 10ê°œ ì¶œë ¥
print("\nğŸ“Š ë°ì´í„° ìƒ˜í”Œ (ìƒìœ„ 5ê°œ):")
print(df[['platform', 'author', 'title', 'category', 'sentiment', 'influence_score']].head())

print("\nğŸ“ˆ ê¸°ë³¸ í†µê³„:")
print(f"  ì¹´í…Œê³ ë¦¬ ìˆ˜: {df['category'].nunique()}ê°œ")
print(f"  ê³ ìœ  ì‘ì„±ì: {df['author'].nunique()}ëª…")
print(f"  í‰ê·  ì˜í–¥ë ¥ ì ìˆ˜: {df['influence_score'].mean():.2f}")
print(f"  ìµœëŒ€ ì˜í–¥ë ¥ ì ìˆ˜: {df['influence_score'].max():.2f}")

# ============================================================================
# 3. í”Œë«í¼ë³„ ë¶„í¬ (íŒŒì´ ì°¨íŠ¸)
# ============================================================================
print("\n[3/12] í”Œë«í¼ë³„ ë¶„í¬ ì‹œê°í™”...")

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# íŒŒì´ ì°¨íŠ¸
platform_counts = df['platform'].value_counts()
colors = ['#1DA1F2', '#FF4500']  # Twitter blue, Reddit orange
axes[0].pie(platform_counts, labels=platform_counts.index, autopct='%1.1f%%',
           colors=colors, startangle=90, textprops={'fontsize': 12})
axes[0].set_title('Platform Distribution', fontsize=14, fontweight='bold')

# ë°” ì°¨íŠ¸
axes[1].bar(platform_counts.index, platform_counts.values, color=colors, alpha=0.7, edgecolor='black')
axes[1].set_ylabel('Number of Posts', fontsize=12)
axes[1].set_title('Posts by Platform', fontsize=14, fontweight='bold')
axes[1].grid(axis='y', alpha=0.3)
for i, v in enumerate(platform_counts.values):
    axes[1].text(i, v + 2, str(v), ha='center', va='bottom', fontsize=11, fontweight='bold')

plt.tight_layout()
plt.savefig('01_platform_distribution.png', dpi=300, bbox_inches='tight')
print("âœ… ì €ì¥: 01_platform_distribution.png")
plt.close()

# ============================================================================
# 4. ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬ (ìˆ˜í‰ ë°” ì°¨íŠ¸)
# ============================================================================
print("\n[4/12] ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬ ì‹œê°í™”...")

fig, ax = plt.subplots(figsize=(12, 8))

category_counts = df['category'].value_counts().sort_values()
colors_cat = plt.cm.Spectral(np.linspace(0, 1, len(category_counts)))

category_counts.plot(kind='barh', ax=ax, color=colors_cat, edgecolor='black', linewidth=0.8)
ax.set_xlabel('Number of Posts', fontsize=12, fontweight='bold')
ax.set_ylabel('Category', fontsize=12, fontweight='bold')
ax.set_title('Post Distribution by Category', fontsize=14, fontweight='bold')
ax.grid(axis='x', alpha=0.3)

# ê°’ í‘œì‹œ
for i, v in enumerate(category_counts.values):
    ax.text(v + 0.5, i, f'{v} ({v/len(df)*100:.1f}%)', 
            va='center', fontsize=10)

plt.tight_layout()
plt.savefig('02_category_distribution.png', dpi=300, bbox_inches='tight')
print("âœ… ì €ì¥: 02_category_distribution.png")
plt.close()

# ============================================================================
# 5. ê°ì • ë¶„ì„ (ìŠ¤íƒ ë°” ì°¨íŠ¸ + ë„ë„› ì°¨íŠ¸)
# ============================================================================
print("\n[5/12] ê°ì • ë¶„ì„ ì‹œê°í™”...")

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# ë„ë„› ì°¨íŠ¸
sentiment_counts = df['sentiment'].value_counts()
colors_sent = {'Very_Negative': '#d62728', 'Negative': '#ff7f0e', 
               'Neutral': '#7f7f7f', 'Mixed': '#bcbd22', 'Positive': '#2ca02c'}
colors_list = [colors_sent.get(s, '#333333') for s in sentiment_counts.index]

wedges, texts, autotexts = axes[0].pie(sentiment_counts, labels=sentiment_counts.index, 
                                        autopct='%1.1f%%', colors=colors_list,
                                        startangle=90, pctdistance=0.85,
                                        textprops={'fontsize': 10})
centre_circle = plt.Circle((0, 0), 0.70, fc='white')
axes[0].add_artist(centre_circle)
axes[0].set_title('Sentiment Distribution', fontsize=14, fontweight='bold')

# í”Œë«í¼ë³„ ê°ì • ë¹„êµ
sentiment_platform = pd.crosstab(df['platform'], df['sentiment'])
sentiment_platform_pct = sentiment_platform.div(sentiment_platform.sum(axis=1), axis=0) * 100

sentiment_platform_pct.plot(kind='bar', stacked=True, ax=axes[1], 
                            color=[colors_sent.get(s, '#333333') for s in sentiment_platform_pct.columns],
                            edgecolor='black', linewidth=0.8)
axes[1].set_ylabel('Percentage (%)', fontsize=12)
axes[1].set_xlabel('Platform', fontsize=12)
axes[1].set_title('Sentiment by Platform', fontsize=14, fontweight='bold')
axes[1].legend(title='Sentiment', bbox_to_anchor=(1.05, 1), loc='upper left')
axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=0)
axes[1].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('03_sentiment_analysis.png', dpi=300, bbox_inches='tight')
print("âœ… ì €ì¥: 03_sentiment_analysis.png")
plt.close()

# ============================================================================
# 6. ì‹œê°„ëŒ€ë³„ ë¶„ì„ (ë°” ì°¨íŠ¸)
# ============================================================================
print("\n[6/12] ì‹œê°„ëŒ€ë³„ ë¶„ì„ ì‹œê°í™”...")

fig, ax = plt.subplots(figsize=(14, 6))

time_counts = df['time_period'].value_counts().sort_values()
colors_time = plt.cm.viridis(np.linspace(0, 1, len(time_counts)))

time_counts.plot(kind='barh', ax=ax, color=colors_time, edgecolor='black', linewidth=0.8)
ax.set_xlabel('Number of Posts', fontsize=12, fontweight='bold')
ax.set_ylabel('Time Period', fontsize=12, fontweight='bold')
ax.set_title('Post Distribution by Time Period', fontsize=14, fontweight='bold')
ax.grid(axis='x', alpha=0.3)

for i, v in enumerate(time_counts.values):
    ax.text(v + 0.5, i, str(v), va='center', fontsize=10)

plt.tight_layout()
plt.savefig('04_time_period_distribution.png', dpi=300, bbox_inches='tight')
print("âœ… ì €ì¥: 04_time_period_distribution.png")
plt.close()

# ============================================================================
# 7. ì˜í–¥ë ¥ ì ìˆ˜ ë¶„ì„ (íˆìŠ¤í† ê·¸ë¨ + ë°•ìŠ¤í”Œë¡¯)
# ============================================================================
print("\n[7/12] ì˜í–¥ë ¥ ì ìˆ˜ ë¶„ì„...")

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# íˆìŠ¤í† ê·¸ë¨
axes[0, 0].hist(df['influence_score'], bins=30, color='skyblue', edgecolor='black', alpha=0.7)
axes[0, 0].axvline(df['influence_score'].mean(), color='red', linestyle='--', 
                   linewidth=2, label=f'Mean: {df["influence_score"].mean():.2f}')
axes[0, 0].axvline(df['influence_score'].median(), color='green', linestyle='--', 
                   linewidth=2, label=f'Median: {df["influence_score"].median():.2f}')
axes[0, 0].set_xlabel('Influence Score', fontsize=12)
axes[0, 0].set_ylabel('Frequency', fontsize=12)
axes[0, 0].set_title('Influence Score Distribution', fontsize=14, fontweight='bold')
axes[0, 0].legend()
axes[0, 0].grid(alpha=0.3)

# í”Œë«í¼ë³„ ë°•ìŠ¤í”Œë¡¯
df.boxplot(column='influence_score', by='platform', ax=axes[0, 1], 
           patch_artist=True, grid=True)
axes[0, 1].set_title('Influence Score by Platform', fontsize=14, fontweight='bold')
axes[0, 1].set_xlabel('Platform', fontsize=12)
axes[0, 1].set_ylabel('Influence Score', fontsize=12)
plt.sca(axes[0, 1])
plt.xticks(rotation=0)

# ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ì˜í–¥ë ¥ (ìƒìœ„ 10ê°œ)
category_influence = df.groupby('category')['influence_score'].mean().sort_values(ascending=False).head(10)
axes[1, 0].barh(range(len(category_influence)), category_influence.values, 
                color=plt.cm.plasma(np.linspace(0, 1, len(category_influence))),
                edgecolor='black', linewidth=0.8)
axes[1, 0].set_yticks(range(len(category_influence)))
axes[1, 0].set_yticklabels(category_influence.index, fontsize=10)
axes[1, 0].set_xlabel('Average Influence Score', fontsize=12)
axes[1, 0].set_title('Top 10 Categories by Avg Influence', fontsize=14, fontweight='bold')
axes[1, 0].grid(axis='x', alpha=0.3)

# ê°ì •ë³„ ì˜í–¥ë ¥
sentiment_influence = df.groupby('sentiment')['influence_score'].mean().sort_values(ascending=False)
axes[1, 1].bar(range(len(sentiment_influence)), sentiment_influence.values,
               color=[colors_sent.get(s, '#333333') for s in sentiment_influence.index],
               edgecolor='black', linewidth=0.8, alpha=0.8)
axes[1, 1].set_xticks(range(len(sentiment_influence)))
axes[1, 1].set_xticklabels(sentiment_influence.index, rotation=45, ha='right')
axes[1, 1].set_ylabel('Average Influence Score', fontsize=12)
axes[1, 1].set_title('Average Influence by Sentiment', fontsize=14, fontweight='bold')
axes[1, 1].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('05_influence_score_analysis.png', dpi=300, bbox_inches='tight')
print("âœ… ì €ì¥: 05_influence_score_analysis.png")
plt.close()

# ============================================================================
# 8. ì›Œë“œí´ë¼ìš°ë“œ (ì „ì²´ + í”Œë«í¼ë³„)
# ============================================================================
print("\n[8/12] ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±...")

# ì œëª©ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
all_text = ' '.join(df['title'].astype(str))

# ë¶ˆìš©ì–´ ì„¤ì •
stopwords = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
                'of', 'with', 'by', 'from', 'up', 'about', 'into', 'through', 'during',
                'is', 'was', 'are', 'were', 'been', 'be', 'have', 'has', 'had', 'do',
                'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might',
                'can', 'this', 'that', 'these', 'those', 'it', 'its', 'as'])

fig, axes = plt.subplots(2, 2, figsize=(20, 16))

# ì „ì²´ ì›Œë“œí´ë¼ìš°ë“œ
wc_all = WordCloud(width=800, height=400, background_color='white', 
                   stopwords=stopwords, colormap='viridis', 
                   max_words=100, relative_scaling=0.5).generate(all_text)
axes[0, 0].imshow(wc_all, interpolation='bilinear')
axes[0, 0].axis('off')
axes[0, 0].set_title('All Posts - Word Cloud', fontsize=16, fontweight='bold', pad=20)

# X (Twitter) ì›Œë“œí´ë¼ìš°ë“œ
twitter_text = ' '.join(df[df['platform'] == 'X']['title'].astype(str))
wc_twitter = WordCloud(width=800, height=400, background_color='white',
                      stopwords=stopwords, colormap='Blues',
                      max_words=80, relative_scaling=0.5).generate(twitter_text)
axes[0, 1].imshow(wc_twitter, interpolation='bilinear')
axes[0, 1].axis('off')
axes[0, 1].set_title('X (Twitter) Posts - Word Cloud', fontsize=16, fontweight='bold', pad=20)

# Reddit ì›Œë“œí´ë¼ìš°ë“œ
reddit_text = ' '.join(df[df['platform'] == 'Reddit']['title'].astype(str))
wc_reddit = WordCloud(width=800, height=400, background_color='white',
                     stopwords=stopwords, colormap='Oranges',
                     max_words=80, relative_scaling=0.5).generate(reddit_text)
axes[1, 0].imshow(wc_reddit, interpolation='bilinear')
axes[1, 0].axis('off')
axes[1, 0].set_title('Reddit Posts - Word Cloud', fontsize=16, fontweight='bold', pad=20)

# ë¶€ì • ê°ì • ì›Œë“œí´ë¼ìš°ë“œ
negative_text = ' '.join(df[df['sentiment'].isin(['Negative', 'Very_Negative'])]['title'].astype(str))
wc_negative = WordCloud(width=800, height=400, background_color='white',
                       stopwords=stopwords, colormap='Reds',
                       max_words=80, relative_scaling=0.5).generate(negative_text)
axes[1, 1].imshow(wc_negative, interpolation='bilinear')
axes[1, 1].axis('off')
axes[1, 1].set_title('Negative Sentiment - Word Cloud', fontsize=16, fontweight='bold', pad=20)

plt.tight_layout()
plt.savefig('06_wordclouds.png', dpi=300, bbox_inches='tight')
print("âœ… ì €ì¥: 06_wordclouds.png")
plt.close()

# ============================================================================
# 9. í‚¤ì›Œë“œ íˆíŠ¸ë§µ
# ============================================================================
print("\n[9/12] í‚¤ì›Œë“œ íˆíŠ¸ë§µ ìƒì„±...")

# í‚¤ì›Œë“œ ì»¬ëŸ¼ ì¶”ì¶œ
keyword_cols = [col for col in df.columns if col.startswith('kw_')]
keyword_data = df[keyword_cols].astype(int)

# í‚¤ì›Œë“œë³„ ì¶œí˜„ ë¹ˆë„
keyword_freq = keyword_data.sum().sort_values(ascending=False)

# ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ì¶œí˜„
category_keyword = pd.DataFrame()
for category in df['category'].unique():
    category_data = df[df['category'] == category][keyword_cols].sum()
    category_keyword[category] = category_data

# ì •ê·œí™” (ê° ì¹´í…Œê³ ë¦¬ë³„ ë¹„ìœ¨)
category_keyword_norm = category_keyword.div(df['category'].value_counts(), axis=1).fillna(0)

# íˆíŠ¸ë§µ
fig, axes = plt.subplots(2, 1, figsize=(16, 14))

# í”Œë«í¼ë³„ í‚¤ì›Œë“œ íˆíŠ¸ë§µ
platform_keyword = pd.DataFrame()
for platform in df['platform'].unique():
    platform_data = df[df['platform'] == platform][keyword_cols].sum()
    platform_keyword[platform] = platform_data

platform_keyword_norm = platform_keyword.div(df['platform'].value_counts(), axis=1).T
sns.heatmap(platform_keyword_norm, annot=True, fmt='.2f', cmap='YlOrRd', 
           cbar_kws={'label': 'Keyword Frequency per Post'},
           linewidths=0.5, ax=axes[0])
axes[0].set_title('Keyword Frequency by Platform (Normalized)', fontsize=14, fontweight='bold')
axes[0].set_xlabel('')
axes[0].set_yticklabels([col.replace('kw_', '') for col in platform_keyword_norm.index], rotation=0)

# ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ íˆíŠ¸ë§µ (ìƒìœ„ 8ê°œ ì¹´í…Œê³ ë¦¬)
top_categories = df['category'].value_counts().head(8).index
category_keyword_top = category_keyword_norm[top_categories].T
sns.heatmap(category_keyword_top, annot=True, fmt='.2f', cmap='viridis',
           cbar_kws={'label': 'Keyword Frequency per Post'},
           linewidths=0.5, ax=axes[1])
axes[1].set_title('Keyword Frequency by Top 8 Categories (Normalized)', fontsize=14, fontweight='bold')
axes[1].set_xlabel('Category', fontsize=12)
axes[1].set_yticklabels([col.replace('kw_', '') for col in category_keyword_top.index], rotation=0)

plt.tight_layout()
plt.savefig('07_keyword_heatmap.png', dpi=300, bbox_inches='tight')
print("âœ… ì €ì¥: 07_keyword_heatmap.png")
plt.close()

# ============================================================================
# 10. ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ (í‚¤ì›Œë“œ ê³µë™ ì¶œí˜„)
# ============================================================================
print("\n[10/12] í‚¤ì›Œë“œ ê³µë™ ì¶œí˜„ íˆíŠ¸ë§µ...")

# í‚¤ì›Œë“œ ê³µë™ ì¶œí˜„ í–‰ë ¬
keyword_cooccurrence = keyword_data.T.dot(keyword_data)

# ëŒ€ê°ì„  ì œê±° (ìê¸° ìì‹ ê³¼ì˜ ìƒê´€ê´€ê³„)
np.fill_diagonal(keyword_cooccurrence.values, 0)

fig, ax = plt.subplots(figsize=(14, 12))
sns.heatmap(keyword_cooccurrence, annot=True, fmt='d', cmap='coolwarm',
           cbar_kws={'label': 'Co-occurrence Count'},
           linewidths=0.5, ax=ax, square=True)
ax.set_title('Keyword Co-occurrence Matrix', fontsize=14, fontweight='bold')
ax.set_xticklabels([col.replace('kw_', '') for col in keyword_cooccurrence.columns], rotation=45, ha='right')
ax.set_yticklabels([col.replace('kw_', '') for col in keyword_cooccurrence.index], rotation=0)

plt.tight_layout()
plt.savefig('08_keyword_cooccurrence.png', dpi=300, bbox_inches='tight')
print("âœ… ì €ì¥: 08_keyword_cooccurrence.png")
plt.close()

# ============================================================================
# 11. ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ (ì‘ì„±ì-ì¹´í…Œê³ ë¦¬ ê´€ê³„)
# ============================================================================
print("\n[11/12] ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±...")

# ìƒìœ„ ì‘ì„±ì ì„ íƒ (5íšŒ ì´ìƒ ë“±ì¥)
author_counts = df['author'].value_counts()
top_authors = author_counts[author_counts >= 3].index[:15]

# ë„¤íŠ¸ì›Œí¬ ìƒì„±
G = nx.Graph()

for author in top_authors:
    author_posts = df[df['author'] == author]
    for category in author_posts['category'].unique():
        count = len(author_posts[author_posts['category'] == category])
        G.add_edge(author, category, weight=count)

# ì‹œê°í™”
fig, ax = plt.subplots(figsize=(16, 12))

pos = nx.spring_layout(G, k=2, iterations=50, seed=42)

# ë…¸ë“œ ìƒ‰ìƒ (ì‘ì„±ì vs ì¹´í…Œê³ ë¦¬)
node_colors = []
for node in G.nodes():
    if node in top_authors:
        node_colors.append('#1DA1F2')  # Blue for authors
    else:
        node_colors.append('#FF4500')  # Orange for categories

# ë…¸ë“œ í¬ê¸°
node_sizes = []
for node in G.nodes():
    if node in top_authors:
        node_sizes.append(1000)
    else:
        node_sizes.append(800)

# ì—£ì§€ ë‘ê»˜
edges = G.edges()
weights = [G[u][v]['weight'] for u, v in edges]
max_weight = max(weights) if weights else 1
edge_widths = [3 * w / max_weight for w in weights]

nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes, 
                       alpha=0.8, ax=ax)
nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.5, ax=ax)
nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold', ax=ax)

ax.set_title('Author-Category Network (Top 15 Authors, 3+ Posts)', 
            fontsize=14, fontweight='bold')
ax.axis('off')

# ë²”ë¡€
from matplotlib.patches import Patch
legend_elements = [Patch(facecolor='#1DA1F2', label='Authors'),
                  Patch(facecolor='#FF4500', label='Categories')]
ax.legend(handles=legend_elements, loc='upper right', fontsize=10)

plt.tight_layout()
plt.savefig('09_network_graph.png', dpi=300, bbox_inches='tight')
print("âœ… ì €ì¥: 09_network_graph.png")
plt.close()

# ============================================================================
# 12. ì¢…í•© ëŒ€ì‹œë³´ë“œ
# ============================================================================
print("\n[12/12] ì¢…í•© ëŒ€ì‹œë³´ë“œ ìƒì„±...")

fig = plt.figure(figsize=(20, 12))
gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

# 1. í”Œë«í¼ ë¶„í¬
ax1 = fig.add_subplot(gs[0, 0])
platform_counts.plot(kind='pie', ax=ax1, colors=colors, autopct='%1.1f%%',
                    startangle=90, textprops={'fontsize': 10})
ax1.set_title('Platform Distribution', fontsize=12, fontweight='bold')
ax1.set_ylabel('')

# 2. ìƒìœ„ ì¹´í…Œê³ ë¦¬
ax2 = fig.add_subplot(gs[0, 1:])
top_cats = df['category'].value_counts().head(8)
ax2.barh(range(len(top_cats)), top_cats.values, 
        color=plt.cm.Spectral(np.linspace(0, 1, len(top_cats))),
        edgecolor='black', linewidth=0.8)
ax2.set_yticks(range(len(top_cats)))
ax2.set_yticklabels(top_cats.index, fontsize=9)
ax2.set_xlabel('Count', fontsize=10)
ax2.set_title('Top 8 Categories', fontsize=12, fontweight='bold')
ax2.grid(axis='x', alpha=0.3)

# 3. ê°ì • ë¶„í¬
ax3 = fig.add_subplot(gs[1, 0])
sentiment_counts.plot(kind='bar', ax=ax3, 
                     color=[colors_sent.get(s, '#333333') for s in sentiment_counts.index],
                     edgecolor='black', linewidth=0.8)
ax3.set_xlabel('')
ax3.set_ylabel('Count', fontsize=10)
ax3.set_title('Sentiment Distribution', fontsize=12, fontweight='bold')
ax3.set_xticklabels(ax3.get_xticklabels(), rotation=45, ha='right', fontsize=9)
ax3.grid(axis='y', alpha=0.3)

# 4. ì‹œê°„ëŒ€ ë¶„í¬
ax4 = fig.add_subplot(gs[1, 1])
time_counts_top = df['time_period'].value_counts().head(5)
ax4.bar(range(len(time_counts_top)), time_counts_top.values,
       color=plt.cm.viridis(np.linspace(0, 1, len(time_counts_top))),
       edgecolor='black', linewidth=0.8)
ax4.set_xticks(range(len(time_counts_top)))
ax4.set_xticklabels(time_counts_top.index, rotation=45, ha='right', fontsize=8)
ax4.set_ylabel('Count', fontsize=10)
ax4.set_title('Top 5 Time Periods', fontsize=12, fontweight='bold')
ax4.grid(axis='y', alpha=0.3)

# 5. ì˜í–¥ë ¥ ë¶„í¬
ax5 = fig.add_subplot(gs[1, 2])
ax5.hist(df['influence_score'], bins=20, color='skyblue', edgecolor='black', alpha=0.7)
ax5.axvline(df['influence_score'].mean(), color='red', linestyle='--', linewidth=2)
ax5.set_xlabel('Influence Score', fontsize=10)
ax5.set_ylabel('Frequency', fontsize=10)
ax5.set_title('Influence Score Distribution', fontsize=12, fontweight='bold')
ax5.grid(alpha=0.3)

# 6. ìƒìœ„ í‚¤ì›Œë“œ
ax6 = fig.add_subplot(gs[2, :2])
top_keywords = keyword_freq.head(10)
ax6.barh(range(len(top_keywords)), top_keywords.values,
        color=plt.cm.plasma(np.linspace(0, 1, len(top_keywords))),
        edgecolor='black', linewidth=0.8)
ax6.set_yticks(range(len(top_keywords)))
ax6.set_yticklabels([kw.replace('kw_', '') for kw in top_keywords.index], fontsize=9)
ax6.set_xlabel('Frequency', fontsize=10)
ax6.set_title('Top 10 Keywords', fontsize=12, fontweight='bold')
ax6.grid(axis='x', alpha=0.3)

# 7. í†µê³„ ì •ë³´
ax7 = fig.add_subplot(gs[2, 2])
ax7.axis('off')
stats_text = f"""
DATASET STATISTICS
{'='*35}

Total Posts: {len(df)}
Unique Authors: {df['author'].nunique()}
Categories: {df['category'].nunique()}

Platform:
  â€¢ X (Twitter): {len(df[df['platform']=='X'])} ({len(df[df['platform']=='X'])/len(df)*100:.1f}%)
  â€¢ Reddit: {len(df[df['platform']=='Reddit'])} ({len(df[df['platform']=='Reddit'])/len(df)*100:.1f}%)

Sentiment:
  â€¢ Negative: {len(df[df['sentiment'].isin(['Negative','Very_Negative'])])} ({len(df[df['sentiment'].isin(['Negative','Very_Negative'])])/len(df)*100:.1f}%)
  â€¢ Positive: {len(df[df['sentiment']=='Positive'])} ({len(df[df['sentiment']=='Positive'])/len(df)*100:.1f}%)

Influence Score:
  â€¢ Mean: {df['influence_score'].mean():.2f}
  â€¢ Max: {df['influence_score'].max():.2f}
  â€¢ Min: {df['influence_score'].min():.2f}

Top Keywords:
  1. {keyword_freq.index[0].replace('kw_', '')}: {keyword_freq.values[0]}
  2. {keyword_freq.index[1].replace('kw_', '')}: {keyword_freq.values[1]}
  3. {keyword_freq.index[2].replace('kw_', '')}: {keyword_freq.values[2]}
"""
ax7.text(0.05, 0.95, stats_text, transform=ax7.transAxes,
        fontsize=9, verticalalignment='top', fontfamily='monospace',
        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))

fig.suptitle('October 2025 Crypto Crash - Community Data Dashboard', 
            fontsize=16, fontweight='bold', y=0.98)

plt.savefig('10_comprehensive_dashboard.png', dpi=300, bbox_inches='tight')
print("âœ… ì €ì¥: 10_comprehensive_dashboard.png")
plt.close()

# ============================================================================
# ìµœì¢… ìš”ì•½
# ============================================================================
print("\n" + "=" * 100)
print("ì‹œê°í™” ì™„ë£Œ!")
print("=" * 100)

print("\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
viz_files = [
    "01_platform_distribution.png",
    "02_category_distribution.png", 
    "03_sentiment_analysis.png",
    "04_time_period_distribution.png",
    "05_influence_score_analysis.png",
    "06_wordclouds.png",
    "07_keyword_heatmap.png",
    "08_keyword_cooccurrence.png",
    "09_network_graph.png",
    "10_comprehensive_dashboard.png"
]

for i, file in enumerate(viz_files, 1):
    print(f"  {i:2d}. {file}")

print("\nâœ… ì´ 10ê°œ ì‹œê°í™” íŒŒì¼ ìƒì„± ì™„ë£Œ")
print("âœ… í•´ìƒë„: 300 DPI (ê³ í™”ì§ˆ)")
print("âœ… ëª¨ë“  ì°¨íŠ¸ PNG í˜•ì‹ìœ¼ë¡œ ì €ì¥")
