{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392a8c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "YouTube ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ê¸°\n",
      "====================================================================================================\n",
      "\n",
      "âœ… YouTube API ì—°ê²° ì„±ê³µ!\n",
      "\n",
      "ìˆ˜ì§‘ ì‹œì‘...\n",
      "  ê¸°ê°„: 2025-09-01 ~ 2025-10-31\n",
      "  ëª©í‘œ: ì˜ìƒ 1000ê°œ, ëŒ“ê¸€ 15000ê°œ\n",
      "\n",
      "[1/4] ì˜ìƒ ê²€ìƒ‰ ì¤‘...\n",
      "  [1/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'October 2025 crypto crash' â†’ 50ê°œ ì˜ìƒ\n",
      "  [2/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'crypto crash October 2025' â†’ 50ê°œ ì˜ìƒ\n",
      "  [3/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ '$19 billion liquidation' â†’ 25ê°œ ì˜ìƒ\n",
      "  [4/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'October 10 2025 crypto' â†’ 50ê°œ ì˜ìƒ\n",
      "  [5/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'October 11 2025 bitcoin' â†’ 50ê°œ ì˜ìƒ\n",
      "  [6/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'Trump tariff crypto crash' â†’ 50ê°œ ì˜ìƒ\n",
      "  [7/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'Binance crash October 2025' â†’ 50ê°œ ì˜ìƒ\n",
      "  [8/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'Hyperliquid liquidation' â†’ 50ê°œ ì˜ìƒ\n",
      "  [9/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'crypto flash crash 2025' â†’ 50ê°œ ì˜ìƒ\n",
      "  [10/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'October 2025 crypto analysis' â†’ 25ê°œ ì˜ìƒ\n",
      "  [11/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'crypto market crash 2025' â†’ 25ê°œ ì˜ìƒ\n",
      "  [12/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'bitcoin crash October' â†’ 50ê°œ ì˜ìƒ\n",
      "  [13/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'ethereum crash October 2025' â†’ 50ê°œ ì˜ìƒ\n",
      "  [14/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'crypto liquidation 2025' â†’ 50ê°œ ì˜ìƒ\n",
      "  [15/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'whale liquidation October' â†’ 50ê°œ ì˜ìƒ\n",
      "  [16/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'crypto bloodbath 2025' â†’ 50ê°œ ì˜ìƒ\n",
      "  [17/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'October crypto dump' â†’ 50ê°œ ì˜ìƒ\n",
      "  [18/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'crypto crash reaction October' â†’ 50ê°œ ì˜ìƒ\n",
      "  [19/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'October 2025 trading disaster' â†’ 50ê°œ ì˜ìƒ\n",
      "  [20/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'crypto portfolio destroyed' â†’ 50ê°œ ì˜ìƒ\n",
      "  [21/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'bitcoin September 2025' â†’ 50ê°œ ì˜ìƒ\n",
      "  [22/24] ê²€ìƒ‰ ì¤‘...\n",
      "    âœ“ 'ethereum October 2025' â†’ 50ê°œ ì˜ìƒ\n",
      "  ğŸ¯ ëª©í‘œ ë‹¬ì„±! (1000ê°œ)\n",
      "\n",
      "âœ… ì´ 719ê°œ ì˜ìƒ ë°œê²¬ (ì¤‘ë³µ ì œê±° í›„)\n",
      "ğŸ’¾ ì €ì¥: ./youtube_data_collection/videos_metadata.csv\n",
      "\n",
      "[2/4] ì˜ìƒ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...\n",
      "âœ… 719ê°œ ì˜ìƒ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘\n",
      "\n",
      "[3/4] ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘... (ì˜ìƒ 719ê°œ)\n",
      "  ì§„í–‰: 50/719 (7.0%) | ëŒ“ê¸€: 2554ê°œ | ETA: 18.2ë¶„\n",
      "  ì§„í–‰: 100/719 (13.9%) | ëŒ“ê¸€: 3219ê°œ | ETA: 12.0ë¶„\n",
      "  ì§„í–‰: 150/719 (20.9%) | ëŒ“ê¸€: 3453ê°œ | ETA: 9.5ë¶„\n",
      "  ì§„í–‰: 200/719 (27.8%) | ëŒ“ê¸€: 4606ê°œ | ETA: 8.1ë¶„\n",
      "  ì§„í–‰: 250/719 (34.8%) | ëŒ“ê¸€: 5503ê°œ | ETA: 6.9ë¶„\n",
      "  ì§„í–‰: 300/719 (41.7%) | ëŒ“ê¸€: 6344ê°œ | ETA: 6.0ë¶„\n",
      "  ì§„í–‰: 350/719 (48.7%) | ëŒ“ê¸€: 8234ê°œ | ETA: 5.2ë¶„\n",
      "  ì§„í–‰: 400/719 (55.6%) | ëŒ“ê¸€: 10155ê°œ | ETA: 4.4ë¶„\n",
      "  ì§„í–‰: 450/719 (62.6%) | ëŒ“ê¸€: 10643ê°œ | ETA: 3.6ë¶„\n",
      "  ì§„í–‰: 500/719 (69.5%) | ëŒ“ê¸€: 11248ê°œ | ETA: 2.9ë¶„\n",
      "  ì§„í–‰: 550/719 (76.5%) | ëŒ“ê¸€: 13987ê°œ | ETA: 2.2ë¶„\n",
      "  ğŸ¯ ëŒ“ê¸€ ëª©í‘œ ë‹¬ì„±! (15000ê°œ)\n",
      "\n",
      "âœ… ì´ 15060ê°œ ëŒ“ê¸€ ìˆ˜ì§‘\n",
      "ğŸ’¾ ì €ì¥: ./youtube_data_collection/comments_temp.csv\n",
      "\n",
      "[4/4] ìë§‰ ìˆ˜ì§‘ ì¤‘... (ì˜ìƒ 719ê°œ)\n",
      "  ì§„í–‰: 50/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 100/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 150/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 200/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 250/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 300/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 350/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 400/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 450/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 500/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 550/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 600/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 650/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "  ì§„í–‰: 700/719 | ìë§‰: 0ê°œ ë¬¸ì¥ | ì„±ê³µ: 0ê°œ ì˜ìƒ\n",
      "\n",
      "âœ… ì´ 0ê°œ ìë§‰ ë¬¸ì¥ ìˆ˜ì§‘ (0ê°œ ì˜ìƒ)\n",
      "\n",
      "====================================================================================================\n",
      "ìˆ˜ì§‘ ì™„ë£Œ!\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“Š ìˆ˜ì§‘ í†µê³„:\n",
      "  ì˜ìƒ: 719ê°œ\n",
      "    - ì œëª©: 719ê°œ\n",
      "    - ì„¤ëª…: 596ê°œ\n",
      "  ëŒ“ê¸€: 15060ê°œ\n",
      "  ìë§‰: 0ê°œ ë¬¸ì¥\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  ì´ í…ìŠ¤íŠ¸ í•­ëª©: 15,779ê°œ\n",
      "  ì†Œìš” ì‹œê°„: 11.2ë¶„\n",
      "  ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: 1,292,575 ê¸€ì\n",
      "\n",
      "ğŸ’¾ ì˜ìƒ ì €ì¥: ./youtube_data_collection/youtube_videos_20260202_174228.csv\n",
      "   í¬ê¸°: 230.2 KB\n",
      "ğŸ’¾ ëŒ“ê¸€ ì €ì¥: ./youtube_data_collection/youtube_comments_20260202_174228.csv\n",
      "   í¬ê¸°: 2390.5 KB\n",
      "ğŸ’¾ JSON ì €ì¥: ./youtube_data_collection/youtube_collection_20260202_174228.json\n",
      "\n",
      "====================================================================================================\n",
      "ë°ì´í„° ë¶„ì„ ë¦¬í¬íŠ¸\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“¹ ì˜ìƒ ë¶„ì„:\n",
      "  í‰ê·  ì¡°íšŒìˆ˜: 248,808\n",
      "  í‰ê·  ì¢‹ì•„ìš”: 6,399\n",
      "  í‰ê·  ëŒ“ê¸€ìˆ˜: 169\n",
      "\n",
      "  ìƒìœ„ ì±„ë„:\n",
      "    - Crypto Nutshell: 13ê°œ\n",
      "    - Altcoin Daily: 11ê°œ\n",
      "    - Coin Bureau: 6ê°œ\n",
      "    - Gerhard - Bitcoin Strategy: 6ê°œ\n",
      "    - Crypto Prices: 6ê°œ\n",
      "\n",
      "ğŸ’¬ ëŒ“ê¸€ ë¶„ì„:\n",
      "  í‰ê·  ê¸¸ì´: 78 ê¸€ì\n",
      "  í‰ê·  ì¢‹ì•„ìš”: 56.2\n",
      "  ë‹µê¸€ ìˆëŠ” ëŒ“ê¸€: 3619ê°œ\n",
      "\n",
      "====================================================================================================\n",
      "ëª©í‘œ ë‹¬ì„± í˜„í™©\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ¯ ëª©í‘œ: 10,000 - 20,000ê°œ í…ìŠ¤íŠ¸ ë°ì´í„°\n",
      "âœ… ë‹¬ì„±: 15,779ê°œ\n",
      "ğŸ‰ ëª©í‘œ ë‹¬ì„±! (105.2%)\n",
      "\n",
      "ğŸ“‚ ì €ì¥ ìœ„ì¹˜: c:\\junwoo\\AI_Project_01_Team6\\data\\Youtube_data\\youtube_data_collection\n",
      "\n",
      "âœ… ìˆ˜ì§‘ ì™„ë£Œ! íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "YouTube ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ê¸°\n",
    "October 2025 Crypto Crash ì „ìš©\n",
    "\n",
    "ëª©í‘œ: 10,000 - 20,000ê°œ í…ìŠ¤íŠ¸ ë°ì´í„°\n",
    "ê¸°ê°„: 2025-09-01 ~ 2025-10-31\n",
    "ìˆ˜ì§‘ ëŒ€ìƒ: ì˜ìƒ ë©”íƒ€ë°ì´í„° + ëŒ“ê¸€ + ìë§‰(ìº¡ì…˜)\n",
    "\n",
    "ì˜ˆìƒ ìˆ˜ì§‘ëŸ‰:\n",
    "- ì˜ìƒ: 500-1,000ê°œ\n",
    "- ëŒ“ê¸€: 10,000-15,000ê°œ\n",
    "- ìë§‰: 5,000-10,000ê°œ ë¬¸ì¥\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ì´: 15,000-26,000ê°œ í…ìŠ¤íŠ¸\n",
    "\n",
    "ì†Œìš” ì‹œê°„: 2-4ì‹œê°„\n",
    "ë¹„ìš©: ë¬´ë£Œ (í• ë‹¹ëŸ‰ 10,000 units/ì¼)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "\n",
    "# ============================================================================\n",
    "# ì„¤ì • (ì—¬ê¸°ë§Œ ìˆ˜ì •í•˜ì„¸ìš”!)\n",
    "# ============================================================================\n",
    "\n",
    "# YouTube API í‚¤\n",
    "# ë°œê¸‰ ë°©ë²•: https://console.cloud.google.com/apis/credentials\n",
    "API_KEY = 'AIzaSyDj4y7cahjxnvq3ffSQHelAGjEWhea65JE'  # â† ì—¬ê¸°ì— ë¶™ì—¬ë„£ê¸°!\n",
    "\n",
    "# ìˆ˜ì§‘ ì„¤ì •\n",
    "COLLECTION_CONFIG = {\n",
    "    'date_range': {\n",
    "        'start': '2025-09-01T00:00:00Z',\n",
    "        'end': '2025-10-31T23:59:59Z'\n",
    "    },\n",
    "    'target_videos': 1000,          # ëª©í‘œ ì˜ìƒ ìˆ˜\n",
    "    'target_comments': 15000,       # ëª©í‘œ ëŒ“ê¸€ ìˆ˜\n",
    "    'max_comments_per_video': 100,  # ì˜ìƒë‹¹ ìµœëŒ€ ëŒ“ê¸€\n",
    "    'target_captions': 10000,       # ëª©í‘œ ìë§‰ ë¬¸ì¥ ìˆ˜\n",
    "}\n",
    "\n",
    "# ê²€ìƒ‰ í‚¤ì›Œë“œ (í¬ë¦½í†  í¬ë˜ì‹œ ê´€ë ¨)\n",
    "SEARCH_KEYWORDS = [\n",
    "    # í•µì‹¬ í‚¤ì›Œë“œ\n",
    "    'October 2025 crypto crash',\n",
    "    'crypto crash October 2025',\n",
    "    '$19 billion liquidation',\n",
    "    'October 10 2025 crypto',\n",
    "    'October 11 2025 bitcoin',\n",
    "    \n",
    "    # ì´ë²¤íŠ¸\n",
    "    'Trump tariff crypto crash',\n",
    "    'Binance crash October 2025',\n",
    "    'Hyperliquid liquidation',\n",
    "    'crypto flash crash 2025',\n",
    "    \n",
    "    # ë¶„ì„\n",
    "    'October 2025 crypto analysis',\n",
    "    'crypto market crash 2025',\n",
    "    'bitcoin crash October',\n",
    "    'ethereum crash October 2025',\n",
    "    \n",
    "    # ì˜í–¥\n",
    "    'crypto liquidation 2025',\n",
    "    'whale liquidation October',\n",
    "    'crypto bloodbath 2025',\n",
    "    'October crypto dump',\n",
    "    \n",
    "    # ë°˜ì‘\n",
    "    'crypto crash reaction October',\n",
    "    'October 2025 trading disaster',\n",
    "    'crypto portfolio destroyed',\n",
    "    \n",
    "    # ì¼ë°˜ í¬ë¦½í†  (ê¸°ê°„ ë‚´)\n",
    "    'bitcoin September 2025',\n",
    "    'ethereum October 2025',\n",
    "    'crypto news October 2025',\n",
    "    'altcoin crash 2025',\n",
    "]\n",
    "\n",
    "# ì €ì¥ ê²½ë¡œ\n",
    "OUTPUT_DIR = './youtube_data_collection'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# YouTube API ì´ˆê¸°í™”\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"YouTube ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ê¸°\")\n",
    "print(\"=\" * 100)\n",
    "print()\n",
    "\n",
    "if API_KEY == 'AIzaSyCKDA_WCioPkAG29BoUOtw7T6KmUH1QQjw':\n",
    "    print(\"âŒ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!\")\n",
    "    print()\n",
    "    print(\"ë°œê¸‰ ë°©ë²•:\")\n",
    "    print(\"1. https://console.cloud.google.com/ ì ‘ì†\")\n",
    "    print(\"2. í”„ë¡œì íŠ¸ ìƒì„±\")\n",
    "    print(\"3. YouTube Data API v3 í™œì„±í™”\")\n",
    "    print(\"4. API í‚¤ ìƒì„±\")\n",
    "    print(\"5. ìœ„ API_KEY ë³€ìˆ˜ì— ë¶™ì—¬ë„£ê¸°\")\n",
    "    print()\n",
    "    exit(1)\n",
    "\n",
    "try:\n",
    "    youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "    print(\"âœ… YouTube API ì—°ê²° ì„±ê³µ!\")\n",
    "    print()\n",
    "except Exception as e:\n",
    "    print(f\"âŒ YouTube API ì—°ê²° ì‹¤íŒ¨: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# ìˆ˜ì§‘ í•¨ìˆ˜ë“¤\n",
    "# ============================================================================\n",
    "\n",
    "def search_videos(keyword, max_results=50):\n",
    "    \"\"\"í‚¤ì›Œë“œë¡œ ì˜ìƒ ê²€ìƒ‰\"\"\"\n",
    "    videos = []\n",
    "    \n",
    "    try:\n",
    "        request = youtube.search().list(\n",
    "            part='snippet',\n",
    "            q=keyword,\n",
    "            type='video',\n",
    "            maxResults=max_results,\n",
    "            order='relevance',\n",
    "            publishedAfter=COLLECTION_CONFIG['date_range']['start'],\n",
    "            publishedBefore=COLLECTION_CONFIG['date_range']['end'],\n",
    "            relevanceLanguage='en',  # ì˜ì–´ ìš°ì„ \n",
    "        )\n",
    "        \n",
    "        response = request.execute()\n",
    "        \n",
    "        for item in response.get('items', []):\n",
    "            video_id = item['id']['videoId']\n",
    "            snippet = item['snippet']\n",
    "            \n",
    "            video_data = {\n",
    "                'video_id': video_id,\n",
    "                'title': snippet.get('title', ''),\n",
    "                'description': snippet.get('description', ''),\n",
    "                'channel_title': snippet.get('channelTitle', ''),\n",
    "                'channel_id': snippet.get('channelId', ''),\n",
    "                'published_at': snippet.get('publishedAt', ''),\n",
    "                'search_keyword': keyword,\n",
    "                'url': f'https://www.youtube.com/watch?v={video_id}',\n",
    "            }\n",
    "            \n",
    "            videos.append(video_data)\n",
    "        \n",
    "        print(f\"    âœ“ '{keyword}' â†’ {len(videos)}ê°œ ì˜ìƒ\")\n",
    "        \n",
    "    except HttpError as e:\n",
    "        print(f\"    âœ— ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    return videos\n",
    "\n",
    "def get_video_details(video_ids):\n",
    "    \"\"\"ì˜ìƒ ìƒì„¸ ì •ë³´ (ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”, ëŒ“ê¸€ ìˆ˜ ë“±)\"\"\"\n",
    "    details = {}\n",
    "    \n",
    "    try:\n",
    "        # 50ê°œì”© ë°°ì¹˜ ì²˜ë¦¬\n",
    "        for i in range(0, len(video_ids), 50):\n",
    "            batch = video_ids[i:i+50]\n",
    "            \n",
    "            request = youtube.videos().list(\n",
    "                part='statistics,contentDetails',\n",
    "                id=','.join(batch)\n",
    "            )\n",
    "            \n",
    "            response = request.execute()\n",
    "            \n",
    "            for item in response.get('items', []):\n",
    "                video_id = item['id']\n",
    "                stats = item.get('statistics', {})\n",
    "                content = item.get('contentDetails', {})\n",
    "                \n",
    "                details[video_id] = {\n",
    "                    'view_count': int(stats.get('viewCount', 0)),\n",
    "                    'like_count': int(stats.get('likeCount', 0)),\n",
    "                    'comment_count': int(stats.get('commentCount', 0)),\n",
    "                    'duration': content.get('duration', ''),\n",
    "                }\n",
    "            \n",
    "            time.sleep(0.5)  # Rate limit\n",
    "        \n",
    "    except HttpError as e:\n",
    "        print(f\"    âœ— ìƒì„¸ ì •ë³´ ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    return details\n",
    "\n",
    "def get_video_comments(video_id, max_comments=100):\n",
    "    \"\"\"ì˜ìƒ ëŒ“ê¸€ ìˆ˜ì§‘\"\"\"\n",
    "    comments = []\n",
    "    \n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part='snippet',\n",
    "            videoId=video_id,\n",
    "            maxResults=min(max_comments, 100),\n",
    "            order='relevance',\n",
    "            textFormat='plainText'\n",
    "        )\n",
    "        \n",
    "        while request and len(comments) < max_comments:\n",
    "            response = request.execute()\n",
    "            \n",
    "            for item in response.get('items', []):\n",
    "                snippet = item['snippet']['topLevelComment']['snippet']\n",
    "                \n",
    "                comment_data = {\n",
    "                    'video_id': video_id,\n",
    "                    'comment_id': item['snippet']['topLevelComment']['id'],\n",
    "                    'text': snippet.get('textDisplay', ''),\n",
    "                    'author': snippet.get('authorDisplayName', ''),\n",
    "                    'like_count': snippet.get('likeCount', 0),\n",
    "                    'published_at': snippet.get('publishedAt', ''),\n",
    "                    'reply_count': item['snippet'].get('totalReplyCount', 0),\n",
    "                }\n",
    "                \n",
    "                comments.append(comment_data)\n",
    "            \n",
    "            # ë‹¤ìŒ í˜ì´ì§€\n",
    "            if 'nextPageToken' in response and len(comments) < max_comments:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part='snippet',\n",
    "                    videoId=video_id,\n",
    "                    pageToken=response['nextPageToken'],\n",
    "                    maxResults=min(max_comments - len(comments), 100),\n",
    "                    order='relevance',\n",
    "                    textFormat='plainText'\n",
    "                )\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "    except HttpError as e:\n",
    "        # ëŒ“ê¸€ ë¹„í™œì„±í™”ëœ ê²½ìš° ë“±\n",
    "        pass\n",
    "    \n",
    "    return comments\n",
    "\n",
    "def get_video_captions(video_id):\n",
    "    \"\"\"ì˜ìƒ ìë§‰(ìº¡ì…˜) ìˆ˜ì§‘\"\"\"\n",
    "    captions = []\n",
    "    \n",
    "    try:\n",
    "        # í•œêµ­ì–´ ë˜ëŠ” ì˜ì–´ ìë§‰ ì‹œë„\n",
    "        for lang in ['ko', 'en']:\n",
    "            try:\n",
    "                transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=[lang])\n",
    "                \n",
    "                for entry in transcript:\n",
    "                    caption_data = {\n",
    "                        'video_id': video_id,\n",
    "                        'text': entry['text'],\n",
    "                        'start': entry['start'],\n",
    "                        'duration': entry['duration'],\n",
    "                        'language': lang,\n",
    "                    }\n",
    "                    captions.append(caption_data)\n",
    "                \n",
    "                # ì„±ê³µí•˜ë©´ ë£¨í”„ íƒˆì¶œ\n",
    "                break\n",
    "                \n",
    "            except (TranscriptsDisabled, NoTranscriptFound):\n",
    "                continue\n",
    "        \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return captions\n",
    "\n",
    "# ============================================================================\n",
    "# ë©”ì¸ ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ìˆ˜ì§‘ ì‹œì‘...\")\n",
    "print(f\"  ê¸°ê°„: {COLLECTION_CONFIG['date_range']['start'][:10]} ~ {COLLECTION_CONFIG['date_range']['end'][:10]}\")\n",
    "print(f\"  ëª©í‘œ: ì˜ìƒ {COLLECTION_CONFIG['target_videos']}ê°œ, ëŒ“ê¸€ {COLLECTION_CONFIG['target_comments']}ê°œ\")\n",
    "print()\n",
    "\n",
    "all_videos = []\n",
    "all_comments = []\n",
    "all_captions = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: ì˜ìƒ ê²€ìƒ‰\n",
    "# ============================================================================\n",
    "print(\"[1/4] ì˜ìƒ ê²€ìƒ‰ ì¤‘...\")\n",
    "\n",
    "for i, keyword in enumerate(SEARCH_KEYWORDS, 1):\n",
    "    print(f\"  [{i}/{len(SEARCH_KEYWORDS)}] ê²€ìƒ‰ ì¤‘...\")\n",
    "    \n",
    "    videos = search_videos(keyword, max_results=50)\n",
    "    all_videos.extend(videos)\n",
    "    \n",
    "    time.sleep(1)  # Rate limit\n",
    "    \n",
    "    # ëª©í‘œ ë‹¬ì„± ì²´í¬\n",
    "    if len(all_videos) >= COLLECTION_CONFIG['target_videos']:\n",
    "        print(f\"  ğŸ¯ ëª©í‘œ ë‹¬ì„±! ({COLLECTION_CONFIG['target_videos']}ê°œ)\")\n",
    "        break\n",
    "\n",
    "# ì¤‘ë³µ ì œê±°\n",
    "videos_df = pd.DataFrame(all_videos)\n",
    "if len(videos_df) > 0:\n",
    "    videos_df = videos_df.drop_duplicates(subset=['video_id'])\n",
    "    all_videos = videos_df.to_dict('records')\n",
    "\n",
    "print(f\"\\nâœ… ì´ {len(all_videos)}ê°œ ì˜ìƒ ë°œê²¬ (ì¤‘ë³µ ì œê±° í›„)\")\n",
    "\n",
    "# ì¤‘ê°„ ì €ì¥\n",
    "videos_file = f\"{OUTPUT_DIR}/videos_metadata.csv\"\n",
    "videos_df.to_csv(videos_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"ğŸ’¾ ì €ì¥: {videos_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: ì˜ìƒ ìƒì„¸ ì •ë³´\n",
    "# ============================================================================\n",
    "print(f\"\\n[2/4] ì˜ìƒ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...\")\n",
    "\n",
    "video_ids = [v['video_id'] for v in all_videos]\n",
    "details = get_video_details(video_ids)\n",
    "\n",
    "# ìƒì„¸ ì •ë³´ ë³‘í•©\n",
    "for video in all_videos:\n",
    "    video_id = video['video_id']\n",
    "    if video_id in details:\n",
    "        video.update(details[video_id])\n",
    "\n",
    "print(f\"âœ… {len(details)}ê°œ ì˜ìƒ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: ëŒ“ê¸€ ìˆ˜ì§‘\n",
    "# ============================================================================\n",
    "print(f\"\\n[3/4] ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘... (ì˜ìƒ {len(all_videos)}ê°œ)\")\n",
    "\n",
    "for i, video in enumerate(all_videos, 1):\n",
    "    video_id = video['video_id']\n",
    "    \n",
    "    # ì§„í–‰ ìƒí™©\n",
    "    if i % 50 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = i / elapsed * 60  # ì˜ìƒ/ë¶„\n",
    "        eta = (len(all_videos) - i) / rate if rate > 0 else 0\n",
    "        print(f\"  ì§„í–‰: {i}/{len(all_videos)} ({i/len(all_videos)*100:.1f}%) | \"\n",
    "              f\"ëŒ“ê¸€: {len(all_comments)}ê°œ | ETA: {eta:.1f}ë¶„\")\n",
    "    \n",
    "    # ëŒ“ê¸€ ìˆ˜ì§‘\n",
    "    comments = get_video_comments(video_id, \n",
    "                                  max_comments=COLLECTION_CONFIG['max_comments_per_video'])\n",
    "    all_comments.extend(comments)\n",
    "    \n",
    "    time.sleep(0.5)  # Rate limit\n",
    "    \n",
    "    # ëª©í‘œ ë‹¬ì„± ì²´í¬\n",
    "    if len(all_comments) >= COLLECTION_CONFIG['target_comments']:\n",
    "        print(f\"  ğŸ¯ ëŒ“ê¸€ ëª©í‘œ ë‹¬ì„±! ({COLLECTION_CONFIG['target_comments']}ê°œ)\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nâœ… ì´ {len(all_comments)}ê°œ ëŒ“ê¸€ ìˆ˜ì§‘\")\n",
    "\n",
    "# ì¤‘ê°„ ì €ì¥\n",
    "if len(all_comments) > 0:\n",
    "    comments_df = pd.DataFrame(all_comments)\n",
    "    comments_file = f\"{OUTPUT_DIR}/comments_temp.csv\"\n",
    "    comments_df.to_csv(comments_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"ğŸ’¾ ì €ì¥: {comments_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: ìë§‰(ìº¡ì…˜) ìˆ˜ì§‘\n",
    "# ============================================================================\n",
    "print(f\"\\n[4/4] ìë§‰ ìˆ˜ì§‘ ì¤‘... (ì˜ìƒ {len(all_videos)}ê°œ)\")\n",
    "\n",
    "caption_videos = 0\n",
    "for i, video in enumerate(all_videos, 1):\n",
    "    video_id = video['video_id']\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(f\"  ì§„í–‰: {i}/{len(all_videos)} | ìë§‰: {len(all_captions)}ê°œ ë¬¸ì¥ | \"\n",
    "              f\"ì„±ê³µ: {caption_videos}ê°œ ì˜ìƒ\")\n",
    "    \n",
    "    # ìë§‰ ìˆ˜ì§‘\n",
    "    captions = get_video_captions(video_id)\n",
    "    \n",
    "    if len(captions) > 0:\n",
    "        all_captions.extend(captions)\n",
    "        caption_videos += 1\n",
    "    \n",
    "    time.sleep(0.3)  # Rate limit\n",
    "    \n",
    "    # ëª©í‘œ ë‹¬ì„± ì²´í¬\n",
    "    if len(all_captions) >= COLLECTION_CONFIG['target_captions']:\n",
    "        print(f\"  ğŸ¯ ìë§‰ ëª©í‘œ ë‹¬ì„±! ({COLLECTION_CONFIG['target_captions']}ê°œ)\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nâœ… ì´ {len(all_captions)}ê°œ ìë§‰ ë¬¸ì¥ ìˆ˜ì§‘ ({caption_videos}ê°œ ì˜ìƒ)\")\n",
    "\n",
    "# ============================================================================\n",
    "# ìµœì¢… ì €ì¥\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ìˆ˜ì§‘ ì™„ë£Œ!\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_text_items = len(all_videos) + len(all_comments) + len(all_captions)\n",
    "\n",
    "print(f\"\\nğŸ“Š ìˆ˜ì§‘ í†µê³„:\")\n",
    "print(f\"  ì˜ìƒ: {len(all_videos)}ê°œ\")\n",
    "print(f\"    - ì œëª©: {len(all_videos)}ê°œ\")\n",
    "print(f\"    - ì„¤ëª…: {len([v for v in all_videos if v.get('description')])}ê°œ\")\n",
    "print(f\"  ëŒ“ê¸€: {len(all_comments)}ê°œ\")\n",
    "print(f\"  ìë§‰: {len(all_captions)}ê°œ ë¬¸ì¥\")\n",
    "print(f\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(f\"  ì´ í…ìŠ¤íŠ¸ í•­ëª©: {total_text_items:,}ê°œ\")\n",
    "print(f\"  ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„\")\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ê¸¸ì´ ê³„ì‚°\n",
    "total_chars = 0\n",
    "if len(all_videos) > 0:\n",
    "    total_chars += sum(len(str(v.get('title', ''))) + len(str(v.get('description', ''))) for v in all_videos)\n",
    "if len(all_comments) > 0:\n",
    "    total_chars += sum(len(str(c.get('text', ''))) for c in all_comments)\n",
    "if len(all_captions) > 0:\n",
    "    total_chars += sum(len(str(c.get('text', ''))) for c in all_captions)\n",
    "\n",
    "print(f\"  ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: {total_chars:,} ê¸€ì\")\n",
    "\n",
    "# ì €ì¥\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# 1. ì˜ìƒ ë©”íƒ€ë°ì´í„°\n",
    "if len(all_videos) > 0:\n",
    "    videos_df = pd.DataFrame(all_videos)\n",
    "    videos_final = f\"{OUTPUT_DIR}/youtube_videos_{timestamp}.csv\"\n",
    "    videos_df.to_csv(videos_final, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nğŸ’¾ ì˜ìƒ ì €ì¥: {videos_final}\")\n",
    "    print(f\"   í¬ê¸°: {os.path.getsize(videos_final) / 1024:.1f} KB\")\n",
    "\n",
    "# 2. ëŒ“ê¸€\n",
    "if len(all_comments) > 0:\n",
    "    comments_df = pd.DataFrame(all_comments)\n",
    "    comments_final = f\"{OUTPUT_DIR}/youtube_comments_{timestamp}.csv\"\n",
    "    comments_df.to_csv(comments_final, index=False, encoding='utf-8-sig')\n",
    "    print(f\"ğŸ’¾ ëŒ“ê¸€ ì €ì¥: {comments_final}\")\n",
    "    print(f\"   í¬ê¸°: {os.path.getsize(comments_final) / 1024:.1f} KB\")\n",
    "\n",
    "# 3. ìë§‰\n",
    "if len(all_captions) > 0:\n",
    "    captions_df = pd.DataFrame(all_captions)\n",
    "    captions_final = f\"{OUTPUT_DIR}/youtube_captions_{timestamp}.csv\"\n",
    "    captions_df.to_csv(captions_final, index=False, encoding='utf-8-sig')\n",
    "    print(f\"ğŸ’¾ ìë§‰ ì €ì¥: {captions_final}\")\n",
    "    print(f\"   í¬ê¸°: {os.path.getsize(captions_final) / 1024:.1f} KB\")\n",
    "\n",
    "# 4. í†µí•© JSON\n",
    "combined_data = {\n",
    "    'metadata': {\n",
    "        'collection_date': timestamp,\n",
    "        'total_videos': len(all_videos),\n",
    "        'total_comments': len(all_comments),\n",
    "        'total_captions': len(all_captions),\n",
    "        'total_text_items': total_text_items,\n",
    "        'duration_minutes': total_time / 60,\n",
    "        'date_range': COLLECTION_CONFIG['date_range'],\n",
    "    },\n",
    "    'videos': all_videos,\n",
    "    'comments': all_comments,\n",
    "    'captions': all_captions,\n",
    "}\n",
    "\n",
    "json_file = f\"{OUTPUT_DIR}/youtube_collection_{timestamp}.json\"\n",
    "with open(json_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(combined_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"ğŸ’¾ JSON ì €ì¥: {json_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ë¶„ì„ ë¦¬í¬íŠ¸\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ë°ì´í„° ë¶„ì„ ë¦¬í¬íŠ¸\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "if len(all_videos) > 0:\n",
    "    print(\"\\nğŸ“¹ ì˜ìƒ ë¶„ì„:\")\n",
    "    print(f\"  í‰ê·  ì¡°íšŒìˆ˜: {videos_df['view_count'].mean():,.0f}\")\n",
    "    print(f\"  í‰ê·  ì¢‹ì•„ìš”: {videos_df['like_count'].mean():,.0f}\")\n",
    "    print(f\"  í‰ê·  ëŒ“ê¸€ìˆ˜: {videos_df['comment_count'].mean():,.0f}\")\n",
    "    print(f\"\\n  ìƒìœ„ ì±„ë„:\")\n",
    "    for channel, count in videos_df['channel_title'].value_counts().head(5).items():\n",
    "        print(f\"    - {channel}: {count}ê°œ\")\n",
    "\n",
    "if len(all_comments) > 0:\n",
    "    print(f\"\\nğŸ’¬ ëŒ“ê¸€ ë¶„ì„:\")\n",
    "    print(f\"  í‰ê·  ê¸¸ì´: {comments_df['text'].str.len().mean():.0f} ê¸€ì\")\n",
    "    print(f\"  í‰ê·  ì¢‹ì•„ìš”: {comments_df['like_count'].mean():.1f}\")\n",
    "    print(f\"  ë‹µê¸€ ìˆëŠ” ëŒ“ê¸€: {len(comments_df[comments_df['reply_count'] > 0])}ê°œ\")\n",
    "\n",
    "if len(all_captions) > 0:\n",
    "    print(f\"\\nğŸ“ ìë§‰ ë¶„ì„:\")\n",
    "    print(f\"  ìë§‰ ìˆëŠ” ì˜ìƒ: {caption_videos}ê°œ ({caption_videos/len(all_videos)*100:.1f}%)\")\n",
    "    print(f\"  í‰ê·  ë¬¸ì¥ ê¸¸ì´: {captions_df['text'].str.len().mean():.0f} ê¸€ì\")\n",
    "    print(f\"  ì–¸ì–´ ë¶„í¬:\")\n",
    "    for lang, count in captions_df['language'].value_counts().items():\n",
    "        lang_name = 'Korean' if lang == 'ko' else 'English'\n",
    "        print(f\"    - {lang_name}: {count}ê°œ\")\n",
    "\n",
    "# ëª©í‘œ ë‹¬ì„± í™•ì¸\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ëª©í‘œ ë‹¬ì„± í˜„í™©\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "target_total = 15000  # ëª©í‘œ í…ìŠ¤íŠ¸ ìˆ˜\n",
    "achieved = total_text_items\n",
    "\n",
    "print(f\"\\nğŸ¯ ëª©í‘œ: 10,000 - 20,000ê°œ í…ìŠ¤íŠ¸ ë°ì´í„°\")\n",
    "print(f\"âœ… ë‹¬ì„±: {achieved:,}ê°œ\")\n",
    "\n",
    "if achieved >= 10000:\n",
    "    print(f\"ğŸ‰ ëª©í‘œ ë‹¬ì„±! ({achieved/target_total*100:.1f}%)\")\n",
    "else:\n",
    "    remaining = 10000 - achieved\n",
    "    print(f\"âš ï¸  {remaining:,}ê°œ ë” í•„ìš”\")\n",
    "\n",
    "print(f\"\\nğŸ“‚ ì €ì¥ ìœ„ì¹˜: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "print(\"\\nâœ… ìˆ˜ì§‘ ì™„ë£Œ! íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-project-01-team6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
