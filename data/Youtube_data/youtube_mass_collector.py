#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
YouTube ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ê¸°
October 2025 Crypto Crash ì „ìš©

ëª©í‘œ: 10,000 - 20,000ê°œ í…ìŠ¤íŠ¸ ë°ì´í„°
ê¸°ê°„: 2025-09-01 ~ 2025-10-31
ìˆ˜ì§‘ ëŒ€ìƒ: ì˜ìƒ ë©”íƒ€ë°ì´í„° + ëŒ“ê¸€ + ìë§‰(ìº¡ì…˜)

ì˜ˆìƒ ìˆ˜ì§‘ëŸ‰:
- ì˜ìƒ: 500-1,000ê°œ
- ëŒ“ê¸€: 10,000-15,000ê°œ
- ìë§‰: 5,000-10,000ê°œ ë¬¸ì¥
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ì´: 15,000-26,000ê°œ í…ìŠ¤íŠ¸

ì†Œìš” ì‹œê°„: 2-4ì‹œê°„
ë¹„ìš©: ë¬´ë£Œ (í• ë‹¹ëŸ‰ 10,000 units/ì¼)
"""

import os
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound
import pandas as pd
from datetime import datetime, timedelta
import time
import json
import re

# ============================================================================
# ì„¤ì • (ì—¬ê¸°ë§Œ ìˆ˜ì •í•˜ì„¸ìš”!)
# ============================================================================

# YouTube API í‚¤
# ë°œê¸‰ ë°©ë²•: https://console.cloud.google.com/apis/credentials
API_KEY = 'YOUR_YOUTUBE_API_KEY_HERE'  # â† ì—¬ê¸°ì— ë¶™ì—¬ë„£ê¸°!

# ìˆ˜ì§‘ ì„¤ì •
COLLECTION_CONFIG = {
    'date_range': {
        'start': '2025-09-01T00:00:00Z',
        'end': '2025-10-31T23:59:59Z'
    },
    'target_videos': 1000,          # ëª©í‘œ ì˜ìƒ ìˆ˜
    'target_comments': 15000,       # ëª©í‘œ ëŒ“ê¸€ ìˆ˜
    'max_comments_per_video': 100,  # ì˜ìƒë‹¹ ìµœëŒ€ ëŒ“ê¸€
    'target_captions': 10000,       # ëª©í‘œ ìë§‰ ë¬¸ì¥ ìˆ˜
}

# ê²€ìƒ‰ í‚¤ì›Œë“œ (í¬ë¦½í†  í¬ë˜ì‹œ ê´€ë ¨)
SEARCH_KEYWORDS = [
    # í•µì‹¬ í‚¤ì›Œë“œ
    'October 2025 crypto crash',
    'crypto crash October 2025',
    '$19 billion liquidation',
    'October 10 2025 crypto',
    'October 11 2025 bitcoin',
    
    # ì´ë²¤íŠ¸
    'Trump tariff crypto crash',
    'Binance crash October 2025',
    'Hyperliquid liquidation',
    'crypto flash crash 2025',
    
    # ë¶„ì„
    'October 2025 crypto analysis',
    'crypto market crash 2025',
    'bitcoin crash October',
    'ethereum crash October 2025',
    
    # ì˜í–¥
    'crypto liquidation 2025',
    'whale liquidation October',
    'crypto bloodbath 2025',
    'October crypto dump',
    
    # ë°˜ì‘
    'crypto crash reaction October',
    'October 2025 trading disaster',
    'crypto portfolio destroyed',
    
    # ì¼ë°˜ í¬ë¦½í†  (ê¸°ê°„ ë‚´)
    'bitcoin September 2025',
    'ethereum October 2025',
    'crypto news October 2025',
    'altcoin crash 2025',
]

# ì €ì¥ ê²½ë¡œ
OUTPUT_DIR = './youtube_data_collection'
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ============================================================================
# YouTube API ì´ˆê¸°í™”
# ============================================================================

print("=" * 100)
print("YouTube ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ê¸°")
print("=" * 100)
print()

if API_KEY == 'YOUR_YOUTUBE_API_KEY_HERE':
    print("âŒ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
    print()
    print("ë°œê¸‰ ë°©ë²•:")
    print("1. https://console.cloud.google.com/ ì ‘ì†")
    print("2. í”„ë¡œì íŠ¸ ìƒì„±")
    print("3. YouTube Data API v3 í™œì„±í™”")
    print("4. API í‚¤ ìƒì„±")
    print("5. ìœ„ API_KEY ë³€ìˆ˜ì— ë¶™ì—¬ë„£ê¸°")
    print()
    exit(1)

try:
    youtube = build('youtube', 'v3', developerKey=API_KEY)
    print("âœ… YouTube API ì—°ê²° ì„±ê³µ!")
    print()
except Exception as e:
    print(f"âŒ YouTube API ì—°ê²° ì‹¤íŒ¨: {e}")
    exit(1)

# ============================================================================
# ìˆ˜ì§‘ í•¨ìˆ˜ë“¤
# ============================================================================

def search_videos(keyword, max_results=50):
    """í‚¤ì›Œë“œë¡œ ì˜ìƒ ê²€ìƒ‰"""
    videos = []
    
    try:
        request = youtube.search().list(
            part='snippet',
            q=keyword,
            type='video',
            maxResults=max_results,
            order='relevance',
            publishedAfter=COLLECTION_CONFIG['date_range']['start'],
            publishedBefore=COLLECTION_CONFIG['date_range']['end'],
            relevanceLanguage='en',  # ì˜ì–´ ìš°ì„ 
        )
        
        response = request.execute()
        
        for item in response.get('items', []):
            video_id = item['id']['videoId']
            snippet = item['snippet']
            
            video_data = {
                'video_id': video_id,
                'title': snippet.get('title', ''),
                'description': snippet.get('description', ''),
                'channel_title': snippet.get('channelTitle', ''),
                'channel_id': snippet.get('channelId', ''),
                'published_at': snippet.get('publishedAt', ''),
                'search_keyword': keyword,
                'url': f'https://www.youtube.com/watch?v={video_id}',
            }
            
            videos.append(video_data)
        
        print(f"    âœ“ '{keyword}' â†’ {len(videos)}ê°œ ì˜ìƒ")
        
    except HttpError as e:
        print(f"    âœ— ì˜¤ë¥˜: {e}")
    
    return videos

def get_video_details(video_ids):
    """ì˜ìƒ ìƒì„¸ ì •ë³´ (ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”, ëŒ“ê¸€ ìˆ˜ ë“±)"""
    details = {}
    
    try:
        # 50ê°œì”© ë°°ì¹˜ ì²˜ë¦¬
        for i in range(0, len(video_ids), 50):
            batch = video_ids[i:i+50]
            
            request = youtube.videos().list(
                part='statistics,contentDetails',
                id=','.join(batch)
            )
            
            response = request.execute()
            
            for item in response.get('items', []):
                video_id = item['id']
                stats = item.get('statistics', {})
                content = item.get('contentDetails', {})
                
                details[video_id] = {
                    'view_count': int(stats.get('viewCount', 0)),
                    'like_count': int(stats.get('likeCount', 0)),
                    'comment_count': int(stats.get('commentCount', 0)),
                    'duration': content.get('duration', ''),
                }
            
            time.sleep(0.5)  # Rate limit
        
    except HttpError as e:
        print(f"    âœ— ìƒì„¸ ì •ë³´ ì˜¤ë¥˜: {e}")
    
    return details

def get_video_comments(video_id, max_comments=100):
    """ì˜ìƒ ëŒ“ê¸€ ìˆ˜ì§‘"""
    comments = []
    
    try:
        request = youtube.commentThreads().list(
            part='snippet',
            videoId=video_id,
            maxResults=min(max_comments, 100),
            order='relevance',
            textFormat='plainText'
        )
        
        while request and len(comments) < max_comments:
            response = request.execute()
            
            for item in response.get('items', []):
                snippet = item['snippet']['topLevelComment']['snippet']
                
                comment_data = {
                    'video_id': video_id,
                    'comment_id': item['snippet']['topLevelComment']['id'],
                    'text': snippet.get('textDisplay', ''),
                    'author': snippet.get('authorDisplayName', ''),
                    'like_count': snippet.get('likeCount', 0),
                    'published_at': snippet.get('publishedAt', ''),
                    'reply_count': item['snippet'].get('totalReplyCount', 0),
                }
                
                comments.append(comment_data)
            
            # ë‹¤ìŒ í˜ì´ì§€
            if 'nextPageToken' in response and len(comments) < max_comments:
                request = youtube.commentThreads().list(
                    part='snippet',
                    videoId=video_id,
                    pageToken=response['nextPageToken'],
                    maxResults=min(max_comments - len(comments), 100),
                    order='relevance',
                    textFormat='plainText'
                )
            else:
                break
        
    except HttpError as e:
        # ëŒ“ê¸€ ë¹„í™œì„±í™”ëœ ê²½ìš° ë“±
        pass
    
    return comments

def get_video_captions(video_id):
    """ì˜ìƒ ìë§‰(ìº¡ì…˜) ìˆ˜ì§‘"""
    captions = []
    
    try:
        # í•œêµ­ì–´ ë˜ëŠ” ì˜ì–´ ìë§‰ ì‹œë„
        for lang in ['ko', 'en']:
            try:
                transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=[lang])
                
                for entry in transcript:
                    caption_data = {
                        'video_id': video_id,
                        'text': entry['text'],
                        'start': entry['start'],
                        'duration': entry['duration'],
                        'language': lang,
                    }
                    captions.append(caption_data)
                
                # ì„±ê³µí•˜ë©´ ë£¨í”„ íƒˆì¶œ
                break
                
            except (TranscriptsDisabled, NoTranscriptFound):
                continue
        
    except Exception as e:
        pass
    
    return captions

# ============================================================================
# ë©”ì¸ ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤
# ============================================================================

print("ìˆ˜ì§‘ ì‹œì‘...")
print(f"  ê¸°ê°„: {COLLECTION_CONFIG['date_range']['start'][:10]} ~ {COLLECTION_CONFIG['date_range']['end'][:10]}")
print(f"  ëª©í‘œ: ì˜ìƒ {COLLECTION_CONFIG['target_videos']}ê°œ, ëŒ“ê¸€ {COLLECTION_CONFIG['target_comments']}ê°œ")
print()

all_videos = []
all_comments = []
all_captions = []

start_time = time.time()

# ============================================================================
# Step 1: ì˜ìƒ ê²€ìƒ‰
# ============================================================================
print("[1/4] ì˜ìƒ ê²€ìƒ‰ ì¤‘...")

for i, keyword in enumerate(SEARCH_KEYWORDS, 1):
    print(f"  [{i}/{len(SEARCH_KEYWORDS)}] ê²€ìƒ‰ ì¤‘...")
    
    videos = search_videos(keyword, max_results=50)
    all_videos.extend(videos)
    
    time.sleep(1)  # Rate limit
    
    # ëª©í‘œ ë‹¬ì„± ì²´í¬
    if len(all_videos) >= COLLECTION_CONFIG['target_videos']:
        print(f"  ğŸ¯ ëª©í‘œ ë‹¬ì„±! ({COLLECTION_CONFIG['target_videos']}ê°œ)")
        break

# ì¤‘ë³µ ì œê±°
videos_df = pd.DataFrame(all_videos)
if len(videos_df) > 0:
    videos_df = videos_df.drop_duplicates(subset=['video_id'])
    all_videos = videos_df.to_dict('records')

print(f"\nâœ… ì´ {len(all_videos)}ê°œ ì˜ìƒ ë°œê²¬ (ì¤‘ë³µ ì œê±° í›„)")

# ì¤‘ê°„ ì €ì¥
videos_file = f"{OUTPUT_DIR}/videos_metadata.csv"
videos_df.to_csv(videos_file, index=False, encoding='utf-8-sig')
print(f"ğŸ’¾ ì €ì¥: {videos_file}")

# ============================================================================
# Step 2: ì˜ìƒ ìƒì„¸ ì •ë³´
# ============================================================================
print(f"\n[2/4] ì˜ìƒ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")

video_ids = [v['video_id'] for v in all_videos]
details = get_video_details(video_ids)

# ìƒì„¸ ì •ë³´ ë³‘í•©
for video in all_videos:
    video_id = video['video_id']
    if video_id in details:
        video.update(details[video_id])

print(f"âœ… {len(details)}ê°œ ì˜ìƒ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘")

# ============================================================================
# Step 3: ëŒ“ê¸€ ìˆ˜ì§‘
# ============================================================================
print(f"\n[3/4] ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘... (ì˜ìƒ {len(all_videos)}ê°œ)")

for i, video in enumerate(all_videos, 1):
    video_id = video['video_id']
    
    # ì§„í–‰ ìƒí™©
    if i % 50 == 0:
        elapsed = time.time() - start_time
        rate = i / elapsed * 60  # ì˜ìƒ/ë¶„
        eta = (len(all_videos) - i) / rate if rate > 0 else 0
        print(f"  ì§„í–‰: {i}/{len(all_videos)} ({i/len(all_videos)*100:.1f}%) | "
              f"ëŒ“ê¸€: {len(all_comments)}ê°œ | ETA: {eta:.1f}ë¶„")
    
    # ëŒ“ê¸€ ìˆ˜ì§‘
    comments = get_video_comments(video_id, 
                                  max_comments=COLLECTION_CONFIG['max_comments_per_video'])
    all_comments.extend(comments)
    
    time.sleep(0.5)  # Rate limit
    
    # ëª©í‘œ ë‹¬ì„± ì²´í¬
    if len(all_comments) >= COLLECTION_CONFIG['target_comments']:
        print(f"  ğŸ¯ ëŒ“ê¸€ ëª©í‘œ ë‹¬ì„±! ({COLLECTION_CONFIG['target_comments']}ê°œ)")
        break

print(f"\nâœ… ì´ {len(all_comments)}ê°œ ëŒ“ê¸€ ìˆ˜ì§‘")

# ì¤‘ê°„ ì €ì¥
if len(all_comments) > 0:
    comments_df = pd.DataFrame(all_comments)
    comments_file = f"{OUTPUT_DIR}/comments_temp.csv"
    comments_df.to_csv(comments_file, index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ ì €ì¥: {comments_file}")

# ============================================================================
# Step 4: ìë§‰(ìº¡ì…˜) ìˆ˜ì§‘
# ============================================================================
print(f"\n[4/4] ìë§‰ ìˆ˜ì§‘ ì¤‘... (ì˜ìƒ {len(all_videos)}ê°œ)")

caption_videos = 0
for i, video in enumerate(all_videos, 1):
    video_id = video['video_id']
    
    if i % 50 == 0:
        print(f"  ì§„í–‰: {i}/{len(all_videos)} | ìë§‰: {len(all_captions)}ê°œ ë¬¸ì¥ | "
              f"ì„±ê³µ: {caption_videos}ê°œ ì˜ìƒ")
    
    # ìë§‰ ìˆ˜ì§‘
    captions = get_video_captions(video_id)
    
    if len(captions) > 0:
        all_captions.extend(captions)
        caption_videos += 1
    
    time.sleep(0.3)  # Rate limit
    
    # ëª©í‘œ ë‹¬ì„± ì²´í¬
    if len(all_captions) >= COLLECTION_CONFIG['target_captions']:
        print(f"  ğŸ¯ ìë§‰ ëª©í‘œ ë‹¬ì„±! ({COLLECTION_CONFIG['target_captions']}ê°œ)")
        break

print(f"\nâœ… ì´ {len(all_captions)}ê°œ ìë§‰ ë¬¸ì¥ ìˆ˜ì§‘ ({caption_videos}ê°œ ì˜ìƒ)")

# ============================================================================
# ìµœì¢… ì €ì¥
# ============================================================================

print("\n" + "=" * 100)
print("ìˆ˜ì§‘ ì™„ë£Œ!")
print("=" * 100)

total_time = time.time() - start_time
total_text_items = len(all_videos) + len(all_comments) + len(all_captions)

print(f"\nğŸ“Š ìˆ˜ì§‘ í†µê³„:")
print(f"  ì˜ìƒ: {len(all_videos)}ê°œ")
print(f"    - ì œëª©: {len(all_videos)}ê°œ")
print(f"    - ì„¤ëª…: {len([v for v in all_videos if v.get('description')])}ê°œ")
print(f"  ëŒ“ê¸€: {len(all_comments)}ê°œ")
print(f"  ìë§‰: {len(all_captions)}ê°œ ë¬¸ì¥")
print(f"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
print(f"  ì´ í…ìŠ¤íŠ¸ í•­ëª©: {total_text_items:,}ê°œ")
print(f"  ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")

# í…ìŠ¤íŠ¸ ê¸¸ì´ ê³„ì‚°
total_chars = 0
if len(all_videos) > 0:
    total_chars += sum(len(str(v.get('title', ''))) + len(str(v.get('description', ''))) for v in all_videos)
if len(all_comments) > 0:
    total_chars += sum(len(str(c.get('text', ''))) for c in all_comments)
if len(all_captions) > 0:
    total_chars += sum(len(str(c.get('text', ''))) for c in all_captions)

print(f"  ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: {total_chars:,} ê¸€ì")

# ì €ì¥
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

# 1. ì˜ìƒ ë©”íƒ€ë°ì´í„°
if len(all_videos) > 0:
    videos_df = pd.DataFrame(all_videos)
    videos_final = f"{OUTPUT_DIR}/youtube_videos_{timestamp}.csv"
    videos_df.to_csv(videos_final, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ ì˜ìƒ ì €ì¥: {videos_final}")
    print(f"   í¬ê¸°: {os.path.getsize(videos_final) / 1024:.1f} KB")

# 2. ëŒ“ê¸€
if len(all_comments) > 0:
    comments_df = pd.DataFrame(all_comments)
    comments_final = f"{OUTPUT_DIR}/youtube_comments_{timestamp}.csv"
    comments_df.to_csv(comments_final, index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ ëŒ“ê¸€ ì €ì¥: {comments_final}")
    print(f"   í¬ê¸°: {os.path.getsize(comments_final) / 1024:.1f} KB")

# 3. ìë§‰
if len(all_captions) > 0:
    captions_df = pd.DataFrame(all_captions)
    captions_final = f"{OUTPUT_DIR}/youtube_captions_{timestamp}.csv"
    captions_df.to_csv(captions_final, index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ ìë§‰ ì €ì¥: {captions_final}")
    print(f"   í¬ê¸°: {os.path.getsize(captions_final) / 1024:.1f} KB")

# 4. í†µí•© JSON
combined_data = {
    'metadata': {
        'collection_date': timestamp,
        'total_videos': len(all_videos),
        'total_comments': len(all_comments),
        'total_captions': len(all_captions),
        'total_text_items': total_text_items,
        'duration_minutes': total_time / 60,
        'date_range': COLLECTION_CONFIG['date_range'],
    },
    'videos': all_videos,
    'comments': all_comments,
    'captions': all_captions,
}

json_file = f"{OUTPUT_DIR}/youtube_collection_{timestamp}.json"
with open(json_file, 'w', encoding='utf-8') as f:
    json.dump(combined_data, f, indent=2, ensure_ascii=False)

print(f"ğŸ’¾ JSON ì €ì¥: {json_file}")

# ============================================================================
# ë¶„ì„ ë¦¬í¬íŠ¸
# ============================================================================

print("\n" + "=" * 100)
print("ë°ì´í„° ë¶„ì„ ë¦¬í¬íŠ¸")
print("=" * 100)

if len(all_videos) > 0:
    print("\nğŸ“¹ ì˜ìƒ ë¶„ì„:")
    print(f"  í‰ê·  ì¡°íšŒìˆ˜: {videos_df['view_count'].mean():,.0f}")
    print(f"  í‰ê·  ì¢‹ì•„ìš”: {videos_df['like_count'].mean():,.0f}")
    print(f"  í‰ê·  ëŒ“ê¸€ìˆ˜: {videos_df['comment_count'].mean():,.0f}")
    print(f"\n  ìƒìœ„ ì±„ë„:")
    for channel, count in videos_df['channel_title'].value_counts().head(5).items():
        print(f"    - {channel}: {count}ê°œ")

if len(all_comments) > 0:
    print(f"\nğŸ’¬ ëŒ“ê¸€ ë¶„ì„:")
    print(f"  í‰ê·  ê¸¸ì´: {comments_df['text'].str.len().mean():.0f} ê¸€ì")
    print(f"  í‰ê·  ì¢‹ì•„ìš”: {comments_df['like_count'].mean():.1f}")
    print(f"  ë‹µê¸€ ìˆëŠ” ëŒ“ê¸€: {len(comments_df[comments_df['reply_count'] > 0])}ê°œ")

if len(all_captions) > 0:
    print(f"\nğŸ“ ìë§‰ ë¶„ì„:")
    print(f"  ìë§‰ ìˆëŠ” ì˜ìƒ: {caption_videos}ê°œ ({caption_videos/len(all_videos)*100:.1f}%)")
    print(f"  í‰ê·  ë¬¸ì¥ ê¸¸ì´: {captions_df['text'].str.len().mean():.0f} ê¸€ì")
    print(f"  ì–¸ì–´ ë¶„í¬:")
    for lang, count in captions_df['language'].value_counts().items():
        lang_name = 'Korean' if lang == 'ko' else 'English'
        print(f"    - {lang_name}: {count}ê°œ")

# ëª©í‘œ ë‹¬ì„± í™•ì¸
print("\n" + "=" * 100)
print("ëª©í‘œ ë‹¬ì„± í˜„í™©")
print("=" * 100)

target_total = 15000  # ëª©í‘œ í…ìŠ¤íŠ¸ ìˆ˜
achieved = total_text_items

print(f"\nğŸ¯ ëª©í‘œ: 10,000 - 20,000ê°œ í…ìŠ¤íŠ¸ ë°ì´í„°")
print(f"âœ… ë‹¬ì„±: {achieved:,}ê°œ")

if achieved >= 10000:
    print(f"ğŸ‰ ëª©í‘œ ë‹¬ì„±! ({achieved/target_total*100:.1f}%)")
else:
    remaining = 10000 - achieved
    print(f"âš ï¸  {remaining:,}ê°œ ë” í•„ìš”")

print(f"\nğŸ“‚ ì €ì¥ ìœ„ì¹˜: {os.path.abspath(OUTPUT_DIR)}")
print("\nâœ… ìˆ˜ì§‘ ì™„ë£Œ! íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
