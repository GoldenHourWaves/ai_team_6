{
    // Place your snippets for python here. Each snippet is defined under a snippet name and has a prefix, body and 
    // description. The prefix is what is used to trigger the snippet and the body will be expanded and inserted. Possible variables are:
    // $1, $2 for tab stops, $0 for the final cursor position, and ${1:label}, ${2:another} for placeholders. Placeholders with the 
    // same ids are connected.
    // Example:
    // "Print to console": {
    // 	"prefix": "log",
    // 	"body": [
    // 		"console.log('$1');",
    // 		"$2"
    // 	],
    // 	"description": "Log output to console"
    // }
    "Python_Print": {
        "prefix": "pprint",
        "body": [
            "print('$1')"
        ],
        "description": "Log output to console"
    },
    "Python_Printf": {
        "prefix": "fstring",
        "body": [
            "print(f'$1')"
        ],
        "description": "fstring Log output to console"
    },
    "Python_Word_Cloud": {
        "prefix": "pythonwordcloud",
        "body": [
            "# í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„ ë° ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±",
            "# ë¶„ì„í•  íŒŒì¼ëª… 1 : ./data/datafile.csv",
            "# cvs íŒŒì¼ì—ì„œ text ì»¬ëŸ¼ì„ ì½ì–´ì„œ í˜•íƒœì†Œ ë¶„ì„ê¸°(Okt)ë¡œ í† í¬ë‚˜ì´ì§•",
            "# ë¬¸ì¥ì„ ë‹¨ì–´ë³„ë¡œ í† í¬ë‚˜ì´ì§•í•´ì„œ sub_listì— ì¶”ê°€, ì¡°ê±´ì€ ëª…ì‚¬ì´ê³  ê¸¸ì´ê°€ 2ê¸€ì ì´ìƒ",
            "# word_list ì—ëŠ” ëª¨ë“  ë‹¨ì–´ë“¤ì´ ë“¤ì–´ê°",
            "# wordë¥¼ counter ë¡œ ë³€í™˜í•˜ì—¬ ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±",
            "import pandas as pd",
            "from konlpy.tag import Okt ",
            "import re ",
            "from collections import Counter ",
            "from wordcloud import WordCloud ",
            "import matplotlib.pyplot as plt",
            "import koreanize_matplotlib",
            "# Dataë¥¼ í´ëœì§•í•˜ì—¬ text,score í˜•ëŒ€ë¡œ ë§Œë“ ë‹¤. indexë¥¼ ì œê±°í•œë‹¤",
            "df = pd.read_csv(",
            "    \"$1\", #./data/datafile.csv",
            "    index_col=0",
            ")",
            "",
            "okt = Okt()",
            "# ë°˜ë³µë¬¸",
            "word_list = []",
            "stopwords = [\"ì§„ì§œ\", \"ì¹´í†¡\", \"ë¡œê·¸ì¸\"]",
            "",
            "# íŒ¨í„´: [^0-9ê°€-í£a-zA-Z\\s]",
            "for sent in df[\"text\"]: # sent: ë¬¸ì¥",
            "    # print(\"STEP1. ë¬¸ì¥ì„ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\")",
            "    # ë¦¬ë·° í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬",
            "    # íŒ¨í„´: \"[^0-9a-zA-Zã„±-ã…ã…-ã…£ê°€-í£\\s]\" : ìˆ«ì, ì˜ì–´ ì†Œë¬¸ì, ì˜ì–´ ëŒ€ë¬¸ì, ììŒ, ëª¨ìŒ, í•œê¸€, ë„ì–´ì“°ê¸°ê°€ ì•„ë‹Œ ê²ƒ",
            "    clean_sent = re.sub(\"[^0-9ê°€-í£a-zA-Z\\\\s]\", \"\", sent)",
            "    # print(\"STEP2. ë¬¸ì¥ì„ í˜•íƒœì†Œë¶„ì„ê¸°ë¡œ í† í¬ë‚˜ì´ì§•í•©ë‹ˆë‹¤.\")",
            "    # í† í¬ë‚˜ì´ì§•(í˜•íƒœì†Œ ë¶„ì„ê¸°: Okt)",
            "    result = okt.pos(clean_sent)",
            "    # resultë¥¼ í•˜ë‚˜ì”© ë½‘ëŠ”ë‹¤. ",
            "    # print(\"STEP3. ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.\")",
            "    sub_list = []",
            "    # print(\"\\tíƒìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤.\")",
            "    for res in result: # res : (ë‹¨ì–´, í’ˆì‚¬)",
            "        word = res[0]",
            "        pos = res[1]",
            "        # wordê°€ stopwordsì— ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°",
            "        if word in stopwords:",
            "            # print(\"\\tê±´ë„ˆë›°ì–´!\", res)",
            "            continue",
            "",
            "        # sub_list ë§Œë“¤ê¸°: ì¡°ê±´ pos == \"Noun\" and len(word) > 1",
            "        if pos == \"Noun\" and len(word) > 1:",
            "            sub_list.append(word)",
            "word_list.extend(sub_list)",
            "print(f\"\\t[WORD LIST] {word_list}\")",
            "print(\"=\"*100)",
            "    ",
            "",
            "counter = Counter(word_list)",
            "wc = WordCloud(",
            "    font_path = \"C:\\\\Windows\\\\Fonts\\\\malgun.ttf\", # \"/usr/share/fonts/truetype/nanum/NanumGothic.ttf\"",
            "    background_color=\"white\",",
            "    width=800,",
            "    height=400",
            ")",
            "",
            "wc.generate_from_frequencies(counter)",
            "",
            "plt.figure(figsize=(5,5))",
            "plt.imshow(wc, interpolation=\"bilinear\")",
            "plt.axis(\"off\")",
            "plt.title(\"ë°°ë‹¬ì˜ ë¯¼ì¡± ì›Œë“œ í´ë¼ìš°ë“œ(ëª…ì‚¬)\", fontsize=15)",
            "plt.show()"
        ],
        "description": "í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„ ë° ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±"
    },
    "Python_Bar_Chart": {
        "prefix": "pythonBarChart",
        "body": [
            "#import pandas as pd",
            "#",
            "#import matplotlib.pyplot as plt",
            "#import koreanize_matplotlib",
            "#",
            "#df = pd.read_csv(",
            "#    \"./data/appreply2.csv\", # \"/content/appreply2.csv\",",
            "#    index_col=0",
            "#)",
            "#df",
            "",
            "df[\"$1\"].unique()",
            "$1_summary = df[\"$1\"].value_counts().sort_index()",
            "$1_summary",
            "",
            "plt.figure(figsize=(5,3))",
            "$1_summary.plot(kind=\"bar\")",
            "plt.title(\"$1 ë§‰ëŒ€ê·¸ë˜í”„\")",
            "plt.ylabel(\"Yì¶•ë¼ë²¨\")",
            "plt.xticks(rotation=0) # x ì¶• ë¼ë²¨ íšŒì „ì‹œí‚¤ê¸°",
            "plt.show()"
        ],
        "description": "ë°ì´í„°í”„ë ˆì„ ì»¬ëŸ¼ ë¶„ì„ ë° ë§‰ëŒ€ê·¸ë˜í”„ ìƒì„±"
    },
    "Python_Hist_chart": {
        "prefix": "pythonHistChart",
        "body": [
            "#import pandas as pd",
            "#import matplotlib.pyplot as plt",
            "#import koreanize_matplotlib",
            "#",
            "#df = pd.read_csv(",
            "#    \"./data/appreply2.csv\", # \"/content/appreply2.csv\",",
            "#    index_col=0",
            "#)",
            "#df",
            "df[\"$1\"] = df[\"text\"].apply(lambda x: len(x))",
            "df.sort_values(by=[\"$1\"], ascending=False)",
            "plt.figure(figsize=(5,3))",
            "plt.hist(df[\"$1\"], bins=30, edgecolor=\"black\")",
            "plt.title(\"$1 íˆìŠ¤í† ê·¸ë¨\")",
            "plt.xlabel(\"$1\")",
            "plt.ylabel(\"Yì¶•ë¼ë²¨\")",
            "plt.show()"
        ],
        "description": "ë°ì´í„°í”„ë ˆì„ ì»¬ëŸ¼ ë¶„ì„ ë° íˆìŠ¤í† ê·¸ë¨ ìƒì„±"
    },
    "Python_Hitmap": {
        "prefix": "pythonHeatmap",
        "body": [
            "# import pandas as pd",
            "# import koreanize_matplotlib # í•œê¸€ ì ìš©",
            "# import matplotlib.pyplot as plt",
            "# import seaborn as sns",
            "",
            "df2 = pd.read_csv(\"$1\")",
            "print(df2.head(2))",
            "",
            "# ì„±ë³„ X ì—°ë ¹ëŒ€ ë¶„í¬ì— ë”°ë¥¸ ê±´ë‹¹ ì´ìš©ì‹œê°„",
            "index = \"$2\"",
            "columns = \"$3\"",
            "values = \"$4\"",
            "pivot_table = df2.pivot_table(",
            "    values=values,    # Value ì»¬ëŸ¼",
            "    index=index,      # Yì¶• ì»¬ëŸ¼",
            "    columns=columns,  # Xì¶• ì»¬ëŸ¼",
            "    aggfunc=\"mean\"",
            ")",
            "",
            "print(pivot_table)",
            "sns.heatmap(",
            "    pivot_table,",
            "    annot=True,",
            "    fmt=\".0f\",",
            "    cmap=\"YlOrRd\"",
            ")",
            "plt.title(f\"{index} Ã— {columns} ë¶„í¬ì— ë”°ë¥¸ {values}\", fontsize=16)",
            "plt.xlabel(\"$2\")",
            "plt.ylabel(\"$3\")",
            "plt.show()"
        ],
        "description": "í”¼ë²— í…Œì´ë¸” ë° íˆíŠ¸ë§µ ìƒì„± - $1: CSV íŒŒì¼ ê²½ë¡œ, $2: index (Yì¶•), $3: columns (Xì¶•), $4: values (ê°’)"
    },
    "Python_Pairplot": {
        "prefix": "pythonPairplot",
        "body": [
            "# import pandas as pd",
            "# import matplotlib.pyplot as plt",
            "# import seaborn as sns",
            "# import koreanize_matplotlib",
            "",
            "sns.pairplot(",
            "    df[['$1', '$2']],",
            "    diag_kind='hist'",
            ")",
            "plt.show()"
        ],
        "description": "ì´ìƒì¹˜ íŒŒì•…ì„ ìœ„í•´ Pairplot ìƒì„± - $1: ì²« ë²ˆì§¸ ì»¬ëŸ¼ëª…, $2: ë‘ ë²ˆì§¸ ì»¬ëŸ¼ëª…"
    },
    "Python_Requests_data.seoul.go.kr": {
        "prefix": "pythonRequests_data.seoul.go.kr",
        "body": [
            "import os",
            "import requests",
            "from dotenv import load_dotenv",
            "load_dotenv()",
            "",
            "base_url = \"$1\"",
            "key = os.getenv(\"$2\")",
            "request_type = \"json\"",
            "service = \"$3\"",
            "start_index = 1",
            "end_index = 5",
            "step = 1000",
            "",
            "url = f\"{base_url}/{key}/{request_type}/{service}/{start_index}/{end_index}\"",
            "",
            "response = requests.get(url)",
            "data = response.json()",
            "print(data[\"$4\"].keys())",
            "items = data.get('$4').get('row')",
            "list_total_count = data.get('$4').get('list_total_count')",
            "print(list_total_count)",
            "",
            "items_all = []",
            "request_count = (int(list_total_count) // step) + 1",
            "print(request_count)",
            "",
            "for i in range(request_count):",
            "    start_idx = i * step + 1",
            "    end_idx = (i + 1) * step",
            "    uri = f\"{base_url}/{key}/{request_type}/{service}/{start_idx}/{end_idx}\"",
            "    print(uri)",
            "    response = requests.get(uri)",
            "    data = response.json()",
            "    items = data.get('$4').get('row')",
            "    items_all.extend(items)",
            "",
            "print(f\"\\nì „ì²´ ë°ì´í„° ê°œìˆ˜: {len(items_all)}\")",
            "",
            "print(\"\\nì²˜ìŒ 5ê°œ ë°ì´í„°:\")",
            "for item in items_all[:5]:",
            "    print(item)"
        ],
        "description": "Open API ë°ì´í„° ìˆ˜ì§‘\n$1: base_url\n$2: API_KEY í™˜ê²½ë³€ìˆ˜ëª…\n$3: serviceëª…\n$4: ë°ì´í„° í‚¤"
    },
    "Python_Requests_data.go.kr": {
        "prefix": "pythonRequests_data.go.kr",
        "body": [
            "from dotenv import load_dotenv",
            "import os",
            "import requests",
            "import math",
            "",
            "load_dotenv()",
            "",
            "base_url = \"$1\"",
            "",
            "# ìš”ì²­íŒŒë¼ë¯¸í„°",
            "serviceKey = os.getenv(\"$2\")",
            "searchYearCd = $3",
            "siDo = $4",
            "guGun = $5",
            "request_type = \"json\"",
            "numOfRows = 10",
            "pageNo = 1",
            "",
            "# base_url?ìš”ì²­íŒŒë¼ë¯¸í„°1=ê°’&ìš”ì²­íŒŒë¼ë¯¸í„°2=ê°’&....",
            "url = f\"{base_url}?ServiceKey={serviceKey}&searchYearCd={searchYearCd}&siDo={siDo}&guGun={guGun}&type={request_type}&numOfRows={numOfRows}&pageNo={pageNo}\"",
            "print(url)",
            "",
            "response = requests.get(url)",
            "print(response.status_code)",
            "",
            "data = response.json()",
            "print(data.keys())",
            "",
            "if (data.get('resultCode') != '00'):",
            "    print(\"Error: \", data.get('resultMsg'))",
            "    exit()",
            "",
            "totalCount = data.get('totalCount')",
            "print(totalCount)",
            "",
            "",
            "",
            "# ì „ì²´ ë°ì´í„°ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸",
            "item_list = []",
            "",
            "# í•„ìš”í•œ í˜ì´ì§€ ìˆ˜ ê³„ì‚° (totalCount / numOfRowsë¥¼ ì˜¬ë¦¼)",
            "total_pages = math.ceil(totalCount / numOfRows)",
            "print(f\"ì´ {totalCount}ê°œì˜ ë°ì´í„°ë¥¼ {total_pages}í˜ì´ì§€ì— ê±¸ì³ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\")",
            "",
            "# ê° í˜ì´ì§€ë¥¼ ë°˜ë³µí•˜ë©° ë°ì´í„° ìˆ˜ì§‘",
            "for page in range(1, total_pages + 1):",
            "    # URL ìƒì„±",
            "    url = f\"{base_url}?ServiceKey={serviceKey}&searchYearCd={searchYearCd}&siDo={siDo}&guGun={guGun}&type={request_type}&numOfRows={numOfRows}&pageNo={page}\"",
            "    ",
            "    # API ìš”ì²­",
            "    response = requests.get(url)",
            "    ",
            "    # ì‘ë‹µ í™•ì¸",
            "    if response.status_code == 200:",
            "        data = response.json()",
            "        ",
            "        # ì—ëŸ¬ ì²´í¬",
            "        if data.get('resultCode') == '00':",
            "            items = data.get('items', {}).get('item', [])",
            "            item_list.extend(items)",
            "            print(f\"í˜ì´ì§€ {page}/{total_pages}: {len(items)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ\")",
            "        else:",
            "            print(f\"í˜ì´ì§€ {page} ì—ëŸ¬: {data.get('resultMsg')}\")",
            "    else:",
            "        print(f\"í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {response.status_code}\")",
            "    ",
            "print(f\"\\nìµœì¢…ì ìœ¼ë¡œ ì´ {len(item_list)}ê°œì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.\")",
            "",
            "import pandas as pd",
            "",
            "# ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜",
            "df = pd.DataFrame(item_list)",
            "df.head()",
            "",
            "# ë°ì´í„° ì €ì¥",
            "df.to_csv(",
            "    \"$6\",",
            "    index=False,",
            "    encoding=\"utf-8-sig\"",
            ")",
            "print(\"ë°ì´í„° ì €ì¥ ì™„ë£Œ!\")"
        ],
        "description": "ê³µê³µë°ì´í„°í¬í„¸ API ë°ì´í„° í˜ì´ì§• ìˆ˜ì§‘\n$1: base_url\n$2: API_KEY í™˜ê²½ë³€ìˆ˜ëª…\n$3: searchYearCd\n$4: siDo\n$5: guGun\n$6: ì €ì¥ íŒŒì¼ ê²½ë¡œ"
    },
    "Python_Youtube_commentThreads": {
        "prefix": "pythonYoutubeCommentThreads",
        "body": [
            "from dotenv import load_dotenv",
            "import os",
            "import requests",
            "",
            "load_dotenv()",
            "",
            "base_url = \"https://www.googleapis.com/youtube/v3/commentThreads\"",
            "",
            "part = \"snippet\"",
            "videoId = \"$1\"                # ëŒ“ê¸€ ìˆ˜ì§‘í•  ì˜ìƒ",
            "key = os.getenv(\"YOUTUBE_API_KEY\")",
            "maxResults = 1000                       # ìµœëŒ€ 1000ê°œ",
            "textFormat = \"plainText\"",
            "",
            "url = f\"{base_url}?part={part}&videoId={videoId}&key={key}&maxResults={maxResults}&textFormat={textFormat}\"",
            "print(url)",
            "",
            "response = requests.get(url)",
            "",
            "data = response.json()",
            "print('pageInfo.totalResults', data.get('pageInfo').get('totalResults'));",
            "totalResults = data.get('pageInfo').get('totalResults')",
            "resultsPerPage = data.get('pageInfo').get('resultsPerPage')",
            "nextPageToken = data.get('nextPageToken')",
            "",
            "print(f\"ì´ ëŒ“ê¸€ ìˆ˜: {totalResults}\")",
            "print(f\"í˜ì´ì§€ë‹¹ ê²°ê³¼ ìˆ˜: {resultsPerPage}\")",
            "print()",
            "",
            "# ì „ì²´ ë°ì´í„°ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸",
            "item_list = []",
            "",
            "# ì²« ë²ˆì§¸ í˜ì´ì§€ ë°ì´í„° ì¶”ê°€",
            "if 'items' in data:",
            "    items = data['items']",
            "    item_list.extend(items)",
            "    print(f\"í˜ì´ì§€ 1: {len(items)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (ì´ {len(item_list)}ê°œ)\")",
            "",
            "# ë‹¤ìŒ í˜ì´ì§€ê°€ ìˆëŠ” ë™ì•ˆ ë°˜ë³µ",
            "page_count = 1",
            "while nextPageToken:",
            "    page_count += 1",
            "",
            "    ",
            "    # nextPageTokenì„ í¬í•¨í•œ URL ìƒì„±",
            "    url = f\"{base_url}?part={part}&videoId={videoId}&key={key}&maxResults={maxResults}&textFormat={textFormat}&pageToken={nextPageToken}\"",
            "    ",
            "    # API ìš”ì²­",
            "    response = requests.get(url)",
            "    ",
            "    # ì‘ë‹µ í™•ì¸",
            "    if response.status_code == 200:",
            "        data = response.json()",
            "        ",
            "        # items ì¶”ì¶œ ë° ì¶”ê°€",
            "        if 'items' in data:",
            "            items = data['items']",
            "            item_list.extend(items)",
            "            print(f\"í˜ì´ì§€ {page_count}: {len(items)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (ì´ {len(item_list)}ê°œ)\")",
            "        ",
            "        # ë‹¤ìŒ í˜ì´ì§€ í† í° ì—…ë°ì´íŠ¸",
            "        nextPageToken = data.get('nextPageToken')",
            "        ",
            "        # nextPageTokenì´ ì—†ìœ¼ë©´ ì¢…ë£Œ",
            "        if not nextPageToken:",
            "            print(\"ë” ì´ìƒ ìˆ˜ì§‘í•  í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.\")",
            "            break",
            "    else:",
            "        print(f\"í˜ì´ì§€ {page_count} ìš”ì²­ ì‹¤íŒ¨: {response.status_code}\")",
            "        if response.status_code == 403:",
            "            print(\"API í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆê±°ë‚˜ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.\")",
            "        break",
            "",
            "print(f\"\\nìµœì¢…ì ìœ¼ë¡œ ì´ {len(item_list)}ê°œì˜ ëŒ“ê¸€ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.\")",
            "",
            "# ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜",
            "import pandas as pd",
            "",
            "# ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ",
            "comment_data = []",
            "for item in item_list:",
            "    snippet = item['snippet']['topLevelComment']['snippet']",
            "    comment_data.append({",
            "        'author': snippet['authorDisplayName'],",
            "        'comment': snippet['textDisplay'],",
            "        'likeCount': snippet['likeCount'],",
            "        'publishedAt': snippet['publishedAt']",
            "    })",
            "",
            "df = pd.DataFrame(comment_data)",
            "print(df.head())",
            "",
            "# ë°ì´í„° ì €ì¥",
            "df.to_csv(",
            "    f\"./data/ìœ íŠœë¸Œ_ëŒ“ê¸€_{videoId}_(260127).csv\",",
            "    index=False,",
            ")",
            "print(\"ë°ì´í„° ì €ì¥ ì™„ë£Œ!\")"
        ],
        "description": "YouTube APIë¥¼ ì‚¬ìš©í•œ ëŒ“ê¸€ ìˆ˜ì§‘ ë° CSV ì €ì¥\n$1: videoId (ëŒ“ê¸€ ìˆ˜ì§‘í•  ìœ íŠœë¸Œ ì˜ìƒ ID)"
    },
    "Python_Naver_New_Search": {
        "prefix": "pythonNaverNewSearch",
        "body": [
            "from dotenv import load_dotenv",
            "import pandas as pd",
            "import requests",
            "import os",
            "from datetime import datetime",
            "",
            "load_dotenv()",
            "",
            "# ì„¤ì •",
            "BASE_URL = \"https://openapi.naver.com/v1/search/news.json\"",
            "QUERY = \"$1\"",
            "DISPLAY = 100",
            "SORT = \"sim\"",
            "",
            "# API Key í™•ì¸",
            "client_id = os.getenv(\"X-Naver-Client-Id\")",
            "client_secret = os.getenv(\"X-Naver-Client-Secret\")",
            "",
            "if not client_id or not client_secret:",
            "    print(\"âŒ ë„¤ì´ë²„ API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")",
            "    exit()",
            "",
            "# ìš”ì²­ íŒŒë¼ë¯¸í„°",
            "params = {",
            "    \"query\": QUERY,",
            "    \"display\": DISPLAY,",
            "    \"start\": 1,",
            "    \"sort\": SORT",
            "}",
            "",
            "# í—¤ë”",
            "headers = {",
            "    \"X-Naver-Client-Id\": client_id,",
            "    \"X-Naver-Client-Secret\": client_secret",
            "}",
            "",
            "# API ìš”ì²­",
            "try:",
            "    response = requests.get(BASE_URL, params=params, headers=headers, timeout=10)",
            "    response.raise_for_status()",
            "except requests.exceptions.HTTPError:",
            "    print(f\"âŒ API ìš”ì²­ ì‹¤íŒ¨: {response.status_code}\")",
            "    if response.status_code == 400:",
            "        print(\"ì˜ëª»ëœ ìš”ì²­ì…ë‹ˆë‹¤.\")",
            "    elif response.status_code == 401:",
            "        print(\"ì¸ì¦ ì‹¤íŒ¨. API Keyë¥¼ í™•ì¸í•˜ì„¸ìš”.\")",
            "    elif response.status_code == 403:",
            "        print(\"API í• ë‹¹ëŸ‰ ì´ˆê³¼ ë˜ëŠ” ê¶Œí•œ ì—†ìŒ\")",
            "    exit()",
            "except requests.exceptions.RequestException as e:",
            "    print(f\"âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}\")",
            "    exit()",
            "",
            "# JSON íŒŒì‹±",
            "try:",
            "    data = response.json()",
            "except ValueError as e:",
            "    print(f\"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}\")",
            "    exit()",
            "",
            "# ë°ì´í„° ì¶”ì¶œ",
            "item_list = data.get(\"items\", [])",
            "total_count = data.get(\"total\", 0)",
            "",
            "print(f\"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: ì´ {total_count:,}ê±´\")",
            "print(f\"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(item_list)}ê°œ\\n\")",
            "",
            "# ë°ì´í„°í”„ë ˆì„ ìƒì„±",
            "df = pd.DataFrame(item_list)",
            "print(\"ğŸ“‹ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\")",
            "print(df.head())",
            "print(f\"\\nğŸ“ˆ ë°ì´í„° í†µê³„: {len(df):,}í–‰ Ã— {len(df.columns)}ì—´\\n\")",
            "",
            "# ë””ë ‰í† ë¦¬ ìƒì„±",
            "os.makedirs('./data', exist_ok=True)",
            "",
            "# ë°ì´í„° ì €ì¥",
            "today = datetime.now().strftime('%y%m%d')",
            "filename = f\"./data/ë„¤ì´ë²„_ë‰´ìŠ¤_{QUERY}_({today}).csv\"",
            "df.to_csv(filename, index=False, encoding='utf-8-sig')",
            "print(f\"ğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename}\")"
        ],
        "description": "ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API ë°ì´í„° ìˆ˜ì§‘ ë° CSV ì €ì¥\n$1: ê²€ìƒ‰ í‚¤ì›Œë“œ"
    },
    "Python_File_Merge": {
        "prefix": "PythonFileMerge",
        "body": [
            "import pandas as pd",
            "import folium",
            "from datetime import datetime",
            "",
            "",
            "",
            "file1 = \"$1\"",
            "file2 = \"$2\"",
            "myKey = '$3'",
            "",
            "df = pd.read_csv(file1)",
            "print(df.head(2))",
            "",
            "loc_df = pd.read_csv(file2)",
            "print(loc_df.head(2))",
            "",
            "# file1 key colums...",
            "file1_colums = ['$4', '$5', '$6']",
            "file1_df_new = df[[myKey] + file1_colums]",
            "",
            "print(file1_df_new)",
            "",
            "# file2 key colums...",
            "file2_key = '$7'",
            "file2_colums = ['$8', '$9']",
            "file2_df_new = loc_df[ [file2_key] + file2_colums ]",
            "file2_df_new.columns = [myKey, \"LAT\", \"LONG\"]",
            "print(file2_df_new)",
            "",
            "merge_data = pd.merge(",
            "    left=file1_df_new,          # ë°ì´í„°1",
            "    right=file2_df_new,         # ë°ì´í„°2",
            "    how=\"left\",                 # ì·¨í•©ë°©ë²•(left, right, inner, outer)",
            "    on=myKey                    # ê¸°ì¤€",
            ")",
            "print(merge_data)",
            "",
            "today = datetime.now().strftime('%y%m%d')",
            "filename = f\"./data/merged_{myKey}_{today}.csv\"",
            "merge_data.to_csv(filename, index=False, encoding='utf-8-sig')",
            "print(f\"ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename}\")"
        ],
        "description": "ë‘ ê°œì˜ CSV íŒŒì¼ì„ ë³‘í•©í•˜ê³  ì €ì¥\n$1: file1 ê²½ë¡œ\n$2: file2 ê²½ë¡œ\n$3: ë³‘í•© í‚¤\n$4-$6: file1 ì»¬ëŸ¼ë“¤\n$7: file2 í‚¤\n$8-$9: file2 ì»¬ëŸ¼ë“¤"
    },
}