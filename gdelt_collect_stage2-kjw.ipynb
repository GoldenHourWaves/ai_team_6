{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86898f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import io\n",
    "import zipfile\n",
    "import re\n",
    "import os\n",
    "\n",
    "def fetch_gdelt_gkg_daily(date_str):\n",
    "    \"\"\"\n",
    "    íŠ¹ì • ë‚ ì§œì˜ GDELT GKG 2.0 ë°ì´í„° ìˆ˜ì§‘ (Bitcoin ê´€ë ¨ ì˜ì–´ ê¸°ì‚¬ë§Œ)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    date_str : str\n",
    "        ìˆ˜ì§‘í•  ë‚ ì§œ (YYYYMMDD í˜•ì‹, ì˜ˆ: '20250901')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list : ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ê° í•­ëª©ì€ ë”•ì…”ë„ˆë¦¬)\n",
    "    \"\"\"\n",
    "    news_data = []  # ìˆ˜ì§‘í•œ ë‰´ìŠ¤ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "    \n",
    "    # í•˜ë£¨ë¥¼ 24ì‹œê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ê° ì‹œê°„ë§ˆë‹¤ ë°ì´í„° ìˆ˜ì§‘\n",
    "    for hour in range(0, 24):\n",
    "        # ì‹œê°„ ë¬¸ìì—´ ìƒì„± (ì˜ˆ: 00ì‹œ -> \"000000\")\n",
    "        time_str = f\"{hour:02d}0000\"\n",
    "        \n",
    "        # GDELT GKG 2.0 íŒŒì¼ URL ìƒì„±\n",
    "        # ì˜ˆ: http://data.gdeltproject.org/gdeltv2/20250901000000.gkg.csv.zip\n",
    "        gkg_url = f\"http://data.gdeltproject.org/gdeltv2/{date_str}{time_str}.gkg.csv.zip\"\n",
    "        \n",
    "        try:\n",
    "            # HTTP ìš”ì²­ìœ¼ë¡œ ZIP íŒŒì¼ ë‹¤ìš´ë¡œë“œ (10ì´ˆ íƒ€ì„ì•„ì›ƒ)\n",
    "            response = requests.get(gkg_url, timeout=10)\n",
    "            \n",
    "            # ìš”ì²­ ì„±ê³µ ì‹œ (ìƒíƒœ ì½”ë“œ 200)\n",
    "            if response.status_code == 200:\n",
    "                hour_count = 0  # í˜„ì¬ ì‹œê°„ëŒ€ì—ì„œ ìˆ˜ì§‘í•œ ê¸°ì‚¬ ìˆ˜\n",
    "                \n",
    "                # ZIP íŒŒì¼ ì••ì¶• í•´ì œ\n",
    "                with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "                    # ZIP ë‚´ë¶€ì˜ ëª¨ë“  íŒŒì¼ ìˆœíšŒ\n",
    "                    for filename in z.namelist():\n",
    "                        with z.open(filename) as f:\n",
    "                            # CSV íŒŒì¼ ì½ê¸°\n",
    "                            # - sep='\\t': íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ ë°ì´í„°\n",
    "                            # - header=None: í—¤ë” ì—†ìŒ\n",
    "                            # - dtype=str: ëª¨ë“  ë°ì´í„°ë¥¼ ë¬¸ìì—´ë¡œ ì½ê¸°\n",
    "                            # - on_bad_lines='skip': ì˜ëª»ëœ í–‰ì€ ê±´ë„ˆë›°ê¸°\n",
    "                            # - encoding_errors='ignore': ì¸ì½”ë”© ì˜¤ë¥˜ ë¬´ì‹œ\n",
    "                            df = pd.read_csv(f, sep='\\t', header=None, dtype=str, \n",
    "                                           on_bad_lines='skip', encoding='utf-8', \n",
    "                                           encoding_errors='ignore')\n",
    "                            \n",
    "                            # CSVì˜ ê° í–‰(ê¸°ì‚¬) ì²˜ë¦¬\n",
    "                            for idx, row in df.iterrows():\n",
    "                                # ì‹œê°„ë‹¹ 10ê°œ ì œí•œì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨\n",
    "                                if hour_count >= 10:\n",
    "                                    break\n",
    "                                    \n",
    "                                try:\n",
    "                                    # GKG 2.0 CSVëŠ” ìµœì†Œ 16ê°œ ì»¬ëŸ¼ í•„ìš”\n",
    "                                    if len(row) > 15:\n",
    "                                        # ===== 1. ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ =====\n",
    "                                        # row[4]: DocumentIdentifier (ê¸°ì‚¬ URL)\n",
    "                                        url = str(row[4]).strip() if len(row) > 4 else ''\n",
    "                                        \n",
    "                                        # row[3]: SourceCommonName (ì¶œì²˜)\n",
    "                                        source = str(row[3]).strip() if len(row) > 3 else ''\n",
    "                                        \n",
    "                                        # row[7]: V2Themes (ì£¼ì œ/í…Œë§ˆ)\n",
    "                                        v2_themes = str(row[7]).strip() if len(row) > 7 else ''\n",
    "                                        \n",
    "                                        # ===== 2. Bitcoin í‚¤ì›Œë“œ í•„í„°ë§ =====\n",
    "                                        # URL, ì¶œì²˜, í…Œë§ˆë¥¼ í•©ì³ì„œ Bitcoin ê´€ë ¨ í‚¤ì›Œë“œ ê²€ìƒ‰\n",
    "                                        bitcoin_keywords = ['bitcoin', 'btc', 'cryptocurrency', 'crypto']\n",
    "                                        text_to_check = f\"{url} {source} {v2_themes}\".lower()\n",
    "                                        \n",
    "                                        # í‚¤ì›Œë“œê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°\n",
    "                                        if not any(keyword in text_to_check for keyword in bitcoin_keywords):\n",
    "                                            continue\n",
    "                                        \n",
    "                                        # ===== 3. ì˜ì–´ ê¸°ì‚¬ë§Œ í•„í„°ë§ =====\n",
    "                                        # ë¹„ì˜ì–´ê¶Œ ë„ë©”ì¸ì„ ê°€ì§„ URL ì œì™¸\n",
    "                                        non_english_domains = ['.cn', '.jp', '.kr', '.ru', '.de', '.fr', '.es', '.it']\n",
    "                                        if any(domain in url.lower() for domain in non_english_domains):\n",
    "                                            continue\n",
    "                                        \n",
    "                                        # ===== 4. Tone ë°ì´í„° ì¶”ì¶œ =====\n",
    "                                        # row[15]: Tone (ê°ì • ì ìˆ˜)\n",
    "                                        # í˜•ì‹: \"AvgTone,PositiveScore,NegativeScore,Polarity,ActivityRefDensity,SelfRefDensity,WordCount\"\n",
    "                                        tone_str = str(row[15]).strip()\n",
    "                                        tone_parts = tone_str.split(',')  # ì‰¼í‘œë¡œ ë¶„ë¦¬\n",
    "                                        \n",
    "                                        # ê° Tone ê°’ ì¶”ì¶œ (ê°’ì´ ì—†ìœ¼ë©´ 0.0)\n",
    "                                        avg_tone = float(tone_parts[0]) if len(tone_parts) > 0 and tone_parts[0] else 0.0\n",
    "                                        positive_score = float(tone_parts[1]) if len(tone_parts) > 1 and tone_parts[1] else 0.0\n",
    "                                        negative_score = float(tone_parts[2]) if len(tone_parts) > 2 and tone_parts[2] else 0.0\n",
    "                                        polarity = float(tone_parts[3]) if len(tone_parts) > 3 and tone_parts[3] else 0.0\n",
    "                                        \n",
    "                                        # ===== 5. ë°ì´í„° ì €ì¥ =====\n",
    "                                        # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë‰´ìŠ¤ ë°ì´í„° ì¶”ê°€\n",
    "                                        news_data.append({\n",
    "                                            'date': date_str,                    # ë‚ ì§œ\n",
    "                                            'time': f\"{hour:02d}:00\",            # ì‹œê°„\n",
    "                                            'source': source,                     # ì¶œì²˜\n",
    "                                            'url': url,                           # URL\n",
    "                                            'v2_themes': v2_themes,              # ì£¼ì œ\n",
    "                                            'avg_tone': avg_tone,                # í‰ê·  ê°ì • ì ìˆ˜ (-100 ~ +100)\n",
    "                                            'positive_score': positive_score,    # ê¸ì • ì ìˆ˜ (0~100)\n",
    "                                            'negative_score': negative_score,    # ë¶€ì • ì ìˆ˜ (0~100)\n",
    "                                            'polarity': polarity,                # ê·¹ì„± (0~100)\n",
    "                                            'title': 'N/A'                        # ì œëª© (ë‚˜ì¤‘ì— ì¶”ê°€)\n",
    "                                        })\n",
    "                                        \n",
    "                                        hour_count += 1  # ìˆ˜ì§‘ ì¹´ìš´íŠ¸ ì¦ê°€\n",
    "                                        \n",
    "                                except (ValueError, IndexError) as e:\n",
    "                                    # ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜ ì‹œ ê±´ë„ˆë›°ê¸°\n",
    "                                    continue\n",
    "                \n",
    "                # í˜„ì¬ ì‹œê°„ëŒ€ì—ì„œ ìˆ˜ì§‘í•œ ê¸°ì‚¬ê°€ ìˆìœ¼ë©´ ì¶œë ¥\n",
    "                if hour_count > 0:\n",
    "                    print(f\"âœ“ {date_str} {hour:02d}:00 ì™„ë£Œ (Bitcoin ê´€ë ¨ {hour_count}ê°œ)\")\n",
    "        \n",
    "        except requests.exceptions.RequestException:\n",
    "            # ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ (íŒŒì¼ì´ ì—†ì„ ìˆ˜ ìˆìŒ) - ì¡°ìš©íˆ ê±´ë„ˆë›°ê¸°\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            # ê¸°íƒ€ ì˜ˆì™¸ ë°œìƒ ì‹œ ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥\n",
    "            print(f\"âš  {date_str} {hour:02d}:00 ì˜¤ë¥˜: {str(e)}\")\n",
    "    \n",
    "    return news_data  # í•˜ë£¨ì¹˜ ìˆ˜ì§‘ ë°ì´í„° ë°˜í™˜\n",
    "\n",
    "\n",
    "def extract_title_from_url_batch(news_data, batch_size=30):\n",
    "    \"\"\"\n",
    "    ë°°ì¹˜ ë‹¨ìœ„ë¡œ URLì—ì„œ ì œëª© ì¶”ì¶œ (ì†ë„ ê°œì„ ì„ ìœ„í•´ ì¼ë¶€ë§Œ ì²˜ë¦¬)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    news_data : list\n",
    "        ë‰´ìŠ¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n",
    "    batch_size : int\n",
    "        ì œëª©ì„ ì¶”ì¶œí•  ê¸°ì‚¬ ê°œìˆ˜ (ê¸°ë³¸ê°’: 30ê°œ)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list : ì œëª©ì´ ì¶”ê°€ëœ ë‰´ìŠ¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    print(f\"  [*] ì œëª© ì¶”ì¶œ ì¤‘ (ì´ {len(news_data)}ê°œ ì¤‘ {batch_size}ê°œë§Œ)...\")\n",
    "    \n",
    "    # ì§€ì •ëœ ê°œìˆ˜ë§Œí¼ë§Œ ì œëª© ì¶”ì¶œ (ë„ˆë¬´ ë§ìœ¼ë©´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)\n",
    "    for idx, item in enumerate(news_data[:batch_size]):\n",
    "        try:\n",
    "            url = item['url']\n",
    "            # URLì´ ìœ íš¨í•œ HTTP ì£¼ì†Œì¸ì§€ í™•ì¸\n",
    "            if url and url != 'N/A' and url.startswith('http'):\n",
    "                # HTTP ìš”ì²­ìœ¼ë¡œ ì›¹í˜ì´ì§€ ë‹¤ìš´ë¡œë“œ (3ì´ˆ íƒ€ì„ì•„ì›ƒ)\n",
    "                # User-Agent í—¤ë”: ë¸Œë¼ìš°ì €ì¸ ê²ƒì²˜ëŸ¼ ìœ„ì¥\n",
    "                response = requests.get(url, timeout=3, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                \n",
    "                # ìš”ì²­ ì„±ê³µ ì‹œ\n",
    "                if response.status_code == 200:\n",
    "                    # HTMLì—ì„œ <title> íƒœê·¸ ì°¾ê¸° (ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©)\n",
    "                    title_match = re.search(r'<title>(.*?)</title>', response.text, re.IGNORECASE)\n",
    "                    if title_match:\n",
    "                        # ì œëª© ì¶”ì¶œ ë° ì•ë’¤ ê³µë°± ì œê±°\n",
    "                        item['title'] = title_match.group(1).strip()\n",
    "        except Exception:\n",
    "            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ìœ ì§€ ('N/A')\n",
    "            pass\n",
    "    \n",
    "    return news_data\n",
    "\n",
    "\n",
    "def save_daily_csv(news_data, date_str, output_dir='./data/news'):\n",
    "    \"\"\"\n",
    "    í•˜ë£¨ì¹˜ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    news_data : list\n",
    "        ì €ì¥í•  ë‰´ìŠ¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n",
    "    date_str : str\n",
    "        ë‚ ì§œ ë¬¸ìì—´ (YYYYMMDD)\n",
    "    output_dir : str\n",
    "        ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ (ê¸°ë³¸ê°’: './data/news')\n",
    "    \"\"\"\n",
    "    # ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ\n",
    "    if len(news_data) == 0:\n",
    "        print(f\"  âš  {date_str} ì €ì¥í•  ë°ì´í„° ì—†ìŒ\")\n",
    "        return\n",
    "    \n",
    "    # ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±\n",
    "    # exist_ok=True: ì´ë¯¸ ì¡´ì¬í•´ë„ ì˜¤ë¥˜ ë°œìƒ ì•ˆ í•¨\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # ë¦¬ìŠ¤íŠ¸ë¥¼ DataFrame(í‘œ í˜•íƒœ)ìœ¼ë¡œ ë³€í™˜\n",
    "    df = pd.DataFrame(news_data)\n",
    "    \n",
    "    # ì»¬ëŸ¼ ìˆœì„œ ì§€ì •\n",
    "    columns = ['date', 'time', 'title', 'source', 'url', 'v2_themes', \n",
    "               'avg_tone', 'positive_score', 'negative_score', 'polarity']\n",
    "    # ì§€ì •ëœ ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "    df = df[[c for c in columns if c in df.columns]]\n",
    "    \n",
    "    # íŒŒì¼ëª… ìƒì„± (ì˜ˆ: bitcoin_news_20250901.csv)\n",
    "    filename = os.path.join(output_dir, f'bitcoin_news_{date_str}.csv')\n",
    "    \n",
    "    # CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "    # - index=False: ì¸ë±ìŠ¤ ë²ˆí˜¸ ì œì™¸\n",
    "    # - encoding='utf-8-sig': í•œê¸€ ë° íŠ¹ìˆ˜ë¬¸ì ì§€ì› (Excel í˜¸í™˜)\n",
    "    df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # í†µê³„ ì •ë³´ ì¶œë ¥\n",
    "    avg_tone = df['avg_tone'].astype(float).mean()  # í‰ê·  Tone ì ìˆ˜\n",
    "    print(f\"  âœ“ {date_str} ì €ì¥ ì™„ë£Œ: {len(df)}ê°œ ê¸°ì‚¬, í‰ê·  Tone: {avg_tone:.2f}\")\n",
    "\n",
    "\n",
    "def fetch_and_save_daily(start_date, end_date):\n",
    "    \"\"\"\n",
    "    ë‚ ì§œ ë²”ìœ„ ë™ì•ˆ ë§¤ì¼ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ì €ì¥\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : str\n",
    "        ì‹œì‘ ë‚ ì§œ (YYYYMMDD)\n",
    "    end_date : str\n",
    "        ì¢…ë£Œ ë‚ ì§œ (YYYYMMDD)\n",
    "    \"\"\"\n",
    "    # ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜\n",
    "    current_date = datetime.strptime(start_date, '%Y%m%d')\n",
    "    end = datetime.strptime(end_date, '%Y%m%d')\n",
    "    \n",
    "    total_count = 0  # ì „ì²´ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜\n",
    "    \n",
    "    # ì‹œì‘ì¼ë¶€í„° ì¢…ë£Œì¼ê¹Œì§€ í•˜ë£¨ì”© ì¦ê°€í•˜ë©° ë°˜ë³µ\n",
    "    while current_date <= end:\n",
    "        # datetimeì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (YYYYMMDD)\n",
    "        date_str = current_date.strftime('%Y%m%d')\n",
    "        \n",
    "        print(f\"\\n[{date_str}] ë°ì´í„° ìˆ˜ì§‘ ì¤‘...\")\n",
    "        \n",
    "        # ===== 1. í•˜ë£¨ì¹˜ ë°ì´í„° ìˆ˜ì§‘ =====\n",
    "        news_data = fetch_gdelt_gkg_daily(date_str)\n",
    "        \n",
    "        # ===== 2. ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì²˜ë¦¬ =====\n",
    "        if len(news_data) > 0:\n",
    "            # ì œëª© ì¶”ì¶œ (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ 30ê°œë§Œ)\n",
    "            news_data = extract_title_from_url_batch(news_data, batch_size=30)\n",
    "            \n",
    "            # CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "            save_daily_csv(news_data, date_str)\n",
    "            \n",
    "            # ì „ì²´ ì¹´ìš´íŠ¸ ëˆ„ì \n",
    "            total_count += len(news_data)\n",
    "        else:\n",
    "            print(f\"  âš  {date_str} ë°ì´í„° ì—†ìŒ\")\n",
    "        \n",
    "        # ===== 3. ë‹¤ìŒ ë‚ ë¡œ ì´ë™ =====\n",
    "        current_date += timedelta(days=1)\n",
    "    \n",
    "    # ì „ì²´ ì™„ë£Œ ë©”ì‹œì§€\n",
    "    print(f\"\\nâœ… ì „ì²´ ì™„ë£Œ: ì´ {total_count}ê°œ ê¸°ì‚¬ ì €ì¥\")\n",
    "\n",
    "\n",
    "# ===== ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ =====\n",
    "if __name__ == \"__main__\":\n",
    "    # ìˆ˜ì§‘í•  ë‚ ì§œ ë²”ìœ„ ì„¤ì •\n",
    "    start_date = \"20250901\"  # 2025ë…„ 9ì›” 1ì¼\n",
    "    end_date = \"20250930\"    # 2025ë…„ 9ì›” 30ì¼\n",
    "    \n",
    "    print(\"GDELT GKG 2.0 Bitcoin ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘ (ì¼ë³„ ì €ì¥)\")\n",
    "    print(f\"ê¸°ê°„: {start_date} ~ {end_date}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥ ì‹¤í–‰\n",
    "    fetch_and_save_daily(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b90d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def merge_bitcoin_news_csv(start_date, end_date, input_dir='./data/news', output_file='bitcoin_news_merged.csv'):\n",
    "    \"\"\"\n",
    "    ë‚ ì§œ ë²”ìœ„ì˜ Bitcoin ë‰´ìŠ¤ CSV íŒŒì¼ë“¤ì„ í•˜ë‚˜ë¡œ ë³‘í•©\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : str\n",
    "        ì‹œì‘ ë‚ ì§œ (YYYYMMDD í˜•ì‹, ì˜ˆ: '20250901')\n",
    "    end_date : str\n",
    "        ì¢…ë£Œ ë‚ ì§œ (YYYYMMDD í˜•ì‹, ì˜ˆ: '20251031')\n",
    "    input_dir : str\n",
    "        CSV íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ (ê¸°ë³¸ê°’: './data/news')\n",
    "    output_file : str\n",
    "        ë³‘í•©ëœ íŒŒì¼ì˜ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸ê°’: 'bitcoin_news_merged.csv')\n",
    "    \"\"\"\n",
    "    \n",
    "    # ë³‘í•©í•  ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "    all_data = []\n",
    "    \n",
    "    # ë‚ ì§œ ë²”ìœ„ ìƒì„±\n",
    "    current_date = datetime.strptime(start_date, '%Y%m%d')\n",
    "    end = datetime.strptime(end_date, '%Y%m%d')\n",
    "    \n",
    "    # íŒŒì¼ ì¹´ìš´í„°\n",
    "    file_count = 0\n",
    "    success_count = 0\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Bitcoin ë‰´ìŠ¤ CSV íŒŒì¼ ë³‘í•© ì‹œì‘\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ì‹œì‘ì¼ë¶€í„° ì¢…ë£Œì¼ê¹Œì§€ í•˜ë£¨ì”© ë°˜ë³µ\n",
    "    while current_date <= end:\n",
    "        # ë‚ ì§œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (YYYYMMDD)\n",
    "        date_str = current_date.strftime('%Y%m%d')\n",
    "        \n",
    "        # íŒŒì¼ëª… ìƒì„± (ì˜ˆ: bitcoin_news_20250901.csv)\n",
    "        filename = os.path.join(input_dir, f'bitcoin_news_{date_str}.csv')\n",
    "        \n",
    "        # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
    "        if os.path.exists(filename):\n",
    "            try:\n",
    "                # CSV íŒŒì¼ ì½ê¸°\n",
    "                # - encoding='utf-8-sig': í•œê¸€ ë° BOM ì²˜ë¦¬\n",
    "                df = pd.read_csv(filename, encoding='utf-8-sig')\n",
    "                \n",
    "                # ë°ì´í„°í”„ë ˆì„ì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "                all_data.append(df)\n",
    "                \n",
    "                # ì„±ê³µ ì¹´ìš´íŠ¸ ì¦ê°€\n",
    "                success_count += 1\n",
    "                \n",
    "                # ì§„í–‰ ìƒí™© ì¶œë ¥\n",
    "                print(f\"âœ“ {date_str}: {len(df)}ê°œ ê¸°ì‚¬ ë¡œë“œ ì™„ë£Œ\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                # íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥\n",
    "                print(f\"âœ— {date_str}: ì˜¤ë¥˜ - {str(e)}\")\n",
    "        else:\n",
    "            # íŒŒì¼ì´ ì—†ìœ¼ë©´ ê²½ê³  ë©”ì‹œì§€\n",
    "            print(f\"âš  {date_str}: íŒŒì¼ ì—†ìŒ\")\n",
    "        \n",
    "        # ì „ì²´ íŒŒì¼ ì¹´ìš´íŠ¸ ì¦ê°€\n",
    "        file_count += 1\n",
    "        \n",
    "        # ë‹¤ìŒ ë‚ ë¡œ ì´ë™\n",
    "        current_date += timedelta(days=1)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ë³‘í•©í•  ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸\n",
    "    if len(all_data) == 0:\n",
    "        print(\"âš  ë³‘í•©í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "        return\n",
    "    \n",
    "    # ëª¨ë“  ë°ì´í„°í”„ë ˆì„ì„ í•˜ë‚˜ë¡œ ë³‘í•©\n",
    "    # - ignore_index=True: ì¸ë±ìŠ¤ë¥¼ ìƒˆë¡œ ìƒì„±\n",
    "    merged_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬ (date ì»¬ëŸ¼ ê¸°ì¤€)\n",
    "    if 'date' in merged_df.columns:\n",
    "        merged_df = merged_df.sort_values(by=['date', 'time'])\n",
    "    \n",
    "    # ë³‘í•©ëœ íŒŒì¼ ì €ì¥\n",
    "    # - index=False: ì¸ë±ìŠ¤ ë²ˆí˜¸ ì œì™¸\n",
    "    # - encoding='utf-8-sig': Excel í˜¸í™˜\n",
    "    merged_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # ê²°ê³¼ í†µê³„ ì¶œë ¥\n",
    "    print(f\"\\nğŸ“Š ë³‘í•© ê²°ê³¼:\")\n",
    "    print(f\"  - ì²˜ë¦¬ëœ íŒŒì¼: {success_count}/{file_count}ê°œ\")\n",
    "    print(f\"  - ì´ ê¸°ì‚¬ ìˆ˜: {len(merged_df)}ê°œ\")\n",
    "    print(f\"  - ì €ì¥ ìœ„ì¹˜: {output_file}\")\n",
    "    \n",
    "    # ë‚ ì§œë³„ í†µê³„\n",
    "    if 'date' in merged_df.columns:\n",
    "        print(f\"\\nğŸ“… ë‚ ì§œë³„ ê¸°ì‚¬ ìˆ˜:\")\n",
    "        date_counts = merged_df['date'].value_counts().sort_index()\n",
    "        for date, count in date_counts.items():\n",
    "            print(f\"  {date}: {count}ê°œ\")\n",
    "    \n",
    "    # Tone í†µê³„ (avg_toneì´ ìˆëŠ” ê²½ìš°)\n",
    "    if 'avg_tone' in merged_df.columns:\n",
    "        avg_tone = merged_df['avg_tone'].astype(float).mean()\n",
    "        print(f\"\\nğŸ˜Š í‰ê·  Tone ì ìˆ˜: {avg_tone:.2f}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"âœ… ë³‘í•© ì™„ë£Œ!\")\n",
    "\n",
    "\n",
    "# ===== ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ =====\n",
    "if __name__ == \"__main__\":\n",
    "    # ë³‘í•©í•  ë‚ ì§œ ë²”ìœ„ ì„¤ì •\n",
    "    start_date = \"20250901\"  # 2025ë…„ 9ì›” 1ì¼\n",
    "    end_date = \"20251031\"    # 2025ë…„ 10ì›” 31ì¼\n",
    "    \n",
    "    # ì…ë ¥/ì¶œë ¥ ê²½ë¡œ ì„¤ì •\n",
    "    input_directory = \"./data/news\"  # CSV íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë”\n",
    "    output_filename = \"bitcoin_news_merged.csv\"  # ë³‘í•©ëœ íŒŒì¼ëª…\n",
    "    \n",
    "    # ë³‘í•© ì‹¤í–‰\n",
    "    merge_bitcoin_news_csv(\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        input_dir=input_directory,\n",
    "        output_file=output_filename\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f6993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_title_column(input_file='bitcoin_news_merged.csv', output_file='bitcoin_news_merged.csv'):\n",
    "    \"\"\"\n",
    "    CSV íŒŒì¼ì—ì„œ title ì»¬ëŸ¼ ì œê±°\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_file : str\n",
    "        ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: 'bitcoin_news_merged.csv')\n",
    "    output_file : str\n",
    "        ì¶œë ¥ CSV íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: 'bitcoin_news_merged.csv' - ë®ì–´ì“°ê¸°)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Title ì»¬ëŸ¼ ì œê±° ì‘ì—… ì‹œì‘\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # CSV íŒŒì¼ ì½ê¸°\n",
    "        print(f\"\\n[*] íŒŒì¼ ë¡œë“œ ì¤‘: {input_file}\")\n",
    "        df = pd.read_csv(input_file, encoding='utf-8-sig')\n",
    "        \n",
    "        # ë¡œë“œëœ ë°ì´í„° ì •ë³´ ì¶œë ¥\n",
    "        print(f\"âœ“ ì´ {len(df)}ê°œ í–‰ ë¡œë“œ ì™„ë£Œ\")\n",
    "        print(f\"âœ“ ê¸°ì¡´ ì»¬ëŸ¼: {list(df.columns)}\")\n",
    "        \n",
    "        # title ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸\n",
    "        if 'title' in df.columns:\n",
    "            # title ì»¬ëŸ¼ ì œê±°\n",
    "            df = df.drop(columns=['title'])\n",
    "            print(f\"\\nâœ“ 'title' ì»¬ëŸ¼ ì œê±° ì™„ë£Œ\")\n",
    "        else:\n",
    "            print(f\"\\nâš  'title' ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤\")\n",
    "        \n",
    "        # ìƒˆë¡œìš´ ì»¬ëŸ¼ ëª©ë¡ ì¶œë ¥\n",
    "        print(f\"âœ“ ìƒˆ ì»¬ëŸ¼: {list(df.columns)}\")\n",
    "        \n",
    "        # íŒŒì¼ ì €ì¥\n",
    "        print(f\"\\n[*] íŒŒì¼ ì €ì¥ ì¤‘: {output_file}\")\n",
    "        df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(f\"âœ“ ì €ì¥ ì™„ë£Œ!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"âœ… ì‘ì—… ì™„ë£Œ!\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"âœ— ì˜¤ë¥˜: '{input_file}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "\n",
    "\n",
    "# ===== ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ =====\n",
    "if __name__ == \"__main__\":\n",
    "    # title ì»¬ëŸ¼ ì œê±° ì‹¤í–‰\n",
    "    remove_title_column(\n",
    "        input_file='bitcoin_news_merged.csv',\n",
    "        output_file='bitcoin_news_no_title.csv'  # ë‹¤ë¥¸ íŒŒì¼ë¡œ ë®ì–´ì“°ê¸°\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-project-01-team6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
