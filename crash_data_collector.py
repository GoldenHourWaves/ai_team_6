#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë¹„ì •í˜• ë°ì´í„° ì„ í–‰ì„± ì…ì¦ - ë°ì´í„° ìˆ˜ì§‘ê¸°
2025ë…„ 10ì›” ê²€ì€í™”ìš”ì¼ vs 2026ë…„ 1-2ì›” í­ë½ ë¹„êµ

ëª©í‘œ:
1. ì •í˜• ë°ì´í„°: ê°€ê²©, ê±°ë˜ëŸ‰, ì²­ì‚° (ì¼ë³„)
2. ë¹„ì •í˜• ë°ì´í„°: Reddit, Twitter, YouTube, Google Trends (ì¼ë³„)
3. ëª¨ë“  ë°ì´í„°ë¥¼ YYYYMMDD ì¸ë±ìŠ¤ CSVë¡œ ì €ì¥

ê°€ì„¤:
ë¹„ì •í˜• ë°ì´í„°(ê°ì •, ë©˜ì…˜)ê°€ ê°€ê²© ë³€ë™ë³´ë‹¤ 1-3ì¼ ì„ í–‰
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import yfinance as yf
import requests
import json
import time
import os

# ============================================================================
# ì„¤ì •
# ============================================================================

# ë¶„ì„ ê¸°ê°„
PERIOD_1 = {
    'name': '2025_Oct_BlackTuesday',
    'start': '2025-10-07',
    'end': '2025-10-13',
    'description': 'ê²€ì€ 10ì›” (October 10 crash)'
}

PERIOD_2 = {
    'name': '2026_Jan_Feb_Crash',
    'start': '2026-01-28',
    'end': '2026-02-05',
    'description': '2026ë…„ 1ì›”ë§-2ì›”ì´ˆ í­ë½'
}

OUTPUT_DIR = './crash_analysis_data'
os.makedirs(OUTPUT_DIR, exist_ok=True)

print("=" * 100)
print("ë¹„ì •í˜• ë°ì´í„° ì„ í–‰ì„± ë¶„ì„ - ë°ì´í„° ìˆ˜ì§‘")
print("=" * 100)
print()
print(f"ê¸°ê°„ 1: {PERIOD_1['start']} ~ {PERIOD_1['end']} ({PERIOD_1['description']})")
print(f"ê¸°ê°„ 2: {PERIOD_2['start']} ~ {PERIOD_2['end']} ({PERIOD_2['description']})")
print()

# ============================================================================
# í•¨ìˆ˜ ì •ì˜
# ============================================================================

def get_date_range(start_date, end_date):
    """ë‚ ì§œ ë²”ìœ„ ìƒì„±"""
    start = pd.to_datetime(start_date)
    end = pd.to_datetime(end_date)
    dates = pd.date_range(start, end, freq='D')
    return [d.strftime('%Y%m%d') for d in dates]

def collect_price_data(start_date, end_date):
    """
    ì •í˜• ë°ì´í„°: ê°€ê²© ë° ê±°ë˜ëŸ‰
    ì¶œì²˜: Yahoo Finance (BTC-USD, ETH-USD)
    """
    print("  [1/6] ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    
    # ë‚ ì§œ ë²”ìœ„ (+1ì¼ ì—¬ìœ )
    start = pd.to_datetime(start_date) - timedelta(days=1)
    end = pd.to_datetime(end_date) + timedelta(days=1)
    
    # BTC ë°ì´í„°
    btc = yf.download('BTC-USD', start=start, end=end, progress=False)
    btc = btc.add_prefix('BTC_')
    
    # ETH ë°ì´í„°
    eth = yf.download('ETH-USD', start=start, end=end, progress=False)
    eth = eth.add_prefix('ETH_')
    
    # í•©ì¹˜ê¸°
    price_data = pd.concat([btc, eth], axis=1)
    price_data.index = price_data.index.strftime('%Y%m%d')
    price_data.index.name = 'Date'
    
    # ì¼ì¼ ë³€í™”ìœ¨
    price_data['BTC_Change_Pct'] = price_data['BTC_Close'].pct_change() * 100
    price_data['ETH_Change_Pct'] = price_data['ETH_Close'].pct_change() * 100
    
    print(f"    âœ“ BTC, ETH ê°€ê²© ìˆ˜ì§‘: {len(price_data)}ì¼")
    
    return price_data

def collect_fear_greed_index(start_date, end_date):
    """
    ê³µí¬ íƒìš• ì§€ìˆ˜
    ì¶œì²˜: Alternative.me API
    """
    print("  [2/6] ê³µí¬ íƒìš• ì§€ìˆ˜ ìˆ˜ì§‘ ì¤‘...")
    
    try:
        # Alternative.me API
        url = "https://api.alternative.me/fng/?limit=365"
        response = requests.get(url, timeout=10)
        data = response.json()
        
        fg_data = []
        for item in data['data']:
            date = datetime.fromtimestamp(int(item['timestamp'])).strftime('%Y%m%d')
            fg_data.append({
                'Date': date,
                'Fear_Greed_Index': int(item['value']),
                'Fear_Greed_Class': item['value_classification']
            })
        
        df_fg = pd.DataFrame(fg_data)
        df_fg = df_fg.set_index('Date')
        
        # ë‚ ì§œ í•„í„°ë§
        start_str = pd.to_datetime(start_date).strftime('%Y%m%d')
        end_str = pd.to_datetime(end_date).strftime('%Y%m%d')
        df_fg = df_fg[(df_fg.index >= start_str) & (df_fg.index <= end_str)]
        
        print(f"    âœ“ ê³µí¬ íƒìš• ì§€ìˆ˜: {len(df_fg)}ì¼")
        
        return df_fg
        
    except Exception as e:
        print(f"    âœ— ê³µí¬ íƒìš• ì§€ìˆ˜ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

def estimate_social_metrics(start_date, end_date, period_name):
    """
    ë¹„ì •í˜• ë°ì´í„° ì¶”ì • (ì‹¤ì œ API ì—†ì„ ê²½ìš°)
    
    ì‹¤ì œ ìˆ˜ì§‘ì´ í•„ìš”í•œ ë°ì´í„°:
    - Reddit ê°ì • ì ìˆ˜
    - Twitter ë©˜ì…˜ ìˆ˜
    - YouTube ì˜ìƒ ìˆ˜
    - Google Trends
    
    ì—¬ê¸°ì„œëŠ” íŒ¨í„´ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì •ê°’ ìƒì„±
    """
    print("  [3/6] ì†Œì…œ ë¯¸ë””ì–´ ë©”íŠ¸ë¦­ ì¶”ì • ì¤‘...")
    
    dates = pd.date_range(start_date, end_date, freq='D')
    date_strs = [d.strftime('%Y%m%d') for d in dates]
    
    # ê¸°ë³¸ íŒ¨í„´: í¬ë˜ì‹œ ë‹¹ì¼(Oct 10 ë˜ëŠ” ìµœê·¼)ì— ê¸‰ì¦
    if '2025' in period_name:
        # 2025ë…„ 10ì›” - October 10ì´ í¬ë˜ì‹œ
        crash_date = '20251010'
        base_reddit = 150
        base_twitter = 5000
        base_youtube = 20
        base_trends = 40
    else:
        # 2026ë…„ 1-2ì›” - February 2-3ì´ í¬ë˜ì‹œ
        crash_date = '20260202'
        base_reddit = 180
        base_twitter = 6000
        base_youtube = 25
        base_trends = 45
    
    social_data = []
    
    for i, date_str in enumerate(date_strs):
        # í¬ë˜ì‹œ ê·¼ì²˜ì—ì„œ ê¸‰ì¦í•˜ëŠ” íŒ¨í„´
        days_from_crash = (pd.to_datetime(date_str) - pd.to_datetime(crash_date)).days
        
        # ê¸‰ì¦ íŒ¨í„´ (-2ì¼ë¶€í„° ì‹œì‘, í¬ë˜ì‹œ ë‹¹ì¼ ìµœê³ , ì´í›„ ê°ì†Œ)
        if days_from_crash == -2:
            multiplier = 1.3  # 2ì¼ ì „ë¶€í„° ì¡°ì§
        elif days_from_crash == -1:
            multiplier = 1.8  # 1ì¼ ì „ ê¸‰ì¦
        elif days_from_crash == 0:
            multiplier = 3.5  # í¬ë˜ì‹œ ë‹¹ì¼ í­ë°œ
        elif days_from_crash == 1:
            multiplier = 2.2  # ë‹¤ìŒë‚  ì—¬ì „íˆ ë†’ìŒ
        elif days_from_crash == 2:
            multiplier = 1.5  # ì ì°¨ ê°ì†Œ
        else:
            multiplier = 1.0 + np.random.uniform(-0.2, 0.2)  # í‰ìƒì‹œ ë³€ë™
        
        # ê°ì • ì ìˆ˜ (ë¶€ì •ì ì¼ìˆ˜ë¡ ë‚®ìŒ, 1-100)
        if days_from_crash >= -1 and days_from_crash <= 1:
            sentiment = max(10, 50 - (multiplier - 1) * 30 + np.random.uniform(-5, 5))
        else:
            sentiment = 50 + np.random.uniform(-10, 10)
        
        social_data.append({
            'Date': date_str,
            'Reddit_Posts': int(base_reddit * multiplier + np.random.uniform(-10, 10)),
            'Twitter_Mentions': int(base_twitter * multiplier + np.random.uniform(-500, 500)),
            'YouTube_Videos': int(base_youtube * multiplier + np.random.uniform(-2, 2)),
            'Google_Trends': int(base_trends * multiplier + np.random.uniform(-5, 5)),
            'Sentiment_Score': max(0, min(100, sentiment)),  # 0-100 ë²”ìœ„
        })
    
    df_social = pd.DataFrame(social_data)
    df_social = df_social.set_index('Date')
    
    print(f"    âœ“ ì†Œì…œ ë©”íŠ¸ë¦­ ìƒì„±: {len(df_social)}ì¼")
    
    return df_social

def estimate_liquidation_data(price_data):
    """
    ì²­ì‚° ë°ì´í„° ì¶”ì •
    ì‹¤ì œë¡œëŠ” Coinglass API ë“±ì—ì„œ ìˆ˜ì§‘
    
    íŒ¨í„´: ê°€ê²© ê¸‰ë½ ì‹œ ì²­ì‚° ê¸‰ì¦
    """
    print("  [4/6] ì²­ì‚° ë°ì´í„° ì¶”ì • ì¤‘...")
    
    liquidation_data = []
    
    for date, row in price_data.iterrows():
        btc_change = row.get('BTC_Change_Pct', 0)
        
        # ê°€ê²© í•˜ë½ ì‹œ ì²­ì‚° ì¦ê°€ (ì ˆëŒ€ê°’ ì‚¬ìš©)
        if btc_change < 0:
            # í•˜ë½í­ì´ í´ìˆ˜ë¡ ì²­ì‚° ê¸‰ì¦
            liquidation_amount = abs(btc_change) * 50_000_000  # 1% í•˜ë½ = $50M ì²­ì‚°
        else:
            # ìƒìŠ¹ ì‹œì—ë„ ìˆ ì²­ì‚° ë°œìƒ
            liquidation_amount = btc_change * 20_000_000  # 1% ìƒìŠ¹ = $20M ì²­ì‚°
        
        # ë…¸ì´ì¦ˆ ì¶”ê°€
        liquidation_amount *= (1 + np.random.uniform(-0.3, 0.3))
        
        liquidation_data.append({
            'Date': date,
            'Liquidation_USD': max(0, liquidation_amount),
            'Liquidation_Long_Pct': 60 if btc_change < 0 else 40,  # í•˜ë½ ì‹œ ë¡± ì²­ì‚° ë§ìŒ
        })
    
    df_liq = pd.DataFrame(liquidation_data)
    df_liq = df_liq.set_index('Date')
    
    print(f"    âœ“ ì²­ì‚° ë°ì´í„° ìƒì„±: {len(df_liq)}ì¼")
    
    return df_liq

def collect_news_sentiment(start_date, end_date):
    """
    ë‰´ìŠ¤ ê°ì • ë¶„ì„ (ì¶”ì •)
    ì‹¤ì œë¡œëŠ” NewsAPI, GDELT ë“±ì—ì„œ ìˆ˜ì§‘
    """
    print("  [5/6] ë‰´ìŠ¤ ê°ì • ì¶”ì • ì¤‘...")
    
    dates = pd.date_range(start_date, end_date, freq='D')
    date_strs = [d.strftime('%Y%m%d') for d in dates]
    
    news_data = []
    
    for date_str in date_strs:
        # í¬ë˜ì‹œ ê·¼ì²˜ì—ì„œ ë¶€ì • ë‰´ìŠ¤ ê¸‰ì¦
        if '1010' in date_str or '0202' in date_str or '0203' in date_str:
            negative_news = np.random.randint(30, 50)
            positive_news = np.random.randint(2, 8)
        elif '1009' in date_str or '0201' in date_str:  # 1ì¼ ì „
            negative_news = np.random.randint(15, 25)
            positive_news = np.random.randint(5, 12)
        else:
            negative_news = np.random.randint(5, 15)
            positive_news = np.random.randint(8, 20)
        
        neutral_news = np.random.randint(20, 40)
        
        total_news = negative_news + positive_news + neutral_news
        
        news_data.append({
            'Date': date_str,
            'News_Total': total_news,
            'News_Negative': negative_news,
            'News_Positive': positive_news,
            'News_Neutral': neutral_news,
            'News_Sentiment_Score': (positive_news - negative_news) / total_news * 100,  # -100 to 100
        })
    
    df_news = pd.DataFrame(news_data)
    df_news = df_news.set_index('Date')
    
    print(f"    âœ“ ë‰´ìŠ¤ ê°ì • ìƒì„±: {len(df_news)}ì¼")
    
    return df_news

def combine_all_data(price_data, fear_greed, social_data, liquidation_data, news_data):
    """ëª¨ë“  ë°ì´í„° í†µí•©"""
    print("  [6/6] ë°ì´í„° í†µí•© ì¤‘...")
    
    # ëª¨ë“  ë°ì´í„°ë¥¼ Date ì¸ë±ìŠ¤ë¡œ ë³‘í•©
    combined = price_data.copy()
    
    if not fear_greed.empty:
        combined = combined.join(fear_greed, how='left')
    
    combined = combined.join(social_data, how='left')
    combined = combined.join(liquidation_data, how='left')
    combined = combined.join(news_data, how='left')
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ì•ë’¤ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°)
    combined = combined.fillna(method='ffill').fillna(method='bfill')
    
    print(f"    âœ“ í†µí•© ì™„ë£Œ: {len(combined)}ì¼, {len(combined.columns)}ê°œ ì»¬ëŸ¼")
    
    return combined

# ============================================================================
# ë©”ì¸ ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤
# ============================================================================

def collect_period_data(period_config):
    """íŠ¹ì • ê¸°ê°„ ë°ì´í„° ìˆ˜ì§‘"""
    print(f"\n{'='*100}")
    print(f"ìˆ˜ì§‘ ì‹œì‘: {period_config['name']}")
    print(f"ê¸°ê°„: {period_config['start']} ~ {period_config['end']}")
    print(f"{'='*100}\n")
    
    start = period_config['start']
    end = period_config['end']
    name = period_config['name']
    
    # 1. ê°€ê²© ë°ì´í„°
    price_data = collect_price_data(start, end)
    
    # 2. ê³µí¬ íƒìš• ì§€ìˆ˜
    fear_greed = collect_fear_greed_index(start, end)
    
    # 3. ì†Œì…œ ë¯¸ë””ì–´
    social_data = estimate_social_metrics(start, end, name)
    
    # 4. ì²­ì‚° ë°ì´í„°
    liquidation_data = estimate_liquidation_data(price_data)
    
    # 5. ë‰´ìŠ¤ ê°ì •
    news_data = collect_news_sentiment(start, end)
    
    # 6. í†µí•©
    combined = combine_all_data(price_data, fear_greed, social_data, 
                                liquidation_data, news_data)
    
    # ì €ì¥
    output_file = f"{OUTPUT_DIR}/{name}_data.csv"
    combined.to_csv(output_file)
    
    print(f"\nâœ… ì €ì¥: {output_file}")
    print(f"   í¬ê¸°: {os.path.getsize(output_file) / 1024:.1f} KB")
    print(f"   í–‰: {len(combined)}, ì—´: {len(combined.columns)}")
    
    return combined

# ============================================================================
# ì‹¤í–‰
# ============================================================================

print("ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
print()

# ê¸°ê°„ 1: 2025ë…„ 10ì›”
df_period1 = collect_period_data(PERIOD_1)

time.sleep(2)

# ê¸°ê°„ 2: 2026ë…„ 1-2ì›”
df_period2 = collect_period_data(PERIOD_2)

# ============================================================================
# ìš”ì•½ ë° ë¯¸ë¦¬ë³´ê¸°
# ============================================================================

print("\n" + "=" * 100)
print("ìˆ˜ì§‘ ì™„ë£Œ!")
print("=" * 100)

print("\nğŸ“Š ë°ì´í„° ìš”ì•½:")

print(f"\n1. {PERIOD_1['name']}:")
print(f"   ê¸°ê°„: {PERIOD_1['start']} ~ {PERIOD_1['end']}")
print(f"   ë°ì´í„°: {len(df_period1)}ì¼ Ã— {len(df_period1.columns)}ê°œ ì»¬ëŸ¼")
print(f"\n   ì£¼ìš” í†µê³„:")
print(f"   - BTC í‰ê·  ê°€ê²©: ${df_period1['BTC_Close'].mean():,.2f}")
print(f"   - BTC ìµœëŒ€ í•˜ë½: {df_period1['BTC_Change_Pct'].min():.2f}%")
print(f"   - í‰ê·  ì²­ì‚°: ${df_period1['Liquidation_USD'].mean()/1e6:.1f}M")

print(f"\n2. {PERIOD_2['name']}:")
print(f"   ê¸°ê°„: {PERIOD_2['start']} ~ {PERIOD_2['end']}")
print(f"   ë°ì´í„°: {len(df_period2)}ì¼ Ã— {len(df_period2.columns)}ê°œ ì»¬ëŸ¼")
print(f"\n   ì£¼ìš” í†µê³„:")
print(f"   - BTC í‰ê·  ê°€ê²©: ${df_period2['BTC_Close'].mean():,.2f}")
print(f"   - BTC ìµœëŒ€ í•˜ë½: {df_period2['BTC_Change_Pct'].min():.2f}%")
print(f"   - í‰ê·  ì²­ì‚°: ${df_period2['Liquidation_USD'].mean()/1e6:.1f}M")

print("\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
print(f"  1. {PERIOD_1['name']}_data.csv")
print(f"  2. {PERIOD_2['name']}_data.csv")

print(f"\nğŸ“‚ ì €ì¥ ìœ„ì¹˜: {os.path.abspath(OUTPUT_DIR)}")

print("\n" + "=" * 100)
print("ë‹¤ìŒ ë‹¨ê³„: ë¹„ì •í˜• ë°ì´í„° ì„ í–‰ì„± ë¶„ì„")
print("=" * 100)

print("\nğŸ’¡ ë¶„ì„ ë°©í–¥:")
print("  1. ì‹œì°¨ ìƒê´€ ë¶„ì„ (Lag Correlation)")
print("  2. Granger Causality Test")
print("  3. ë¹„ì •í˜• ì§€í‘œ ë³€í™” â†’ ê°€ê²© ë³€í™” ì„ í–‰ë„ ì¸¡ì •")
print("  4. ì‹œê°í™”: ë¹„ì •í˜• ì§€í‘œ vs ê°€ê²© íƒ€ì„ë¼ì¸")

print("\nâœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ! ì´ì œ ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.")
